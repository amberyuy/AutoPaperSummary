{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summarizing Text with Amazon Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The objective of this project is to build a model that can create relevant summaries for reviews written about papers.\n",
    "\n",
    "To build our model we will use a two-layered bidirectional RNN with LSTMs on the input data and two layers, \n",
    "each with an LSTM using bahdanau attention on the target data.\n",
    "\n",
    "The sections of this project are:\n",
    "- [1.Inspecting the Data](#1.-Insepcting-the-Data)\n",
    "- [2.Preparing the Data](#2.-Preparing-the-Data)\n",
    "- [3.Building the Model](#3.-Building-the-Model)\n",
    "- [4.Training the Model](#4.-Training-the-Model)\n",
    "- [5.Making Our Own Summaries](#5.-Making-Our-Own-Summaries)\n",
    "\n",
    "## Download data\n",
    "\n",
    "word embeddings [numberbatch-en-17.06.txt.gz](https://conceptnet.s3.amazonaws.com/downloads/2017/numberbatch/numberbatch-en-17.06.txt.gz)\n",
    "after download, extract to **./model/numberbatch-en-17.06.txt**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow Version: 1.10.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import time\n",
    "from tensorflow.python.layers.core import Dense\n",
    "from tensorflow.python.ops.rnn_cell_impl import _zero_state_tensors\n",
    "from tensorflow.python.ops import array_ops\n",
    "from tensorflow.python.ops import tensor_array_ops\n",
    "print('TensorFlow Version: {}'.format(tf.__version__))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "def __pickleStuff(filename, stuff):\n",
    "    save_stuff = open(filename, \"wb\")\n",
    "    pickle.dump(stuff, save_stuff)\n",
    "    save_stuff.close()\n",
    "def __loadStuff(filename):\n",
    "    saved_stuff = open(filename,\"rb\")\n",
    "    stuff = pickle.load(saved_stuff)\n",
    "    saved_stuff.close()\n",
    "    return stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load those prepared data and skip to section \"[3. Building the Model](#3.-Building-the-Model)\"\n",
    "Once we have run through the \"[2.Preparing the Data](#2.-Preparing-the-Data)\" section, we should have those data, uncomment and run those lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clean_summaries = __loadStuff(\"./data/clean_summaries.p\")\n",
    "clean_texts = __loadStuff(\"./data/clean_texts.p\")\n",
    "\n",
    "sorted_summaries = __loadStuff(\"./data/sorted_summaries.p\")\n",
    "sorted_texts = __loadStuff(\"./data/sorted_texts.p\")\n",
    "word_embedding_matrix = __loadStuff(\"./data/word_embedding_matrix.p\")\n",
    "\n",
    "vocab_to_int = __loadStuff(\"./data/vocab_to_int.p\")\n",
    "int_to_vocab = __loadStuff(\"./data/int_to_vocab.p\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Insepcting the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews = pd.read_csv(\"Papers.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(403, 6)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Title</th>\n",
       "      <th>EventType</th>\n",
       "      <th>PdfName</th>\n",
       "      <th>Abstract</th>\n",
       "      <th>PaperText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5677</td>\n",
       "      <td>Double or Nothing: Multiplicative Incentive Me...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>5677-double-or-nothing-multiplicative-incentiv...</td>\n",
       "      <td>Crowdsourcing has gained immense popularity in...</td>\n",
       "      <td>Double or Nothing: Multiplicative\\nIncentive M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5941</td>\n",
       "      <td>Learning with Symmetric Label Noise: The Impor...</td>\n",
       "      <td>Spotlight</td>\n",
       "      <td>5941-learning-with-symmetric-label-noise-the-i...</td>\n",
       "      <td>Convex potential minimisation is the de facto ...</td>\n",
       "      <td>Learning with Symmetric Label Noise: The\\nImpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6019</td>\n",
       "      <td>Algorithmic Stability and Uniform Generalization</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6019-algorithmic-stability-and-uniform-general...</td>\n",
       "      <td>One of the central questions in statistical le...</td>\n",
       "      <td>Algorithmic Stability and Uniform Generalizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6035</td>\n",
       "      <td>Adaptive Low-Complexity Sequential Inference f...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>6035-adaptive-low-complexity-sequential-infere...</td>\n",
       "      <td>We develop a sequential low-complexity inferen...</td>\n",
       "      <td>Adaptive Low-Complexity Sequential Inference f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5978</td>\n",
       "      <td>Covariance-Controlled Adaptive Langevin Thermo...</td>\n",
       "      <td>Poster</td>\n",
       "      <td>5978-covariance-controlled-adaptive-langevin-t...</td>\n",
       "      <td>Monte Carlo sampling for Bayesian posterior in...</td>\n",
       "      <td>Covariance-Controlled Adaptive Langevin\\nTherm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id                                              Title  EventType  \\\n",
       "0  5677  Double or Nothing: Multiplicative Incentive Me...     Poster   \n",
       "1  5941  Learning with Symmetric Label Noise: The Impor...  Spotlight   \n",
       "2  6019   Algorithmic Stability and Uniform Generalization     Poster   \n",
       "3  6035  Adaptive Low-Complexity Sequential Inference f...     Poster   \n",
       "4  5978  Covariance-Controlled Adaptive Langevin Thermo...     Poster   \n",
       "\n",
       "                                             PdfName  \\\n",
       "0  5677-double-or-nothing-multiplicative-incentiv...   \n",
       "1  5941-learning-with-symmetric-label-noise-the-i...   \n",
       "2  6019-algorithmic-stability-and-uniform-general...   \n",
       "3  6035-adaptive-low-complexity-sequential-infere...   \n",
       "4  5978-covariance-controlled-adaptive-langevin-t...   \n",
       "\n",
       "                                            Abstract  \\\n",
       "0  Crowdsourcing has gained immense popularity in...   \n",
       "1  Convex potential minimisation is the de facto ...   \n",
       "2  One of the central questions in statistical le...   \n",
       "3  We develop a sequential low-complexity inferen...   \n",
       "4  Monte Carlo sampling for Bayesian posterior in...   \n",
       "\n",
       "                                           PaperText  \n",
       "0  Double or Nothing: Multiplicative\\nIncentive M...  \n",
       "1  Learning with Symmetric Label Noise: The\\nImpo...  \n",
       "2  Algorithmic Stability and Uniform Generalizati...  \n",
       "3  Adaptive Low-Complexity Sequential Inference f...  \n",
       "4  Covariance-Controlled Adaptive Langevin\\nTherm...  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id           0\n",
       "Title        0\n",
       "EventType    0\n",
       "PdfName      0\n",
       "Abstract     0\n",
       "PaperText    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check for any nulls values\n",
    "reviews.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove null values and unneeded features\n",
    "reviews = reviews.dropna()\n",
    "reviews = reviews.drop(['Id','Title','EventType','PdfName'], 1)\n",
    "reviews = reviews.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(403, 2)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Abstract</th>\n",
       "      <th>PaperText</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Crowdsourcing has gained immense popularity in...</td>\n",
       "      <td>Double or Nothing: Multiplicative\\nIncentive M...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Convex potential minimisation is the de facto ...</td>\n",
       "      <td>Learning with Symmetric Label Noise: The\\nImpo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>One of the central questions in statistical le...</td>\n",
       "      <td>Algorithmic Stability and Uniform Generalizati...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We develop a sequential low-complexity inferen...</td>\n",
       "      <td>Adaptive Low-Complexity Sequential Inference f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Monte Carlo sampling for Bayesian posterior in...</td>\n",
       "      <td>Covariance-Controlled Adaptive Langevin\\nTherm...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Abstract  \\\n",
       "0  Crowdsourcing has gained immense popularity in...   \n",
       "1  Convex potential minimisation is the de facto ...   \n",
       "2  One of the central questions in statistical le...   \n",
       "3  We develop a sequential low-complexity inferen...   \n",
       "4  Monte Carlo sampling for Bayesian posterior in...   \n",
       "\n",
       "                                           PaperText  \n",
       "0  Double or Nothing: Multiplicative\\nIncentive M...  \n",
       "1  Learning with Symmetric Label Noise: The\\nImpo...  \n",
       "2  Algorithmic Stability and Uniform Generalizati...  \n",
       "3  Adaptive Low-Complexity Sequential Inference f...  \n",
       "4  Covariance-Controlled Adaptive Langevin\\nTherm...  "
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paper # 1\n",
      "Crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data. Crowdsourcing is cheap and fast, but suffers from the problem of low-quality data. To address this fundamental challenge in crowdsourcing, we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest. We show that surprisingly, under a mild and natural no-free-lunch requirement, this mechanism is the one and only incentive-compatible payment mechanism possible. We also show that among all possible incentive-compatible  mechanisms (that may or may not satisfy no-free-lunch), our mechanism makes the smallest possible payment to spammers.  Interestingly, this unique mechanism takes a multiplicative form. The simplicity of the mechanism is an added benefit.  In preliminary experiments involving over several hundred workers, we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure.\n",
      "\n",
      "32749\n",
      "Paper # 2\n",
      "Convex potential minimisation is the de facto approach to binary classification. However, Long and Servedio [2008] proved that under symmetric label noise (SLN), minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing. This ostensibly shows that convex losses are not SLN-robust. In this paper, we propose a convex, classification-calibrated loss and prove that it is SLN-robust. The loss avoids the Long and Servedio [2008] result by virtue of being negatively unbounded. The loss is a modification of the hinge loss, where one does not clamp at zero; hence, we call it the unhinged loss. We show that the optimal unhinged solution is equivalent to that of a strongly regularised SVM, and is the limiting solution for any convex potential; this implies that strong l2 regularisation makes most standard learners SLN-robust. Experiments confirm the unhinged loss’ SLN-robustness.\n",
      "\n",
      "33783\n",
      "Paper # 3\n",
      "One of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience. This includes the necessary and sufficient conditions for generalization from a given finite training set to new observations. In this paper, we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions. We provide various interpretations of this result.  For instance,  a relationship is proved between stability and data processing, which reveals that algorithmic stability can be improved by post-processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning. In addition, we establish a relationship between algorithmic stability and the size of the observation space, which provides a formal justification for dimensionality reduction methods. Finally, we connect algorithmic stability to the size of the hypothesis space, which recovers the classical PAC result that the size (complexity) of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization.\n",
      "\n",
      "36855\n",
      "Paper # 4\n",
      "We develop a sequential low-complexity inference procedure for Dirichlet process mixtures of Gaussians for online clustering and parameter estimation when the number of clusters are unknown a-priori. We present an easily computable, closed form parametric expression for the conditional likelihood, in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors. Motivated by large-sample asymptotics, we propose a noveladaptive low-complexity design for the Dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate. We further prove that in the large-sample limit, the conditional likelihood and datapredictive distribution become asymptotically Gaussian. We demonstrate through experiments on synthetic and real data sets that our approach is superior to otheronline state-of-the-art methods.\n",
      "\n",
      "29870\n",
      "Paper # 5\n",
      "Monte Carlo sampling for Bayesian posterior inference is a common approach used in machine learning. The Markov Chain Monte Carlo procedures that are used are often discrete-time analogues of associated stochastic differential equations (SDEs). These SDEs are guaranteed to leave invariant the required posterior distribution. An area of current research addresses the computational benefits of stochastic gradient methods in this setting. Existing techniques rely on estimating the variance or covariance of the subsampling error, and typically assume constant variance. In this article, we propose a covariance-controlled adaptive Langevin thermostat that can effectively dissipate parameter-dependent noise while maintaining a desired target distribution. The proposed method achieves a substantial speedup over popular alternative schemes for large-scale machine learning applications.\n",
      "\n",
      "32672\n"
     ]
    }
   ],
   "source": [
    "# Inspecting some of the reviews\n",
    "for i in range(5):\n",
    "    print(\"Paper #\",i+1)\n",
    "    print(reviews.Abstract[i]+'\\n')\n",
    "    print(len(reviews.PaperText[i]))\n",
    "    #print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Preparing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of contractions from http://stackoverflow.com/questions/19790188/expanding-english-language-contractions-in-python\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, remove_stopwords = True):\n",
    "    '''Remove unwanted characters, stopwords, and format the text to create fewer nulls word embeddings'''\n",
    "    \n",
    "    # Convert words to lower case\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Replace contractions with their longer forms \n",
    "    if True:\n",
    "        # We are not using \"text.split()\" here\n",
    "        #since it is not fool proof, e.g. words followed by punctuations \"Are you kidding?I think you aren't.\"\n",
    "        text = re.findall(r\"[\\w']+\", text)\n",
    "        new_text = []\n",
    "        for word in text:\n",
    "            if word in contractions:\n",
    "                new_text.append(contractions[word])\n",
    "            else:\n",
    "                new_text.append(word)\n",
    "        text = \" \".join(new_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "    text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)# remove links\n",
    "    text = re.sub(r'\\<a href', ' ', text)# remove html link tag\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br />', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'great movie believe may'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clean_text(\"That's a great movie,Can you believe it?I've.But you may not.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the summaries and texts\n",
    "We will remove the stopwords from the texts because they do not provide much use for training our model. However, we will keep them for our summaries so that they sound more like natural phrases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries are complete.\n",
      "Texts are complete.\n"
     ]
    }
   ],
   "source": [
    "clean_summaries = []\n",
    "for summary in reviews.Abstract:\n",
    "    clean_summaries.append(clean_text(summary, remove_stopwords=False))\n",
    "print(\"Summaries are complete.\")\n",
    "\n",
    "clean_texts = []\n",
    "for text in reviews.PaperText:\n",
    "    clean_texts.append(clean_text(text))\n",
    "print(\"Texts are complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clean Review # 1\n",
      "crowdsourcing has gained immense popularity in machine learning applications for obtaining large amounts of labeled data crowdsourcing is cheap and fast but suffers from the problem of low quality data to address this fundamental challenge in crowdsourcing we propose a simple payment mechanism to incentivize workers to answer only the questions that they are sure of and skip the rest we show that surprisingly under a mild and natural no free lunch requirement this mechanism is the one and only incentive compatible payment mechanism possible we also show that among all possible incentive compatible mechanisms that may or may not satisfy no free lunch our mechanism makes the smallest possible payment to spammers interestingly this unique mechanism takes a multiplicative form the simplicity of the mechanism is an added benefit in preliminary experiments involving over several hundred workers we observe a significant reduction in the error rates under our unique mechanism for the same or lower monetary expenditure\n",
      "\n",
      "double nothing multiplicative incentive mechanisms crowdsourcing nihar b shah university california berkeley nihar eecs berkeley edu dengyong zhou microsoft research dengyong zhou microsoft com abstract crowdsourcing gained immense popularity machine learning applications obtaining large amounts labeled data crowdsourcing cheap fast suffers problem low quality data address fundamental challenge crowdsourcing propose simple payment mechanism incentivize workers answer questions sure skip rest show surprisingly mild natural free lunch requirement mechanism one incentive compatible payment mechanism possible also show among possible incentive compatible mechanisms may may satisfy free lunch mechanism makes smallest possible payment spammers interestingly unique mechanism takes multiplicative form simplicity mechanism added benefit preliminary experiments involving several hundred workers observe significant reduction error rates unique mechanism lower monetary expenditure 1 introduction complex machine learning tools deep learning gaining increasing popularity applied wide variety problems tools however require large amounts labeled data hdy 12 ryz 10 dds 09 cbw 10 large labeling tasks performed coordinating crowds semi skilled workers internet known crowdsourcing crowdsourcing means collecting labeled training data become indispensable engineering intelligent systems workers crowdsourcing experts consequence labels obtained crowdsourcing typically significant amount error kkkmf11 vdve11 wlc 10 recent efforts focused developing statistical techniques post process noisy labels order improve quality e g ryz 10 zlp 15 kos11 ipsw14 however inputs algorithms erroneous difficult guarantee processed labels reliable enough subsequent use machine learning applications order avoid garbage garbage take complementary approach problem cleaning data time collection consider crowdsourcing settings workers paid services popular crowdsourcing platforms amazon mechanical turk others commercial platforms gained substantial popularity due support diverse range tasks machine learning labeling varying image annotation text recognition speech captioning machine translation consider problems objective nature definite answer figure 1a depicts example question worker shown set images image worker required identify image depicts golden gate bridge 1 golden gate bridge golden gate bridge yes yes sure b figure 1 different interfaces crowdsourcing setup conventional interface b option skip approach builds simple insight typical crowdsourcing setups workers simply paid proportion amount tasks complete result workers attempt answer questions sure thereby increasing error rate labels questions worker sure answers could unreliable wlc 10 kkkmf11 vdve11 jsv14 ensure acquisition high quality labels wish encourage worker skip questions unsure instance providing explicit sure option every question see figure 1b goal develop payment mechanisms encourage worker select option unsure term payment mechanism incentivizes worker incentive compatible addition incentive compatibility preventing spammers another desirable requirement incentive mechanisms crowdsourcing spammers workers answer randomly without regard question asked hope earning free money known exist large numbers crowdsourcing platforms wlc 10 boh11 kkkmf11 vdve11 thus interest deter spammers paying low possible intuitive objective end ensure zero expenditure spammers answer randomly paper however impose strictly significantly weaker condition show one one incentive compatible mechanism satisfy weak condition requirement referred free lunch axiom says questions attempted worker answered incorrectly payment must zero propose payment mechanism aforementioned setting incentive compatibility plus free lunch show surprisingly possible mechanism also show additionally mechanism makes smallest possible payment spammers among possible incentive compatible mechanisms may may satisfy free lunch axiom payment mechanism takes multiplicative form evaluation worker response question certain score final payment product scores mechanism additional appealing features simple compute also simple explain workers mechanism applicable type objective questions including multiple choice annotation questions transcription tasks etc order test whether mechanism practical assess quality final labels obtained conducted experiments amazon mechanical turk crowdsourcing platform preliminary experiments involved several hundred workers found quality data improved two fold unique mechanism total monetary expenditure lower compared conventional baseline 2 problem setting crowdsourcing setting consider one workers perform task task consists multiple questions questions objective mean question precisely one correct answer examples objective questions include multiple choice classification questions figure 1 questions transcribing text audio images etc possible answer question define worker confidence answer probability according belief answer correct words one assume worker mind probability distribution possible answers question confidence answer probability answer correct shorthand also define confidence question confidence answer worker 2 confident question assume worker confidences different questions independent goal every question worker incentivized 1 skip confidence certain pre defined threshold otherwise 2 select answer thinks confident formally let 2 0 1 predefined value goal design payment mechanisms incentivize worker skip questions confidence lower attempt confidence higher 1 moreover questions attempts answer must incentivized select answer believes likely correct threshold may chosen based various factors problem hand example downstream machine learning algorithms using crowdsourced data knowledge statistics worker abilities etc paper assume threshold given us let n denote total number questions task among assume existence gold standard questions set questions whose answers known requester let g 1 g n denote number gold standard questions g gold standard questions assumed distributed uniformly random pool n questions course worker know g n questions form gold standard payment worker task computed receiving responses questions task payment based worker performance gold standard questions since payment based known answers payments different workers depend thereby allowing us consider presence one worker without loss generality employ following standard notation positive integer k set 1 k denoted k indicator function denoted 1 e 1 z 1 z true 0 otherwise notation r denotes set non negative real numbers let x1 xg 2 1 0 1 denote evaluations answers worker gives g gold standard questions 0 denotes worker skipped question 1 denotes worker attempted answer question answer incorrect 1 denotes worker attempted answer question answer correct let f 1 0 1 g r denote payment function namely function determines payment worker based evaluations x1 xg note crowdsourcing platforms today mandate payments non negative let µ 0 denote budget e maximum amount paid individual worker task max f x1 xg µ x1 xg amount µ thus amount compensation paid perfect agent work assume budget condition µ throughout rest paper assume worker attempts maximize overall expected payment follows expression worker expected payment refer expected payment worker point view expectation taken respect worker confidences answers uniformly random choice g gold standard questions among n questions task question 2 n let yi 1 worker attempts question set yi 0 otherwise every question 2 n yi 6 0 let pi confidence worker answer selected question every question 2 n yi 0 let pi 2 0 1 arbitrary value let e 1 g 2 1 1 g worker perspective expected payment selected answers confidence levels g x x 1 1 1 f 1 yj1 g yjg pji 2 1 pji 2 n g 1 j1 jg e2 1 1 g 1 n expression outermost summation corresponds expectation respect randomness arising unknown choice gold standard questions inner summation corresponds expectation respect worker beliefs correctness responses 1 event confidence question exactly equal worker may equally incentivized answer skip 3 call payment function f incentive compatible mechanism expected payment worker payment function strictly maximized worker responds manner desired 2 3 main results incentive compatible mechanism guarantees section present main results paper namely design incentive compatible mechanisms practically useful properties end impose following natural requirement payment function f motivated practical considerations budget constraints discouraging spammers miscreants boh11 kkkmf11 vdve11 wlc 10 term requirement free lunch axiom axiom 1 free lunch axiom answers attempted worker gold standard wrong payment zero formally every set evaluations x1 xg satisfy pg pg 0 1 1 xi 6 0 1 1 xi 1 require payment satisfy f x1 xg 0 observe free lunch extremely mild requirement fact significantly weaker imposing zero payment workers answer randomly instance questions binary choice format randomly choosing among two options question would result 50 answers correct expectation free lunch axiom applicable none turns correct 3 1 proposed multiplicative mechanism present proposed payment mechanism algorithm 1 algorithm 1 multiplicative incentive compatible mechanism inputs threshold budget µ evaluations x1 xg 2 1 0 1 g worker answers g gold standard questions pg pg let c 1 1 xi 1 w 1 1 xi 1 payment f x1 xg µt g c 1 w 0 proposed mechanism multiplicative form answer gold standard given score based whether correct score t1 incorrect score 0 skipped score 1 final payment simply product scores scaled µ mechanism easy describe workers instance 12 g 3 µ 80 cents description reads reward starts 10 cents every correct answer 3 gold standard questions reward double however questions answered incorrectly reward become zero please use sure option wisely observe payment rule similar popular double nothing paradigm dou14 algorithm makes zero payment one attempted answers gold standard wrong note property significantly stronger property free lunch originally required wanted zero payment attempted answers wrong surprisingly prove shortly algorithm 1 incentive compatible mechanism satisfies free lunch following theorem shows proposed payment mechanism indeed incentivizes worker skip questions confidence answering confidence greater latter case worker incentivized select answer thinks likely correct theorem 1 payment mechanism algorithm 1 incentive compatible satisfies nofree lunch condition 2 payment function based gold standard questions also called strictly proper scoring rule gr07 4 proof theorem 1 presented appendix easy see mechanism satisfies nofree lunch proof incentive compatibility also hard consider arbitrary worker arbitrary belief distributions compute expected payment worker case choices task follow requirements show choice leads strictly smaller expected payment started weak condition free lunch making zero payment attempted answers wrong mechanism proposed algorithm 1 significantly strict makes zero payment attempted answers wrong natural question arises design alternative mechanism satisfying incentive compatibility nofree lunch operates somewhere 3 2 uniqueness mechanism previous section showed proposed multiplicative mechanism incentive compatible satisfies intuitive requirement free lunch turns perhaps surprisingly mechanism unique respect theorem 2 payment mechanism algorithm 1 incentive compatible mechanism satisfies free lunch condition theorem 2 gives strong result despite imposing weak requirements see recall earlier discussion deterring spammers incurring low expenditure workers answer randomly instance task comprises binary choice questions one may wish design mechanisms make zero payment responses 50 questions gold standard incorrect free lunch axiom much weaker requirement mechanism satisfy requirement mechanism algorithm 1 proof theorem 2 available appendix b proof relies following key lemma establishes condition incentive compatible mechanism must necessarily satisfy lemma applies incentive compatible mechanism satisfying free lunch lemma incentive compatible payment mechanism f must satisfy every 2 1 g every y1 yi 1 yi 1 yg 2 1 0 1 g 1 f y1 yi 1 1 yi 1 yg 1 f y1 yi 1 1 yi 1 yg f y1 yi 1 0 yi 1 yg proof lemma provided appendix c given lemma proof theorem 2 completed via induction number skipped questions 3 3 optimality spamming behavior discussed earlier crowdsouring tasks especially multiple choice questions often encounter spammers answer randomly without heed question asked instance binary choice setup spammer choose one two options uniformly random every question highly desirable objective crowdsourcing settings deter spammers end one may wish impose condition zero payment responses 50 attempted questions gold standard incorrect second desirable metric could minimize expenditure worker simply skips questions aforementioned requirements deterministic functions worker responses one may alternatively wish impose requirements depend distribution worker answering process instance third desirable feature would minimize expected payment worker answers questions uniformly random show interestingly unique multiplicative payment mechanism simultaneously satisfies requirements result stated assuming multiplechoice setup extends trivially non multiple choice settings theorem 3 distributional consider value 2 0 g among incentivecompatible mechanisms may may satisfy free lunch algorithm 1 strictly minimizes expenditure worker skips questions gold standard chooses answers remaining g questions uniformly random 5 theorem 3 b deterministic consider value b 2 0 1 among incentive compatible mechanisms may may satisfy free lunch algorithm 1 strictly minimizes expenditure worker gives incorrect answers fraction b questions attempted gold standard proof theorem 3 presented appendix see result multiplicative payment mechanism algorithm 1 thus possesses useful properties geared deter spammers ensuring good worker paid high enough amount illustrate point let us compare mechanism algorithm 1 popular additive class payment mechanisms example 1 consider popular class additive mechanisms payments worker added across gold standard questions additive payment mechanism offers reward µ µt g every correct answer gold standard g every question skipped 0 every incorrect answer importantly final payment worker sum rewards across g gold standard questions one verify additive mechanism incentive compatible one also see guaranteed theory additive payment mechanism satisfy free lunch axiom suppose question involves choosing two options let us compute expenditure two mechanisms make spamming behavior choosing answer randomly question given 50 likelihood question correct compute additive mechanism makes payment µ2 expectation hand mechanism pays expected amount µ2 g payment spammers thus reduces exponentially number gold standard questions mechanism whereas reduce additive mechanism consider different means exploiting mechanism worker simply skips questions end observe worker skips questions additive payment mechanism incur expenditure µt hand proposed payment mechanism algorithm 1 pays exponentially smaller amount µt g recall 1 4 simulations experiments section present synthetic simulations real world experiments evaluate effects setting mechanism final label quality 4 1 synthetic simulations employ synthetic simulations understand effects various kinds labeling errors crowdsourcing consider binary choice questions set simulations whenever worker answers question confidence correct answer drawn distribution p independent else investigate effects following five choices distribution p uniform distribution support 0 5 1 triangular distribution lower end point 0 2 upper end point 1 mode 0 6 beta distribution parameter values 5 1 hammer spammer distribution kos11 uniform discrete set 0 5 1 truncated gaussian distribution truncation n 0 75 0 5 interval 0 1 worker confidence p drawn distribution p attempts question probability making error equals 1 p compare setting workers attempt every question b setting workers skip questions confidence certain threshold set simulations set 0 75 either setting aggregate labels obtained workers question via majority vote two classes ties broken choosing one two options uniformly random 6 figure 2 error different interfaces synthetic simulations five distributions workers error probabilities figure 2 depicts results simulations bar represents fraction questions labeled incorrectly average across 50 000 trials standard error mean small visible see skip based setting consistently outperforms conventional setting gains obtained moderate high depending underlying distribution workers errors particular gains quite striking hammer spammer model result surprising since mechanism ideally screens spammers leaves hammers answer perfectly 4 2 experiments amazon mechanical turk conducted preliminary experiments amazon mechanical turk commercial crowdsourcing platform mturk com evaluate proposed scheme real world scenarios complete data including interface presented workers tasks results obtained workers ground truth solutions available website first author goal delving details first note certain caveats relating study mechanism design crowdsourcing platforms worker encounters mechanism small amount time handful tasks typical research experiments small amount money dollars typical crowdsourcing tasks cannot expect worker completely understand mechanism act precisely required instance expect experimental results change significantly even upon moderate modifications promised amounts furthermore expect outcomes noisy incentive compatibility kicks worker encounters mechanism across longer term example proposed mechanism adopted standard platform higher amounts involved would expect workers others e g bloggers researchers design strategies game mechanism theoretical guarantee incentive compatibility strict properness prevents gaming long run thus regard experiments preliminary intentions towards experimental exercise evaluate potential algorithms work practice b investigate effect proposed algorithms net error collected labelled data experimental setup conducted five following experiments tasks amazon mechanical turk identifying golden gate bridge pictures b identifying breeds dogs pictures c identifying heads countries identifying continents flags belong e identifying textures displayed images tasks comprised 20 126 multi7 figure 3 error different interfaces mechanisms five experiments conducted mechanical turk ple choice questions 3 experiment compared baseline setting figure 1a additive payment mechanism pays fixed amount per correct answer ii skip based setting figure 1b multiplicative mechanism algorithm 1 experiment two settings 35 workers independently perform task upon completion tasks amazon mechanical turk aggregated data following manner mechanism experiment subsampled 3 5 7 9 11 workers took majority vote responses averaged accuracy across questions across 1 000 iterations subsample aggregate procedure results figure 3 reports error aggregate data five experiments see cases skip based setting results higher quality data many instances reduction two fold higher experiments observed substantial reduction amount error labelled data expending lower amounts receiving negative comments workers observations suggest proposed skip based setting coupled multiplicative payment mechanisms potential work practice underlying fundamental theory ensures system cannot gamed long run 5 discussion conclusions extended version paper sz14 generalize skip based setting considered one also elicit workers confidence answers moreover companion paper szp15 construct mechanisms elicit support worker beliefs mechanism offers additional benefits pattern skips workers provide reasonable estimate difficulty question practice questions estimated difficult may delegated expert additional non expert workers secondly theoretical guarantees mechanism may allow better post processing data incorporating confidence information improving overall accuracy developing statistical aggregation algorithms augmenting existing ones e g ryz 10 kos11 lpi12 zlp 15 purpose useful direction research thirdly simplicity mechanisms may facilitate easier adoption among workers conclusion given uniqueness optimality theory simplicity good performance observed practice envisage multiplicative payment mechanisms interest practitioners well researchers employ crowdsourcing 3 see extended version paper sz14 additional experiments involving free form responses text transcription 8 references boh11 john bohannon social science pennies science 334 6054 307 307 2011 cbw 10 andrew carlson justin betteridge richard c wang estevam r hruschka jr tom mitchell coupled semi supervised learning information extraction acm wsdm pages 101 110 2010 dds 09 jia deng wei dong richard socher li jia li kai li li fei fei imagenet large scale hierarchical image database ieee conference computer vision pattern recognition pages 248 255 2009 dou14 double nothing http wikipedia org wiki double nothing 2014 last accessed july 31 2014 gr07 tilmann gneiting adrian e raftery strictly proper scoring rules prediction estimation journal american statistical association 102 477 359 378 2007 hdy 12 geoffrey hinton li deng dong yu george e dahl abdel rahman mohamed navdeep jaitly andrew senior vincent vanhoucke patrick nguyen tara n sainath et al deep neural networks acoustic modeling speech recognition shared views four research groups ieee signal processing magazine 29 6 82 97 2012 panagiotis g ipeirotis foster provost victor sheng jing wang repeated labeling using multiple noisy labelers data mining knowledge discovery 28 2 402 441 2014 ipsw14 jsv14 srikanth jagabathula lakshminarayanan subramanian ashwin venkataraman reputation based worker filtering crowdsourcing advances neural information processing systems 27 pages 2492 2500 2014 kkkmf11 gabriella kazai jaap kamps marijn koolen natasa milic frayling crowdsourcing book search evaluation impact hit design comparative system ranking acm sigir pages 205 214 2011 kos11 david r karger sewoong oh devavrat shah iterative learning reliable crowdsourcing systems advances neural information processing systems pages 1953 1961 2011 lpi12 qiang liu jian peng alexander ihler variational inference crowdsourcing nips pages 701 709 2012 ryz 10 vikas c raykar shipeng yu linda h zhao gerardo hermosillo valadez charles florin luca bogoni linda moy learning crowds journal machine learning research 11 1297 1322 2010 sz14 nihar b shah dengyong zhou double nothing multiplicative incentive mechanisms crowdsourcing arxiv 1408 1387 2014 szp15 nihar b shah dengyong zhou yuval peres approval voting incentives crowdsourcing international conference machine learning icml 2015 vdve11 jeroen vuurens arjen p de vries carsten eickhoff much spam take analysis crowdsourcing results increase accuracy acm sigir workshop crowdsourcing information retrieval pages 21 26 2011 wlc 10 paul wais shivaram lingamneni duncan cook jason fennell benjamin goldenberg daniel lubarov david marin hari simons towards building highquality workforce mechanical turk nips workshop computational social science wisdom crowds 2010 zlp 15 dengyong zhou qiang liu john c platt christopher meek nihar b shah regularized minimax conditional entropy crowdsourcing arxiv preprint arxiv 1503 07240 2015 9\n",
      "\n",
      "Clean Review # 2\n",
      "convex potential minimisation is the de facto approach to binary classification however long and servedio 2008 proved that under symmetric label noise sln minimisation of any convex potential over a linear function class can result in classification performance equivalent to random guessing this ostensibly shows that convex losses are not sln robust in this paper we propose a convex classification calibrated loss and prove that it is sln robust the loss avoids the long and servedio 2008 result by virtue of being negatively unbounded the loss is a modification of the hinge loss where one does not clamp at zero hence we call it the unhinged loss we show that the optimal unhinged solution is equivalent to that of a strongly regularised svm and is the limiting solution for any convex potential this implies that strong l2 regularisation makes most standard learners sln robust experiments confirm the unhinged loss sln robustness\n",
      "\n",
      "learning symmetric label noise importance unhinged brendan van rooyen aditya krishna menon australian national university robert c williamson national ict australia brendan vanrooyen aditya menon bob williamson nicta com au abstract convex potential minimisation de facto approach binary classification however long servedio 2010 proved symmetric label noise sln minimisation convex potential linear function class result classification performance equivalent random guessing ostensibly shows convex losses sln robust paper propose convex classification calibrated loss prove sln robust loss avoids long servedio 2010 result virtue negatively unbounded loss modification hinge loss one clamp zero hence call unhinged loss show optimal unhinged solution equivalent strongly regularised svm limiting solution convex potential implies strong 2 regularisation makes standard learners sln robust experiments confirm unhinged loss sln robustness borne practice apologies wilde 1895 truth rarely pure simple 1 learning symmetric label noise binary classification canonical supervised learning problem given instance space x samples distribution x 1 goal learn scorer x r low misclassification error future samples drawn interest realistic scenario learner observes samples corruption labels constant probability flipped goal still perform well respect problem known learning symmetric label noise sln learning angluin laird 1988 long servedio 2010 showed exist linearly separable learner observes corruption symmetric label noise nonzero rate minimisation convex potential linear function class results classification performance equivalent random guessing ostensibly establishes convex losses sln robust motivates use non convex losses stempfel ralaivola 2009 masnadi shirazi et al 2010 ding vishwanathan 2010 denchev et al 2012 manwani sastry 2013 paper propose convex loss prove sln robust loss avoids result long servedio 2010 virtue negatively unbounded loss modification hinge loss one clamp zero thus call unhinged loss loss several appealing properties unique convex loss satisfying notion strong sln robustness proposition 5 classification calibrated proposition 6 consistent minimised proposition 7 simple optimal solution difference two kernel means equation 8 finally show optimal solution equivalent strongly regularised svm proposition 8 twice differentiable convex potential proposition 9 implying strong 2 regularisation endows standard learners sln robustness 1 classifier resulting minimising unhinged loss new devroye et al 1996 chapter 10 scho lkopf smola 2002 section 1 2 shawe taylor cristianini 2004 section 5 1 however establishing classifier strong sln robustness uniqueness thereof equivalence highly regularised svm solution knowledge novel 2 background problem setup fix instance space x denote distribution x 1 random variables x may expressed via class conditionals p q p x 1 p x 1 base rate π p 1 via marginal p x class probability function η x 7 p 1 x x interchangeably write dp q π dm η 2 1 classifiers scorers risks scorer function x r loss function 1 r r use 1 1 refer 1 1 conditional risk l 0 1 r r defined l η v 7 η 1 v 1 η 1 v given distribution risk scorer defined ld x e x 1 ld e l η x x set l set risks scorers x function class f rx given f set restricted bayes optimal scorers loss scorers f minimise risk sd f argmin ld f set unrestricted bayes optimal scorers sd sd f f rx restricted regret scorer excess risk restricted bayes optimal scorer regretd f ld inf l f binary classification concerned zero one loss 01 v 7 jyv 0k 21 jv 0k loss classification calibrated bayes optimal scorers also optimal zero one loss sd sd 01 convex potential loss v 7 φ yv φ r r convex non increasing differentiable φ0 0 0 φ 0 long servedio 2010 definition 1 convex potentials classification calibrated bartlett et al 2006 theorem 2 1 2 2 learning symmetric label noise sln learning problem learning symmetric label noise sln learning following angluin laird 1988 kearns 1998 blum mitchell 1998 natarajan et al 2013 notional clean distribution would like observe instead observe samples corrupted distribution sln ρ ρ 0 1 2 distribution sln ρ marginal distribution instances unchanged label independently flipped probability ρ goal learn scorer corrupted samples ld 01 small quantity denote corrupted counterparts sln ρ bar e g corrupted marginal distribution η corrupted class probability function additionally ρ clear context occasionally refer sln ρ easy check corrupted marginal distribution natarajan et al 2013 lemma 7 x x η x 1 2ρ η x ρ 3 2 sln robustness formalisation consider learners f loss function class f learning search f minimises risk informally f robust symmetric label noise slnrobust minimising f gives classifier clean distribution 2 learner would like observe sln ρ ρ 0 1 2 learner actually observes formalise notion review known sln robust learners 3 1 sln robust learners formal definition fixed instance space x let denote set distributions x 1 given notional clean distribution nsln 2 returns set possible corrupted versions learner may observe labels flipped unknown probability ρ 1 nsln 7 sln ρ ρ 0 2 equipped define notion sln robustness definition 1 sln robustness say learner f sln robust f f nsln ld ld 01 01 3 sln robustness requires level label noise observed distribution classification performance wrt learner learner directly observes unfortunately widely adopted class learners sln robust see 3 2 convex potentials linear function classes sln robust fix x rd consider learners convex potential function class linear scorers flin x 7 hw xi w rd captures e g linear svm logistic regression widely studied theory applied practice disappointingly learners sln robust long servedio 2010 theorem 2 give example learning symmetric label noise convex potential corrupted risk minimiser flin classification performance equivalent random guessing implies flin sln robust1 per definition 1 proposition 1 long servedio 2010 theorem 2 let x rd 2 pick convex potential flin sln robust 3 3 fallout learners sln robust light proposition 1 two ways proceed order obtain sln robust learners either change class losses change function class f first approach pursued large body work embraces non convex losses stempfel ralaivola 2009 masnadi shirazi et al 2010 ding vishwanathan 2010 denchev et al 2012 manwani sastry 2013 losses avoid conditions proposition 1 automatically imply sln robust used flin appendix b present evidence losses fact sln robust used flin second approach consider suitably rich f contains bayes optimal scorer e g employing universal kernel choice one still use convex potential loss fact owing equation 2 classification calibrated loss proposition 2 pick classification calibrated rx sln robust approaches drawbacks first approach computational penalty requires optimising non convex loss second approach statistical penalty estimation rates rich f require larger sample size thus appears sln robustness involves computational statistical tradeoff however variant first option pick loss convex convex potential loss would afford computational statistical advantages minimising convex risks linear scorers manwani sastry 2013 demonstrated square loss v 1 yv 2 one loss show simpler loss convex sln robust class convex potentials virtue negatively unbounded derive loss first interpret robustness via noise correction procedure 1 even content difference 0 1 2 clean corrupted minimisers performance long servedio 2010 theorem 2 implies worst case 1 2 3 4 noise corrected loss perspective sln robustness express sln robustness reason optimal scorers distribution two different losses help characterise set strongly sln robust losses 4 1 reformulating sln robustness via noise corrected losses given ρ 0 1 2 natarajan et al 2013 lemma 1 showed associate loss noise corrected counterpart ld l loss defined follows definition 2 noise corrected loss given loss ρ 0 1 2 noise corrected loss 1 v r v 1 ρ v ρ v 1 2ρ 4 since depends unknown parameter ρ directly usable design sln robust learner nonetheless useful theoretical device since construction f sd f sd f sd f means sufficient condition f sln robust sd f ghosh et al 2015 theorem 1 proved sufficient condition holds namely c r v r 1 v 1 v c 5 interestingly equation 5 necessary stronger notion robustness explore 4 2 characterising stronger notion sln robustness first step towards stronger notion robustness rewrite slight abuse notation ld e x x e r l r r distribution labels scores standard sln robustness requires label noise change risk minimisers e l r l r s0 s0 relation holds place strong sln robustness strengthens notion requiring label noise affect ordering pairs joint distributions labels scores course trivially implies sln robustness definition given distribution r labels scores let r corresponding distribution labels flipped probability ρ strong sln robustness made precise follows definition 3 strong sln robustness call loss strongly sln robust every ρ 0 1 2 r r0 l r l r0 l r l r0 express strong sln robustness using notion order equivalence loss pairs simply requires two losses order distributions labels scores identically order equivalent definition 4 order equivalent loss pairs call pair losses r r0 l r l r0 l r l r0 clearly order equivalence implies sd f sd f turn implies sln robustness thus surprising relate order equivalence strong sln robustness proposition 3 loss strongly sln robust iff every ρ 0 1 2 order equivalent connection lets us exploit classical result decision theory order equivalent losses affine transformations combined definition lets us conclude sufficient condition equation 5 also necessary strong sln robustness proposition 4 loss strongly sln robust satisfies equation 5 return original goal find convex sln robust flin ideally general function classes suggests reasonable consider losses satisfy equation 5 unfortunately evident convex non constant bounded zero cannot possibly admissible sense show removing boundedness restriction allows existence convex admissible loss 4 5 unhinged loss convex strongly sln robust loss consider following simple non standard convex loss unh unh 1 v 1 v 1 v 1 v compared hinge loss loss clamp zero e hinge thus peculiarly negatively unbounded issue discuss 5 3 thus call unhinged loss2 loss number attractive properties immediate sln robustness 5 1 unhinged loss strongly sln robust unh unh since unh strongly sln robust thus 1 v 1 v 2 proposition 4 implies unh f sln robust f following uniqueness property hard show proposition 5 pick convex loss c r 1 v 1 v c b r 1 v v b 1 v v scaling translation unh convex loss strongly sln robust returning case linear scorers implies unh flin sln robust contradict proposition 1 since unh convex potential negatively unbounded intuitively property allows loss offset penalty incurred instances misclassified high margin awarding gain instances correctly classified high margin 5 2 unhinged loss classification calibrated sln robustness insufficient learner useful example loss uniformly zero strongly sln robust useless classification calibrated fortunately unhinged loss classification calibrated establish technical reasons see 5 3 operate fb b b x set scorers range bounded b 0 proposition 6 fix unh dm η b 0 fb x 7 b sign 2η x 1 thus every b 0 restricted bayes optimal scorer fb sign bayes optimal classifier 0 1 loss limiting case f rx optimal scorer attainable operate extended reals r unh classification calibrated 5 3 enforcing boundedness loss classification calibration unh encouraging proposition 6 implies unrestricted bayes risk thus regret every non optimal scorer identically hampers analysis consistency orthodox decision theory analogous theoretical issues arise attempting establish basic theorems unbounded losses ferguson 1967 pg 78 side step issue restricting attention bounded scorers unh effectively bounded proposition 6 affect classification calibration loss context linear scorers boundedness scorers achieved regularisation instead work ing flin one instead use flin λ x 7 hw xi w 2 1 λ λ 0 flin λ fr λ r supx x x 2 observe unh f sln robust f unh flin λ sln robust λ 0 shall see 6 3 working flin λ also lets us establish sln robustness hinge loss λ large 5 4 unhinged loss minimisation corrupted distribution consistent using bounded scorers makes possible establish surrogate regret bound unhinged loss shows classification consistency unhinged loss minimisation corrupted distribution 2 loss considered sriperumbudur et al 2009 reid williamson 2011 context maximum mean discrepancy see appendix analysis sln robustness knowledge novel 5 proposition 7 fix unh ρ 0 1 2 b 1 scorer fb 1 fb regretd regret fb 01 regret 1 2ρ standard rates convergence via generalisation bounds also trivial derive see appendix 6 learning unhinged loss kernels show optimal solution unhinged loss employing regularisation kernelised scorers simple form sheds light sln robustness regularisation 6 1 centroid classifier optimises unhinged loss consider minimising unhinged risk class kernelised scorers fh λ x 7 hw φ x ih w h 1 λ λ 0 φ x h feature mapping reproducing kernel hilbert space h kernel k equivalently given distribution3 want λ wunh λ argmin e 1 hw φ x hw wih 6 2 x w h first order optimality condition implies 1 7 wunh λ e φ x λ x kernel mean map smola et al 2007 thus optimal unhinged scorer 1 1 unh λ x 7 e k x x x 7 π e k x x 1 π e k x x x p x q λ x λ 8 equation 8 unhinged solution equivalent nearest centroid classifier manning et al 2008 pg 181 tibshirani et al 2002 shawe taylor cristianini 2004 section 5 1 equation 8 gives simple way understand sln robustness unh fh λ optimal scorers clean corrupted distributions differ scaling see appendix 1 e k x x 9 x x e k x x 1 2ρ x x interestingly servedio 1999 theorem 4 established nearest centroid classifier termed average robust general class label noise required assumption uniform unit sphere result establishes sln robustness classifier holds without assumptions fact ghosh et al 2015 theorem 1 lets one quantify unhinged loss performance general noise model see appendix discussion 6 2 practical considerations note several points relating practical usage unhinged loss kernelised scorers first cross validation required select λ since changing λ changes magnitude scores sign thus purposes classification one simply use λ 1 second easily extend scorers use bias regularised strength 0 λb 6 λ tuning λb equivalent computing unh λ per equation 8 tuning threshold holdout set third h rd small store wunh λ explicitly use make predictions high infinite dimensional h either make predictions directly via equation 8 use random fourier features rahimi recht 2007 approximately embed h low dimensional rd store wunh λ usual latter requires translation invariant kernel show assumptions wunh λ coincides solution two established methods appendix discusses relationships e g maximum mean discrepancy 3 given training sample dn use plugin estimates appropriate 6 6 3 equivalence highly regularised svm convex potentials interesting equivalence unhinged solution highly regularised svm noted e g hastie et al 2004 section 6 showed svms approach nearest centroid classifier course optimal unhinged solution proposition 8 pick φ x h r supx x φ x h λ 0 let whinge λ argmin w h e x max 0 1 hw φ x ih λ hw wih 2 soft margin svm solution λ r2 whinge λ wunh λ since unh fh λ sln robust follows hinge v 7 max 0 1 yv hinge fh λ similarly sln robust provided λ sufficiently large strong 2 regularisation bounded feature map endows hinge loss sln robustness4 proposition 8 generalised show wunh λ limiting solution twice differentiable convex potential shows strong 2 regularisation endows learners sln robustness intuitively strong regularisation one considers behaviour loss near zero since convex potential φ φ0 0 0 behave similarly linear approximation around zero viz unhinged loss proposition 9 pick bounded feature mapping φ x h twice differentiable convex potential φ φ00 1 1 bounded let wφ λ minimiser regularised φ risk 2 w wunh λ φ λ lim 0 λ wφ λ h wunh λ h h 6 4 equivalence fisher linear discriminant whitened data binary classification dm η fisher linear discriminant fld finds weight vector proportional minimiser square loss sq v 7 1 yv 2 bishop 2006 section 4 1 5 wsq λ ex xxt λi 1 e x x 10 wsq λ changed scaling equation 9 fact corrupted marginal factor label noise provides alternate proof fact sq flin sln robust5 manwani sastry 2013 theorem 2 clearly unhinged loss solution wunh λ equivalent fld square loss solution wsq λ input data whitened e e xx x well specified f e g universal kernel unhinged square loss asymptotically recover optimal classifier unhinged loss require matrix inversion misspecified f one cannot general argue superiority unhinged loss square loss vice versa universally good surrogate 0 1 loss reid williamson 2010 appendix appendix illustrate examples losses may underperform 7 sln robustness unhinged loss empirical illustration illustrate unhinged loss sln robustness empirically manifest reiterate high regularisation unhinged solution equivalent svm limit classification calibrated loss solution thus aim assert unhinged loss better losses rather demonstrate sln robustness purely theoretical first show unhinged risk minimiser performs well example long servedio 2010 henceforth ls10 figure 1 shows distribution x 1 0 γ 5γ γ γ r2 marginal distribution 14 14 12 three instances deterministically positive pick γ 1 2 unhinged minimiser perfectly classifies three points regardless level label noise figure 1 hinge minimiser perfect noise even small amount noise achieves 50 error rate 4 5 long servedio 2010 section 6 show 1 regularisation endow sln robustness square loss escapes result long servedio 2010 since monotone decreasing 7 1 unhinged hinge 0 noise hinge 1 noise 0 5 0 5 ρ ρ ρ ρ ρ ρ 1 0 5 0 0 1 0 2 0 3 0 4 0 49 hinge logistic unhinged 0 00 0 00 0 15 0 27 0 21 0 30 0 38 0 37 0 42 0 36 0 47 0 38 0 00 0 00 0 00 0 00 0 00 0 00 0 22 0 08 0 22 0 08 0 39 0 23 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 00 0 34 0 48 table 1 mean standard deviation 01 error 125 trials ls10 grayed cells denote best performer noise rate 1 figure 1 ls10 dataset next consider empirical risk minimisers random training sample construct training set 800 instances injected varying levels label noise evaluate classification performance test set 1000 instances compare hinge logistic 2 ding vishwanathan 2010 unhinged minimisers using linear scorer without bias term regularisation strength λ 10 16 table 1 even 40 label noise unhinged classifier able find perfect solution contrast losses suffer even moderate noise rates next report results uci datasets additionally tune threshold ensure best training set 0 1 accuracy table 2 summarises results sample four datasets appendix contains results datasets performance metrics losses even noise close 50 unhinged loss often able learn classifier discriminative power ρ ρ ρ ρ ρ ρ 0 0 1 0 2 0 3 0 4 0 49 hinge logistic unhinged 0 00 0 00 0 01 0 03 0 06 0 12 0 17 0 20 0 35 0 24 0 60 0 20 0 00 0 00 0 01 0 03 0 04 0 05 0 09 0 11 0 24 0 16 0 49 0 20 0 00 0 00 0 00 0 00 0 00 0 01 0 02 0 07 0 13 0 22 0 45 0 33 ρ ρ ρ ρ ρ ρ 0 0 1 0 2 0 3 0 4 0 49 hinge logistic unhinged 0 05 0 00 0 06 0 01 0 06 0 01 0 08 0 04 0 14 0 10 0 45 0 26 0 05 0 00 0 07 0 02 0 08 0 03 0 11 0 05 0 24 0 13 0 49 0 16 0 05 0 00 0 05 0 00 0 05 0 00 0 05 0 01 0 09 0 10 0 46 0 30 iris ρ ρ ρ ρ ρ ρ 0 0 1 0 2 0 3 0 4 0 49 b housing hinge logistic unhinged 0 00 0 00 0 10 0 08 0 19 0 11 0 31 0 13 0 39 0 13 0 50 0 16 0 00 0 00 0 11 0 02 0 15 0 02 0 22 0 03 0 33 0 04 0 48 0 04 0 00 0 00 0 00 0 00 0 00 0 00 0 01 0 00 0 02 0 02 0 34 0 21 ρ ρ ρ ρ ρ ρ c usps0v7 0 0 1 0 2 0 3 0 4 0 49 hinge logistic unhinged 0 05 0 00 0 15 0 03 0 21 0 03 0 25 0 03 0 31 0 05 0 48 0 09 0 04 0 00 0 24 0 00 0 24 0 00 0 24 0 00 0 24 0 00 0 40 0 24 0 19 0 00 0 19 0 01 0 19 0 01 0 19 0 03 0 22 0 05 0 45 0 08 splice table 2 mean standard deviation 0 1 error 125 trials uci datasets 8 conclusion future work proposed convex classification calibrated loss proved robust symmetric label noise sln robust showed unique loss satisfies notion strong sln robustness established optimised nearest centroid classifier showed convex potentials svm also sln robust highly regularised apologies wilde 1895 truth rarely pure simple acknowledgments nicta funded australian government department communications australian research council ict centre excellence program authors thank cheng soon ong valuable comments draft paper 8 references dana angluin philip laird learning noisy examples machine learning 2 4 343 370 1988 peter l bartlett michael jordan jon mcauliffe convexity classification risk bounds journal american statistical association 101 473 138 156 2006 christopher bishop pattern recognition machine learning springer verlag new york inc 2006 avrim blum tom mitchell combining labeled unlabeled data co training conference computational learning theory colt pages 92 100 1998 vasil denchev nan ding hartmut neven v n vishwanathan robust classification adiabatic quantum optimization international conference machine learning icml pages 863 870 2012 luc devroye la szlo gyo rfi ga bor lugosi probabilistic theory pattern recognition springer 1996 nan ding v n vishwanathan logistic regression advances neural information processing systems nips pages 514 522 curran associates inc 2010 thomas ferguson mathematical statistics decision theoretic approach academic press 1967 aritra ghosh naresh manwani p sastry making risk minimization tolerant label noise neurocomputing 160 93 107 2015 trevor hastie saharon rosset robert tibshirani ji zhu entire regularization path support vector machine journal machine learning research 5 1391 1415 december 2004 issn 1532 4435 michael kearns efficient noise tolerant learning statistical queries journal acm 5 6 392 401 november 1998 philip long rocco servedio random classification noise defeats convex potential boosters machine learning 78 3 287 304 2010 issn 0885 6125 christopher manning prabhakar raghavan hinrich schu tze introduction information retrieval cambridge university press new york ny usa 2008 isbn 0521865719 9780521865715 naresh manwani p sastry noise tolerance risk minimization ieee transactions cybernetics 43 3 1146 1151 june 2013 hamed masnadi shirazi vijay mahadevan nuno vasconcelos design robust classifiers computer vision ieee conference computer vision pattern recognition cvpr 2010 nagarajan natarajan inderjit dhillon pradeep ravikumar ambuj tewari learning noisy labels advances neural information processing systems nips pages 1196 1204 2013 ali rahimi benjamin recht random features large scale kernel machines advances neural information processing systems nips pages 1177 1184 2007 mark reid robert c williamson composite binary losses journal machine learning research 11 2387 2422 december 2010 mark reid robert c williamson information divergence risk binary experiments journal machine learning research 12 731 817 mar 2011 bernhard scho lkopf alexander j smola learning kernels volume 129 mit press 2002 rocco servedio pac learning using winnow perceptron perceptron like algorithm conference computational learning theory colt 1999 john shawe taylor nello cristianini kernel methods pattern analysis cambridge uni press 2004 alex smola arthur gretton le song bernhard scho lkopf hilbert space embedding distributions algorithmic learning theory alt 2007 bharath k sriperumbudur kenji fukumizu arthur gretton gert r g lanckriet bernhard scho lkopf kernel choice classifiability rkhs embeddings probability distributions advances neural information processing systems nips 2009 guillaume stempfel liva ralaivola learning svms sloppily labeled data artificial neural networks icann volume 5768 pages 884 893 springer berlin heidelberg 2009 robert tibshirani trevor hastie balasubramanian narasimhan gilbert chu diagnosis multiple cancer types shrunken centroids gene expression proceedings national academy sciences 99 10 6567 6572 2002 oscar wilde importance earnest 1895 9\n",
      "\n",
      "Clean Review # 3\n",
      "one of the central questions in statistical learning theory is to determine the conditions under which agents can learn from experience this includes the necessary and sufficient conditions for generalization from a given finite training set to new observations in this paper we prove that algorithmic stability in the inference process is equivalent to uniform generalization across all parametric loss functions we provide various interpretations of this result for instance a relationship is proved between stability and data processing which reveals that algorithmic stability can be improved by post processing the inferred hypothesis or by augmenting training examples with artificial noise prior to learning in addition we establish a relationship between algorithmic stability and the size of the observation space which provides a formal justification for dimensionality reduction methods finally we connect algorithmic stability to the size of the hypothesis space which recovers the classical pac result that the size complexity of the hypothesis space should be controlled in order to improve algorithmic stability and improve generalization\n",
      "\n",
      "algorithmic stability uniform generalization ibrahim alabdulmohsin king abdullah university science technology thuwal 23955 saudi arabia ibrahim alabdulmohsin kaust edu sa abstract one central questions statistical learning theory determine conditions agents learn experience includes necessary sufficient conditions generalization given finite training set new observations paper prove algorithmic stability inference process equivalent uniform generalization across parametric loss functions provide various interpretations result instance relationship proved stability data processing reveals algorithmic stability improved post processing inferred hypothesis augmenting training examples artificial noise prior learning addition establish relationship algorithmic stability size observation space provides formal justification dimensionality reduction methods finally connect algorithmic stability size hypothesis space recovers classical pac result size complexity hypothesis space controlled order improve algorithmic stability improve generalization 1 introduction one fundamental goal learning algorithm strike right balance underfitting overfitting mathematical terms often translated two separate objectives first would like learning algorithm produce hypothesis reasonably consistent empirical evidence e small empirical risk second would like guarantee empirical risk training error valid estimate true unknown risk test error former condition protects underfitting latter condition protects overfitting rationale behind two objectives understood define generalization risk rgen absolute difference empirical true risks rgen remp rtrue elementary observe true risk rtrue bounded sum remp rgen hence minimizing empirical risk underfitting generalization risk overfitting one obtains inference procedure whose true risk minimal minimizing empirical risk alone carried using empirical risk minimization erm procedure 1 approximations however generalization risk often impossible deal directly instead common practice bound analyticaly establish conditions guaranteed small establishing conditions generalization one hopes design better learning algorithms perform well empirically generalize well novel observations future prominent example approach support vector machines svm algorithm binary classification 2 however bounding generalization risk quite intricate approached various angles fact several methods proposed past prove generalization bounds including uniform convergence algorithmic stability rademacher gaussian complexities generic chaining bounds pac bayesian framework robustness based analysis 1 1 3 4 5 6 7 8 9 concentration measure inequalities form building blocks rich theories proliferation generalization bounds understood look general setting learning introduced vapnik 1 setting observation space z hypothesis space h learning algorithm henceforth denoted l h uses finite set 1 z observations infer hypothesis h h general setting inference process end end influenced three key factors 1 nature observation space z 2 nature hypothesis space h 3 details learning algorithm l imposing constraints three components one may able derive new generalization bounds example vapnik chervonenkis vc theory derives generalization bounds assuming constraints h stability bounds e g 6 10 11 12 derived assuming constraints l given different generalization bounds established imposing constraints z h l intriguing ask exists single view generalization ties different components together paper answer question affirmative establishing algorithmic stability alone equivalent uniform generalization informally speaking inference process said generalize uniformly generalization risk vanishes uniformly across bounded parametric loss functions limit large training sets precise definition presented sequel show constraints imposed either h z l improve uniform generalization interpreted methods improving stability learning algorithm l similar spirit result kearns ron showed finite vc dimension hypothesis space h implies certain notion algorithmic stability inference process 13 statement however general applies learning algorithms fall vapnik general setting learning well beyond uniform convergence rest paper follows first review current literature algorithmic stability generalization learnability introduce key definitions repeatedly used throughout paper next prove central theorem reveals algorithmic stability equivalent uniform generalization provide various interpretations result afterward 2 related work perhaps two fundamental concepts statistical learning theory learnability generalization 12 14 two concepts distinct discussed details next whereas learnability concerned measuring excess risk within hypothesis space generalization concerned estimating true risk order define learnability generalization suppose observation space z probability distribution observations p z bounded stochastic loss function l h z 0 1 h h inferred hypothesis note l implicitly function parameterized h well define true risk hypothesis h h risk functional rtrue h ez p z l z h 1 learning algorithm called consistent true risk inferred hypothesis h converges optimal true risk within hypothesis space h limit large training sets problem called learnable admits consistent learning algorithm 14 known learnability supervised classification regression problems equivalent uniform convergence 3 14 however shalev shwartz et al recently showed uniform convergence necessary vapnik general setting learning proposed algorithmic stability alternative key condition learnability 14 unlike learnability question generalization concerned primarily representative empirical risk remp true risk rtrue elaborate suppose finite training set sm zi 1 comprises observations zi p z define empirical risk hypothesis h respect sm 1 x remp h sm l zi h 2 zi sm also let rtrue h true risk defined eq 1 learning algorithm l said generalize empirical risk inferred hypothesis converges true risk 2 similar learnability uniform convergence definition sufficient generalization 1 necessary learning algorithm always restrict search space smaller subset h artificially speak contrast known whether algorithmic stability necessary generalization shown various notions algorithmic stability defined sufficient generalization 6 10 11 12 15 16 however known whether appropriate notion algorithmic stability defined necessary sufficient generalization vapnik general setting learning paper answer question showing stability inference process sufficient generalization fact equivalent uniform generalization notion generalization stronger one traditionally considered literature 3 preliminaries simplify discussion always assume sets countable including observation space z hypothesis space h similar assumptions used previous works 6 however main results presented section 4 readily generalized addition assume learning algorithms invariant permutations training set hence order training examples irrelevant moreover x p x random variable drawn alphabet x f x function p x write ex p x f x mean x x p x f x often simply write ex f x mean ex p x f x distribution x clear context x takes values finite set uniformly random write x denote distribution x x boolean random variable x 1 x true otherwise x 0 general random variables denoted capital letters instances random variables denoted small letters alphabets denoted calligraphic typeface also given two probability mass functions p q defined alphabet write hp qi denote overlapping p coefficient e intersection p q hp qi min p q note hp qi 1 p q p q total variation distance last write b k φ n nk φk 1 φ n k denote binomial distribution paper consider general setting learning introduced vapnik 1 reiterate observation space z hypothesis space h learning algorithm l receives set observations sm zi 1 z generated fixed unknown distribution p z picks hypothesis h h probability pl h h sm formally l h 1 z stochastic map paper allow hypothesis h summary statistic training set measure central tendency unsupervised learning mapping input space output space supervised learning fact even allow h subset training set formal terms l stochastic map two random variables h h sm z exact interpretation random variables irrelevant learning task assume non negative bounded loss function l z h z 0 1 used measure quality inferred hypothesis h h observation z z importantly assume l h z 0 1 parametric definition 1 parametric loss functions loss function l h z 0 1 called parametric independent training set sm given inferred hypothesis h parametric loss function satisfies markov chain sm h l h fixed hypothesis h h define true risk rtrue h eq 1 define empirical risk training set sm denoted remp h sm eq 2 also define true empirical risks learning algorithm l expected risk inferred hypothesis r true l esm eh pl h sm rtrue h esm eh sm rtrue h 3 r emp l esm eh pl h sm remp h sm esm eh sm remp h sm 4 simplify notation write r true r emp instead r true l r emp l consider following definition generalization definition 2 generalization learning algorithm l h parametric 1 z loss function l h z 0 1 generalizes distribution p z z limm r emp r true 0 r true r emp given eq 3 eq 4 respectively 3 words learning algorithm l generalizes according definition 2 empirical performance training loss becomes unbiased estimator true risk next define uniform generalization definition 3 uniform generalization learning algorithm l h generalizes 1 z uniformly 0 exists m0 0 distributions p z z parametric loss functions sample sizes m0 r emp l r true l uniform generalization stronger original notion generalization definition 2 particular learning algorithm generalizes uniformly generalizes according definition 2 well converse however true even though uniform generalization appears quite strong condition first sight key contribution paper show strong condition equivalent simple condition namely algorithmic stability 4 main results prove algorithmic stability equivalent uniform generalization introduce probabilistic notion mutual stability two random variables order abstract away labeling information random variables might possess e g observation space may may metric space define stability impact observations probability distributions definition 4 mutual stability let x x two random variables mutual stability x defined x hp x p p x ex hp p x ey hp x p x recall 0 hp qi 1 overlapping coefficient two probability distributions p q see x given definition 4 indeed probabilistic measure mutual stability measures stable distribution observing instance x vice versa small value x means probability distribution x heavily perturbed single observation random variable perfect mutual stability achieved two random variables independent probabilistic notion mutual stability mind define stability learning algorithm l mutual stability inferred hypothesis random training example h learning algorithm receives definition 5 algorithmic stability let l 1 z finite set training examples sm zi 1 z drawn fixed distribution p z let h pl h sm hypothesis inferred l let ztrn sm single random training example define stability l l inf p z h ztrn infimum taken possible distributions observations p z learning algorithm called algorithmically stable limm l 1 note definition algorithmic stability rather weak requires contribution single training example overall inference process negligible sample size increases addition well defined even learning algorithm deterministic hypothesis h deterministic function entire training set observations remains stochastic function individual observation illustrate concept following example example 1 suppose observations zi 0 1 bernoulli p trials p zi 1 φ 1 hypothesis produced l empirical average h 1 zi p h k ztrn 1 b k 1 φ 1 p h k ztrn 0 b k φ 1 shown using stirling approximation 17 algorithmic stability learning algorithm asymptotically given l 1 21π achieved φ 1 2 general statement proved later section 5 next show notion algorithmic stability definition 5 equivalent notion uniform generalization definition 3 first state following lemma lemma 1 data processing inequality let b c three random variables satisfy markov chain b c b c 4 proof proof consists two steps 1 first note markov chain implies p c b p c b b c b direct substitution definition 5 second similar information cannot hurt inequality information theory 18 shown b c c random variables b c proved using algebraic manipulation minimum sums always larger thep p fact p α β sum minimums e min min αi βi combining results yields b b c c desired result ready state main result paper theorem 1 learning algorithm l h algorithmic stability given defm 1 z inition 5 necessary sufficient uniform generalization see definition 3 addition r true r emp 1 h ztrn 1 l rtrue remp true empirical risks learning algorithm defined eq 3 4 respectively proof outline proof first parametric loss function l h z 0 1 random variable satisfies markov chain sm h l h independent ztrn sm hence empirical risk given r emp el h eztrn l h l ztrn h contrast true risk given r true el h eztrn p z l ztrn h difference r true r emp el h eztrn l ztrn h eztrn l h l ztrn h sandwich right hand side upper lower bound note p1 z p2 z two distributions defined alphabet z f z 0 1 bounded loss function ez p1 z f z ez p2 z f z p1 z p2 z p q total variation distance proof result immediately deduced considering two regions z z p1 z p2 z z z p1 z p2 z separately used deduce inequalities r true r emp 1 l h ztrn 1 h ztrn 1 l second inequality follows data processing inequality lemma 1 whereas last inequality follows definition algorithmic stability see definition 5 proves l algorithmically stable e l 1 r true r emp converges zero uniformly across parametric loss functions therefore algorithmic stability sufficient uniform generalization converse proved showing bounded δ 0 exists parametric loss distribution pδ z 1 l δ r true r emp 1 l therefore algorithmic stability also necessary uniform generalization 5 interpreting algorithmic stability uniform generalization section provide several interpretations algorithmic stability uniform generalization addition show theorem 1 recovers classical results learning theory 5 1 algorithmic stability data processing relationship algorithmic stability data processing presented lemma 1 given random variables b c markov chain b c always b c presents us qualitative insights design machine learning algorithms first suppose two different hypotheses h1 h2 say h2 contains less informative h1 markov chain sm h1 h2 holds example observations zi 0 1 bernoulli trials h1 r empirical average given example 1 h2 0 1 label occurs often training set h2 h1 2 hypothesis h2 contains strictly less information original training set h1 formally sm h1 h2 case h2 enjoys better uniform generalization bound h1 data processing intuitively know result hold h2 less tied original training set h1 brings us following remark 1 detailed proofs available supplementary file 5 remark 1 improve uniform generalization bound equivalently algorithmic stability learning algorithm post processing inferred hypothesis h manner conditionally independent original training set given h example 2 post processing hypotheses common technique used machine learning includes sparsifying coefficient vector w rd linear methods wj set zero small absolute magnitude also includes methods proposed reduce number support vectors svm exploiting linear dependence 19 data processing inequality methods improve algorithmic stability uniform generalization needless mention better generalization immediately translate smaller true risk empirical risk may increase inferred hypothesis post processed independently original training set second markov chain b c holds also obtain c b c applying data processing inequality reverse markov chain c b result improve algorithmic stability contaminating training examples artificial noise prior learning perturbed version training set sm sm h implies ztrn h z trn h ztrn sm z trn random training examples drawn uniformly random training set respectively brings us following remark remark 2 improve algorithmic stability learning algorithm introducing artificial noise training examples applying learning algorithm perturbed training set example 3 corrupting training examples artificial noise recent dropout method popular techniques neural networks improve generalization 20 data processing inequality methods indeed improve algorithmic stability uniform generalization 5 2 algorithmic stability size observation space next look size observation space z influences algorithmic stability first start following definition definition 6 lazy learning learning algorithm l called lazy hypothesis h h mapped one one training set sm e mapping h sm injective lazy learner called lazy hypothesis equivalent original training set information content hence learning actually takes place one example instance based learning h sm despite simple nature lazy learners useful practice useful theoretical tools well particular equivalence h sm data processing inequality algorithmic stability lazy learner provides lower bound stability possible learning algorithm therefore relate algorithmic stability uniform generalization size observation space quantifying algorithmic stability lazy learners size z usually infinite however introduce following definition effective set size definition 7 countable space z endowed probability mass function p z effective p 2 p size z w r p z defined ess z p z 1 p z 1 p z z z one extreme p z uniform finite alphabet z ess z p z z extreme p z kronecker delta distribution ess z p z 1 proved next notion effective set size determines rate convergence empirical probability mass function true distribution distance measured total variation sense result allows us relate algorithmic stability property observation space z theorem 2 let z countable space endowed probability mass function p z let sm set samples zi p z define psm z empirical probability mass function q induced drawing samples uniformly random sm esm p z psm z ess z p z 1 1 1 ess z p z z effective size z see def2πm inition q 7 addition learning algorithm l 1 z h h ztrn p z 1 1 ess z 1 bound achieved lazy learners see definition 6 2 2πm 2 special case theorem 2 proved de moivre 1730s showed pempirical mean bernoulli trials probability success φ converges true mean rate 2φ 1 φ πm 6 m1 m2 proof outline proof first know p sm m1 p1 p2 2 1 multinomial coefficient using relation p q 2 p q 1 multinomial series de moivre formula mean deviation binomial random variable 22 shown algebraic manipulations 1 x k esm p z psm z 1 pk 1 pk p1 mp k pk 1 pk 1 k 1 2 using stirling approximation factorial 17 obtain simple asymptotic expression r r x 1 2pk 1 pk ess z p z 1 esm p z psm z 1 2 πm 2πm k 1 2 3 tight due tightness stirling approximation rest theorem follows markov chain sm sm h data processing inequality definition 6 corollary 1 given conditions theorem 2 q z addition finite e z learning algorithm l l 1 z 1 2πm 1 proof finite observation space z maximum effective set size see definition 7 z attained uniform distribution p z 1 z intuitively speaking theorem 2 corollary state order guarantee good uniform generalization possible learning algorithms number observations must sufficiently large cover entire effective size observation space z needless mention difficult achieve practice algorithmic stability machine learning algorithms must controlled order guarantee good generalization empirical observations similarly uniform generalization bound improved reducing effective size observation space using dimensionality reduction methods 5 3 algorithmic stability complexity hypothesis space finally look hypothesis space influences algorithmic stability first look role size hypothesis space formalized following theorem theorem 3 denote h h hypothesis inferred learning algorithm l 1 z h following bound algorithmic stability always holds r r h h log h 1 l 1 2m 2m h shannon entropy measured nats e using natural logarithms proof proof information theoretic let x mutual information r v x let sm z1 z2 zm random choice training set hx h sm h h sm h sm h h zi h z1 h h z2 z1 h 1 conditioning reduces entropy e h b h r v b sm h x h zi h zi h h ztrn h ztrn h 1 therefore ztrn h sm h 5 average believed first appearance square root law statistical inference literature 21 effective set size bernoulli distribution according definition 7 given 1 4φ 1 φ theorem 2 agrees fact generalizes de moivre result 7 next use pinsker q inequality 18 states probability distributions p p q q p q p q total variation distance p q 2 kullback leibler divergence measured nats e using natural logarithms recall sm h 1 p sm p h p sm h mutual information sm h p sm h p sm p h deduce pinsker inequality eq 5 ztrn h 1 p ztrn p h p ztrn h r r r r ztrn h sm h h h log h 1 1 1 1 2 2m 2m 2m last line used fact x h x random variables x theorem 3 establishes classical pac result finite hypothesis space 23 terms algorithmic stability learning algorithm enjoy high stability size hypothesis space small terms uniform generalization states generalizationp risk learning algorithm bounded uniformly across parametric loss functions h h 2m p log h 2m h h shannon entropy h next relate algorithmic stability vapnik chervonenkis vc dimension despite fact vc dimension defined binary valued functions whereas algorithmic stability functional probability distributions exists connection two concepts show first introduce notion induced concept class exists learning algorithm l definition 8 concept class c induced learning algorithm l h defined 1 z set total boolean functions c z p ztrn z h p ztrn z h h intuitively every hypothesis h h induces total partition observation space z given boolean function definition 8 h splits z two disjoint sets set values z posteriori less likely present training set given inferred hypothesis h set values complexity richness induced concept class c related algorithmic stability via vc dimension theorem 4 let l h learning algorithm induced concept class c let 1 z dv c c vc dimension c following bound holds dv c c 1 p 4 dv c c 1 log 2m l 1 2m particular l algorithmically stable induced concept class c finite vc dimension proof bounded 1 n proof relies fact algorithmic stability l supp z esm suph h ez p z ch z ez sm ch z ch z p ztrn z h p ztrn z final bound follows applying uniform convergence results 23 6 conclusions paper showed probabilistic notion algorithmic stability equivalent uniform generalization informal terms learning algorithm called algorithmically stable impact single training example probability distribution final hypothesis always vanishes limit large training sets words inference process never depends heavily single training example algorithmic stability holds learning algorithm generalizes well regardless choice parametric loss function also provided several interpretations result instance relationship algorithmic stability data processing reveals algorithmic stability improved either post processing inferred hypothesis augmenting training examples artificial noise prior learning addition established relationship algorithmic stability effective size observation space provided formal justification dimensionality reduction methods finally connected algorithmic stability complexity richness hypothesis space established classical pac result complexity hypothesis space controlled order improve stability hence improve generalization 8 references 1 v n vapnik overview statistical learning theory neural networks ieee transactions vol 10 september 1999 2 c cortes v vapnik support vector networks machine learning vol 20 pp 273 297 1995 3 blumer ehrenfeucht haussler k warmuth learnability vapnikchervonenkis dimension journal acm jacm vol 36 4 pp 929 965 1989 4 talagrand majorizing measures generic chaining annals probability vol 24 3 pp 1049 1103 1996 5 mcallester pac bayesian stochastic model selection machine learning vol 51 pp 5 21 2003 6 bousquet elisseeff stability generalization journal machine learning research jmlr vol 2 pp 499 526 2002 7 p l bartlett mendelson rademacher gaussian complexities risk bounds structural results journal machine learning research jmlr vol 3 pp 463 482 2002 8 j audibert bousquet combining pac bayesian generic chaining bounds journal machine learning research jmlr vol 8 pp 863 889 2007 9 h xu mannor robustness generalization machine learning vol 86 3 pp 391 423 2012 10 elisseeff pontil et al leave one error stability learning algorithms applications nato asi series learning theory practice science series sub series iii computer systems sciences 2002 11 kutin p niyogi almost everywhere algorithmic stability generalization error proceedings eighteenth conference uncertainty artificial intelligence uai 2002 12 poggio r rifkin mukherjee p niyogi general conditions predictivity learning theory nature vol 428 pp 419 422 2004 13 kearns ron algorithmic stability sanity check bounds leave one crossvalidation neural computation vol 11 6 pp 1427 1453 1999 14 shalev shwartz shamir n srebro k sridharan learnability stability uniform convergence journal machine learning research jmlr vol 11 pp 2635 2670 2010 15 l devroye l gyo rfi g lugosi probabilistic theory pattern recognition springer 1996 16 v vapnik chapelle bounds error expectation support vector machines neural computation vol 12 9 pp 2013 2036 2000 17 h robbins remark stirling formula american mathematical monthly pp 26 29 1955 18 cover j thomas elements information theory wiley sons 1991 19 downs k e gates masters exact simplification support vector solutions jmlr vol 2 pp 293 297 2002 20 wager wang p liang dropout training adaptive regularization nips pp 351 359 2013 21 stigler history statistics measurement uncertainty 1900 harvard university press 1986 22 p diaconis zabell closed form summation classical distributions variations theme de moivre statlstlcal science vol 6 3 pp 284 302 1991 23 shalev shwartz ben david understanding machine learning theory algorithms cambridge university press 2014 9\n",
      "\n",
      "Clean Review # 4\n",
      "we develop a sequential low complexity inference procedure for dirichlet process mixtures of gaussians for online clustering and parameter estimation when the number of clusters are unknown a priori we present an easily computable closed form parametric expression for the conditional likelihood in which hyperparameters are recursively updated as a function of the streaming data assuming conjugate priors motivated by large sample asymptotics we propose a noveladaptive low complexity design for the dirichlet process concentration parameter and show that the number of classes grow at most at a logarithmic rate we further prove that in the large sample limit the conditional likelihood and datapredictive distribution become asymptotically gaussian we demonstrate through experiments on synthetic and real data sets that our approach is superior to otheronline state of the art methods\n",
      "\n",
      "adaptive low complexity sequential inference dirichlet process mixture models theodoros tsiligkaridis keith w forsythe massachusetts institute technology lincoln laboratory lexington 02421 usa ttsili mit edu forsythe mit edu abstract develop sequential low complexity inference procedure dirichlet process mixtures gaussians online clustering parameter estimation number clusters unknown priori present easily computable closed form parametric expression conditional likelihood hyperparameters recursively updated function streaming data assuming conjugate priors motivated large sample asymptotics propose novel adaptive low complexity design dirichlet process concentration parameter show number classes grow logarithmic rate prove large sample limit conditional likelihood data predictive distribution become asymptotically gaussian demonstrate experiments synthetic real data sets approach superior online state art methods 1 introduction dirichlet process mixture models dpmm widely used clustering data neal 1992 rasmussen 2000 traditional finite mixture models often suffer overfitting underfitting data due possible mismatch model complexity amount data thus model selection model averaging required find correct number clusters model appropriate complexity requires significant computation high dimensional data sets large samples bayesian nonparametric modeling alternative approaches parametric modeling example dpmm automatically infer number clusters data via bayesian inference techniques use markov chain monte carlo mcmc methods dirichlet process mixtures made inference tractable neal 2000 however methods exhibit slow convergence convergence tough detect alternatives include variational methods blei jordan 2006 deterministic algorithms convert inference optimization approaches take significant computational effort even moderate sized data sets large scale data sets low latency applications streaming data need inference algorithms much faster require multiple passes data work focus lowcomplexity algorithms adapt sample arrive making highly scalable online algorithm learning dpmm based sequential variational approximation sva proposed lin 2013 authors wang dunson 2011 recently proposed sequential maximum posterior map estimator class labels given streaming data algorithm called sequential updating greedy search sugs iteration composed greedy selection step posterior update step choice concentration parameter α critical dpmm controls number clusters antoniak 1974 fast dpmm algorithms use fixed α fearnhead 2004 daume 1 2007 kurihara et al 2006 imposing prior distribution α sampling provides flexibility approach still heavily relies experimentation prior knowledge thus many fast inference methods dirichlet process mixture models proposed adapt α data including works escobar west 1995 learning α incorporated gibbs sampling analysis blei jordan 2006 gamma prior used conjugate manner directly variational inference algorithm wang dunson 2011 also account model uncertainty concentration parameter α bayesian manner directly sequential inference procedure approach computationally expensive discretization domain α needed stability highly depends initial distribution α range values α best knowledge first analytically study evolution stability adapted sequence α online learning setting paper propose adaptive non bayesian approach adapting α motivated largesample asymptotics call resulting algorithm asugs adaptive sugs basic idea behind asugs directly related greedy approach sugs main contribution novel low complexity stable method choosing concentration parameter adaptively new data arrive greatly improves clustering performance derive upper bound number classes logarithmic number samples prove sequence concentration parameters results adaptive design almost bounded finally prove conditional likelihood primary tool used bayesian based online clustering asymptotically gaussian large sample limit implying clustering part asugs asymptotically behaves gaussian classifier experiments show method outperforms state art methods online learning dpmm paper organized follows section 2 review sequential inference framework dpmm build upon introduce notation propose adaptive modification section 3 probabilistic data model given sequential inference steps shown section 4 contains growth rate analysis number classes adaptively designed concentration parameters section 5 contains gaussian large sample approximation conditional likelihood experimental results shown section 6 conclude section 7 2 sequential inference framework dpmm review sugs framework wang dunson 2011 online clustering nonparametric nature dirichlet process manifests modeling mixture models countably infinite components let observations given yi rd γi denote class label ith observation latent variable define available information time y1 yi γ 1 γ1 γi 1 online sequential updating greedy search sugs algorithm summarized next completeness set γ1 1 calculate π θ1 y1 γ1 2 1 choose best class label yi γi arg max1 h ki 1 1 p γi h γ 1 2 update posterior distribution f yi θγi π θγi 1 γ 1 using yi γi π θγi γ θh parameters class h f yi θh observation density conditioned class h ki 1 number classes created time 1 algorithm sequentially allocates observations yi classes based maximizing conditional posterior probability calculate posterior probability p γi h γ 1 define variables def def li h yi p yi γi h 1 γ 1 πi h α p γi h α 1 γ 1 bayes rule p γi h γ 1 li h yi πi h α h 1 ki 1 1 α considered fixed iteration updated fully bayesian manner according dirichlet process prediction predictive probability assigning observation yi class h mi 1 h h 1 ki 1 πi h α 1 α 1 α h ki 1 1 1 α 2 algorithm 1 adaptive sequential updating greedy search asugs input streaming data yi 1 rate parameter λ 0 set γ1 1 k1 1 calculate π θ1 y1 γ1 2 ki 1 update concentration parameter αi 1 λ log 1 n l yi πi h αi 1 b choose best label yi γi qh p 0 h l 0 yi π 0 αi 1 h c update posterior distribution end h h π θγi γ f yi θγi π θγi 1 γ 1 pi 1 mi 1 h l 1 γl h counts number observations labeled class h time 1 α 0 concentration parameter 2 1 adaptation concentration parameter α well known concentration parameter α strong influence growth number classes antoniak 1974 experiments show sequential framework choice α even critical choosing fixed α online sva algorithm lin 2013 requires cross validation computationally prohibitive large scale data sets furthermore streaming data setting estimate data complexity exists impractical perform cross validation although parameter α handled fully bayesian treatment wang dunson 2011 pre specified grid possible values α take say αl l l 1 along prior distribution needs chosen advance storage updating matrix size ki 1 1 l marginalization needed compute p γi h γ 1 iteration thus propose alternative data driven method choosing α works well practice simple compute theoretical guarantees idea start prior distribution α favors small α shape posterior distribution using data define pi α p α γ posterior distribution formed time used asugs time 1 let p1 α p1 α 1 γ 1 denote prior α e g exponential distribution p1 α λe λα dependence γ trivial first step bayes rule pi α p yi γi 1 γ 1 α p α 1 γ 1 pi 1 α πi γi α πi γi α given 1 update made selection γi α used next selection step mean distribution pi α e αi e α γ shown section 5 distribution pi α approximated gamma distribution shape parameter ki rate parameter λ log approximation αi ki λ log requiring storage update one scalar parameter ki iteration asugs algorithm summarized algorithm 1 selection step may implemented sampling probability mass function qh posterior update step efficiently performed updating hyperparameters function streaming data case conjugate distributions section 3 derives updates case multivariate gaussian observations conjugate priors parameters 3 sequential inference unknown mean unknown covariance consider general case unknown mean covariance class probabilistic model parameters class given yi µ n µ µ n µ0 co w δ0 v0 2 n µ denotes multivariate normal distribution mean µ precision matrix w δ v wishart distribution 2δ degrees freedom scale matrix v follow normal wishart joint distribution model 2 leads parameters θ µ rd closed form expressions li h yi due conjugacy tzikas et al 2008 calculate class posteriors conditional likelihoods yi given assignment class h previous class assignments need calculated first conditional likelihood yi given 3 assignment class h history 1 γ 1 given z li h yi f yi θh π θh 1 γ 1 dθh 3 due conjugacy distributions posterior π θh 1 γ 1 always form 1 π θh 1 γ 1 n µh µh 1 1 1 1 ch 1 th w th δh 1 vh 1 µh ch δh vh hyperparameters recursively computed new samples come form recursive computation hyperparameters derived appendix ease interpretation numerical stability define σh w δh vh vh 1 2δh σh inverse mean wishart distribution matrix natural interpretation covariance matrix class h iteration γi th component chosen parameter updates γi th class become µ γi 1 1 1 1 cγi cγi 1 1 cγi µγ 1 4 1 c 1 γi cγi σ γi 1 5 1 2δγi σ 1 1 γi 2δγi δγ δγ 1 1 1 1 2δγi 1 1 cγi yi 1 cγi µγ 1 yi µ 1 γi 1 2 6 7 0 starting matrix σh positive definite matrices σh remain positive definite let us return calculation conditional likelihood 3 iterated integration follows 2 1 1 1 1 2 rh ρd δh det σh li h yi 1 δh 1 21 8 1 2δh rh 1 1 1 1 σh yi µh 1 1 yi µh 2δh def ρd γ 21 γ 1 2 1 def rh 1 ch 1 1 ch detailed mathematical derivation conditional likelihood included appendix b remark new class h ki 1 1 li ki 1 1 form 8 initial choice hyperparameters r 0 δ 0 µ 0 σ 0 4 growth rate analysis number classes stability section derive model posterior distribution pn α using large sample approximations allow us derive growth rates number classes sequence concentration parameters showing number classes grows e kn log1 n arbitarily small certain mild conditions probability density α parameter updated jth step following fashion α innovation class chosen j α pj 1 α pj α 1 otherwise j α α dependent factors update shown α independent factors absorbed normalization probability density choosing innovation class pushes mass toward infinity choosing class pushes mass toward zero thus possibility innovation probability grows undesired manner assess growth number def innovations rn kn 1 simple assumptions likelihood functions appear naturally asugs algorithm assuming initial distribution α p1 α λe λα distribution used step n 1 qn 1 proportional αrn j 1 1 αj 1 e λα make use limiting relation 4 theorem 1 following asymptotic behavior holds limn log qn 1 α j 1 1 j α log n 1 proof see appendix c using theorem 1 large sample model pn α αrn e λ log n α suitably normalized recognizing gamma distribution shape parameter rn 1 rate parameter λ log n rn 1 mean given αn λ log n use mean form choose class membership alg 1 asymptotic approximation leads simple scalar update concentration parameter need discretization tracking evolution continuous probability distributions α experiments approximation accurate recall innovation class labeled k kn 1 1 nth step modeled updates randomly select previous class innovation new class sampling probability distrip k n bution qk p γn k n γ n 1 k 1 note n 1 k6 k mn k mn k represents number members class k time n assume data follows gaussian mixture distribution def pt k x πh n µh σh 9 h 1 πh prior probabilities µh σh parameters gaussian clusters define mixture model probability density function plays role predictive distribution x mn 1 k def l n k ln k 10 n 1 k6 k probabilities choosing previous class innovation using equ 1 proporp mn 1 k αn 1 n 1 tional k6 k n 1 α ln k yn n 1 α l n k yn n 1 α ln k yn respecn 1 n 1 n 1 tively τn 1 denotes innovation probability step n n 1 l n k yn αn 1 ln k yn τn 1 1 τn 1 11 ρn 1 ρn 1 n 1 αn 1 n 1 αn 1 positive proportionality factor ρn 1 define likelihood ratio lr beginning stage n 1 def ln ln k l n k 12 conceptually mixture 10 represents modeled distribution fitting currently observed data modes data observed reasonable expect l n k good model future observations lr ln yn large future observations well modeled 10 fact expect l n k pt n discussed section 5 ln yn αn 1 ln yn αn 1 lemma 1 following bound holds τn 1 n 1 l min 1 n 1 n yn αn 1 proof result follows directly 11 simple calculation innovation random variable rn described random process associated probabilities transition τn k rn 1 p rn 1 k rn 13 1 τn k rn 1 def l0 ln k independent n depends initial choice hyperparameters discussed sec 3 5 expectation rn majorized expectation similar random process r n based def transition probability σn min rna 1 1 instead τn appendix shows random n 1 sequence given ln 1 yn 1 n λ log n latter described modification polya urn process selection probability σn asymptotic behavior rn related variables described following theorem theorem 2 let τn sequence real valued random variables 0 τn 1 satisfying τn rna 1 n n n ln 1 yn 1 1 n λ log n nonnegative integer valued random variables rn evolve according 13 assume following n n 1 ln yn ζ 2 pt k l n k δ p k q kullback leibler divergence distributions p q n rn op log1 ζ δ 2 n αn op logζ δ 2 n 14 proof see appendix e theorem 2 bounds growth rate mean number class innovations concentration parameter αn terms sample size n parameter ζ bounded lr bounded kl divergence conditions thm 2 manifest rate exponents 14 experiments section shows conditions thm 2 hold iterations n n n n fact assuming correct clustering mixture distribution l n kn 1 1 converges true mixture distribution pt implying number class innovations grows log1 n sequence concentration parameters log n 0 arbitrarily small 5 asymptotic normality conditional likelihood section derive asymptotic expression conditional likelihood 8 order gain insight steady state algorithm let πh denote true prior probability class h using bounds gamma function ρd theorem 1 6 batir 2008 follows lima e 2 1 2 2 1 normal convergence conditions algorithm pruning merging steps included classes h 1 k correctly identified populated approximately ni 1 h πh 1 observations time 1 thus conditional class prior class h converges πh ni 1 h π h virtue 14 πi h αi 1 1 α πh according ζ δ 2 1 1 1 rh 1 ch op log 1 1 1 5 expect 1 since πh 1 also expect 2δh 1 1 πh 1 according 7 also ρd δh e 2 δh 1 2 2 1 2 e 2 πh 1 parameter updates 4 7 imply µh µh σh σh 2 2 follows strong law large numbers updates recursive implementations sample mean sample covariance matrix thus large sample approximation conditional likelihood becomes 1 1 1 πh 1 1 1 1 2π h µ σ µ lim 1 h h h 1 li h yi 1 1 2 limi det σh 1 1 e 2 yi µh σh yi µh det σh 15 used limu 1 uc u ec conditional likelihood 15 corresponds multivariate gaussian distribution mean µh covariance matrix σh similar asymptotic normality 6 result recently obtained tsiligkaridis forsythe 2015 gaussian observations von n n h mises prior asymptotics mn 1 πh µh µh σh σh ln h n µh σh n 1 n imply mixture distribution l n k 10 converges true gaussian mixture distribution pt 9 thus small δ expect pt k l n k δ n n validating assumption theorem 2 6 experiments apply asugs learning algorithm synthetic 16 class example real data set verify stability accuracy method experiments show value adaptation dirichlet concentration parameter online clustering parameter estimation since possible multiple clusters similar classes might created due outliers due particular ordering streaming data sequence add pruning merging step asugs algorithm done lin 2013 compare asugs asugs pm sugs sugs pm sva sva pm proposed lin 2013 since shown lin 2013 sva sva pm outperform block based methods perform iterative updates entire data set including collapsed gibbs sampling mcmc split merge truncation free variational inference 6 1 synthetic data set consider learning parameters 16 class gaussian mixture equal variance σ 2 0 025 training set made 500 iid samples test set made 1000 iid samples clustering results shown fig 1 showing asugs based approaches stable sva based algorithms asugs pm performs best identifies correct number clusters parameters fig 1 b shows data log likelihood test set averaged 100 monte carlo trials mean variance number classes iteration asugs based approaches achieve higher log likelihood sva based approaches asymptotically fig 6 1 provides numerical verification assumptions theorem 2 expected predictive likelihood l k 10 converges true mixture distribution pt 9 likelihood ratio li yi bounded enough samples processed sva pm 0 0 2 2 4 4 2 0 2 4 4 4 2 2 asugs 0 2 4 asugs pm 4 4 2 2 0 0 2 2 25 mean number classes 2 avg joint log likelihood 4 2 20 4 15 6 8 10 2 0 2 4 4 4 2 0 2 4 5 0 0 4 4 asugs asugs pm sugs sugs pm sva sva pm 10 100 200 300 iteration 400 500 0 100 200 300 iteration 400 variance number classes sva 4 500 5 4 3 2 1 0 0 100 200 300 400 500 iteration b figure 1 clustering performance sva sva pm asugs asugs pm synthetic data set asugs pm identifies 16 clusters correctly b joint log likelihood synthetic data mean variance number classes function iteration likelihood values evaluated held set 1000 samples asugs pm achieves highest log likelihood lowest asymptotic variance number classes 6 2 real data set applied online nonparametric bayesian methods clustering image data used mnist data set consists 60 000 training samples 10 000 test samples sample 7 10000 3 9000 2 5 k pt k2 kl 2 8000 li yi 7000 2 6000 5000 1 5 4000 1 3000 2000 0 5 1000 0 0 100 200 300 400 500 sample l 0 100 200 300 400 500 sample figure 2 likelihood ratio li yi l k yi left l2 distance l k true k mixture distribution pt right synthetic example see 1 28 28 image handwritten digit total 784 dimensions perform pca preprocessing reduce dimensionality 50 dimensions kurihara et al 2006 use random 1 667 subset consisting 1000 random samples training training set contains data 10 digits approximately uniform proportion fig 3 shows predictive log likelihood test set mean images clusters obtained using asugspm sva pm respectively note asugs pm achieves higher log likelihood values finds digits correctly using 23 clusters sva pm finds digits using 56 clusters 0 predictive log likelihood 500 asugs pm sugs pm sva pm 1000 1500 2000 2500 3000 3500 4000 4500 5000 0 100 200 300 400 500 600 700 800 900 1000 iteration b c figure 3 predictive log likelihood test set mean images clusters found using asugs pm b sva pm c mnist data set 6 3 discussion although sva asugs methods similar computational complexity use decisions information obtained processing previous samples order decide class innovations mechanics methods quite different asugs uses adaptive α motivated asymptotic theory sva uses fixed α furthermore sva updates parameters components iteration weighted fashion asugs updates parameters likely cluster thus minimizing leakage unrelated components λ parameter asugs affect performance much threshold parameter sva often leads instability requiring lots pruning merging steps increasing latency critical large data sets streaming applications cross validation would required set appropriately observe higher log likelihoods better numerical stability asugs based methods comparison sva mathematical formulation asugs allows theoretical guarantees theorem 2 asymptotically normal predictive distribution 7 conclusion developed fast online clustering parameter estimation algorithm dirichlet process mixtures gaussians capable learning single data pass motivated large sample asymptotics proposed novel low complexity data driven adaptive design concentration parameter showed leads logarithmic growth rates number classes experiments synthetic real data sets show method achieves better performance fast state art online learning dpmm methods 8 references antoniak c e mixtures dirichlet processes applications bayesian nonparametric problems annals statistics 2 6 1152 1174 1974 batir n inequalities gamma function archiv der mathematik 91 6 554 563 2008 blei jordan variational inference dirichlet process mixtures bayesian analysis 1 1 121 144 2006 daume h fast search dirichlet process mixture models conference artificial intelligence statistics 2007 escobar west bayesian density estimation inference using mixtures journal american statistical association 90 430 577 588 june 1995 fearnhead p particle filters mixture models uknown number components statistics computing 14 11 21 2004 kurihara k welling vlassis n accelerated variational dirichlet mixture models advances neural information processing systems nips 2006 lin dahua online learning nonparametric mixture models via sequential variational approximation burges c j c bottou l welling ghahramani z weinberger k q eds advances neural information processing systems 26 pp 395 403 curran associates inc 2013 neal r bayesian mixture modeling proceedings workshop maximum entropy bayesian methods statistical analysis volume 11 pp 197 211 1992 neal r markov chain sampling methods dirichlet process mixture models journal computational graphical statistics 9 2 249 265 june 2000 rasmussen c e infinite gaussian mixture model advances neural information processing systems 12 pp 554 560 mit press 2000 tsiligkaridis forsythe k w sequential bayesian inference framework blind frequency offset estimation proceedings ieee international workshop machine learning signal processing boston september 2015 tzikas g likas c galatsanos n p variational approximation bayesian inference ieee signal processing magazine pp 131 146 november 2008 wang l dunson b fast bayesian inference dirichlet process mixture models journal computational graphical statistics 20 1 196 216 2011 9\n",
      "\n",
      "Clean Review # 5\n",
      "monte carlo sampling for bayesian posterior inference is a common approach used in machine learning the markov chain monte carlo procedures that are used are often discrete time analogues of associated stochastic differential equations sdes these sdes are guaranteed to leave invariant the required posterior distribution an area of current research addresses the computational benefits of stochastic gradient methods in this setting existing techniques rely on estimating the variance or covariance of the subsampling error and typically assume constant variance in this article we propose a covariance controlled adaptive langevin thermostat that can effectively dissipate parameter dependent noise while maintaining a desired target distribution the proposed method achieves a substantial speedup over popular alternative schemes for large scale machine learning applications\n",
      "\n",
      "covariance controlled adaptive langevin thermostat large scale bayesian sampling xiaocheng shang university edinburgh x shang ed ac uk zhanxing zhu university edinburgh zhanxing zhu ed ac uk benedict leimkuhler university edinburgh b leimkuhler ed ac uk amos j storkey university edinburgh storkey ed ac uk abstract monte carlo sampling bayesian posterior inference common approach used machine learning markov chain monte carlo procedures used often discrete time analogues associated stochastic differential equations sdes sdes guaranteed leave invariant required posterior distribution area current research addresses computational benefits stochastic gradient methods setting existing techniques rely estimating variance covariance subsampling error typically assume constant variance article propose covariance controlled adaptive langevin thermostat effectively dissipate parameter dependent noise maintaining desired target distribution proposed method achieves substantial speedup popular alternative schemes large scale machine learning applications 1 introduction machine learning applications direct sampling entire large scale dataset computationally infeasible instance standard markov chain monte carlo mcmc methods 16 well typical hybrid monte carlo hmc methods 3 6 9 require calculation acceptance probability creation informed proposals based whole dataset order improve computational efficiency number stochastic gradient methods 4 5 20 21 proposed setting bayesian sampling based random much smaller subsets approximate likelihood whole dataset thus substantially reducing computational cost practice welling teh proposed called stochastic gradient langevin dynamics sgld 21 combining ideas stochastic optimization 18 traditional brownian dynamics sequence stepsizes decreasing zero fixed stepsize often adopted practice choice article vollmer et al 20 modified sgld msgld also introduced designed reduce sampling bias sgld generates samples first order brownian dynamics thus fixed timestep one show unable dissipate excess noise gradient approximations maintaining desired invariant distribution 4 stochastic gradient hamiltonian monte carlo sghmc method proposed chen et al 4 relies second order langevin dynamics incorporates parameter dependent diffusion matrix intended effectively offset stochastic perturbation gradient however difficult accommodate additional diffusion term practice first second authors contributed equally listed author order decided lot 1 moreover pointed 5 poor estimation may significant adverse influence sampling target distribution example effective system temperature may altered thermostat idea widely used molecular dynamics 7 13 recently adopted stochastic gradient nose hoover thermostat sgnht ding et al 5 order adjust kinetic energy simulation way canonical ensemble preserved e prescribed constant temperature distribution maintained fact sgnht method essentially equivalent adaptive langevin ad langevin thermostat proposed earlier jones leimkuhler 10 molecular dynamics setting see 15 discussions despite substantial interest generated methods mathematical foundation stochastic gradient methods incomplete underlying dynamics sgnht method 5 taken leimkuhler shang 15 together design discretization schemes high effective order accuracy sgnht methods designed based assumption constant noise variance article propose covariance controlled adaptive langevin ccadl thermostat handle parameter dependent noise improving robustness reliability practice effectively speed convergence desired invariant distribution large scale machine learning applications rest article organized follows section 2 describe setting bayesian sampling noisy gradients briefly review existing techniques section 3 considers construction novel ccadl method effectively dissipate parameter dependent noise maintaining correct distribution various numerical experiments performed section 4 verify usefulness ccadl wide range large scale machine learning applications finally summarize findings section 5 2 bayesian sampling noisy gradients typical setting bayesian sampling 3 19 one interested drawing states posterior distribution defined π θ x π x θ π θ 1 θ rnd parameter vector interest x denotes entire dataset π x θ π θ likelihood prior distributions respectively introduce potential energy function u θ defining π θ x exp βu θ β positive parameter interpreted proportional reciprocal temperature associated physical system e β 1 kb kb boltzmann constant temperature practice β often set unity notational simplicity taking logarithm 1 yields u θ log π x θ log π θ 2 assuming data independent identically distributed logarithm likelihood calculated n x log π x θ log π xi θ 3 1 n size entire dataset however already mentioned computationally infeasible deal entire large scale dataset timestep would typically required mcmc hmc methods instead order improve efficiency random much smaller e n n subset preferred stochastic gradient methods likelihood dataset given parameters approximated n nx log π x θ log π xri θ 4 n 1 xri ni 1 represents random subset x thus noisy potential energy written n nx u θ log π xri θ log π θ 5 n 1 negative gradient potential referred noisy force e f θ u θ 2 goal correctly sample gibbs distribution ρ θ exp βu θ 1 4 5 gradient noise assumed gaussian mean zero unknown variance case one may rewrite noisy force p f θ u θ σ θ m1 2 r 6 typically diagonal matrix σ θ represents covariance matrix noise p r vector standard normal random variables note σ θ m1 2 r actually equivalent n 0 σ θ typical setting numerical integration associated stepsize h one p p hf θ h u θ σ θ m1 2 r h u θ h hς θ m1 2 r 7 2 therefore assuming constant covariance matrix e σ σ identity matrix sgnht method ding et al 5 following underlying dynamics written standard ito stochastic differential equation sde system 15 dθ 1 pdt p 8 dp u θ dt σ hm1 2 dw ξpdt 2aβ 1 m1 2 dwa 1 1 dξ µ p p nd kb dt colloquially dw dwa represent vectors independent wiener increments p often informally denoted n 0 dti 4 coefficient 2aβ 1 m1 2 represents strength artificial noise added system improve ergodicity termed effective friction positive parameter proportional variance noise auxiliary variable ξ r governed nose hoover device 8 17 via negative feedback mechanism e instantaneous temperature average kinetic energy per degree freedom calculated kb pt 1 p nd 9 target temperature dynamical friction ξ would decrease allowing increase temperature ξ would increase temperature target µ coupling parameter referred thermal mass molecular dynamics setting proposition 1 see jones leimkuhler 10 sgnht method 8 preserves modified gibbs stationary distribution 2 2 ρ β θ p ξ z 1 exp βh θ p exp βµ ξ ξ 10 z normalizing constant h θ p pt 1 p 2 u θ hamiltonian ξ βhσ 2 2 11 proposition 1 tells us sgnht method adaptively dissipate excess noise pumped system maintaining correct distribution variance gradient noise σ 2 need known priori long σ 2 constant auxiliary variable ξ able automatically find mean value ξ fly however parameter dependent covariance matrix σ θ sgnht method 8 would produce required target distribution 10 ding et al 5 claimed reasonable assume covariance matrix σ θ constant size dataset n large case variance posterior θ small magnitude posterior variance actually relate constancy σ however general σ constant simply assuming non constancy σ significant impact performance method notably stability measured largest usable stepsize therefore essential approach handle parameter dependent noise following section propose covariance controlled thermostat effectively dissipate parameter dependent noise maintaining target stationary distribution 3 covariance controlled adaptive langevin thermostat mentioned previous section sgnht method 8 dissipate noise constant covariance matrix covariance matrix becomes parameter dependent general parameter dependent covariance matrix imply required thermal equilibrium e system cannot expected converge desired invariant distribution 10 typically resulting poor estimation functions parameters interest fact case clear whether exists invariant distribution 3 order construct stochastic dynamical system preserves canonical distribution suggest adding suitable damping viscous term effectively dissipate parameter dependent gradient noise end propose following covariance controlled adaptive langevin ccadl thermostat dθ 1 pdt p p dp u θ dt hς θ m1 2 dw h 2 βς θ pdt ξpdt 2aβ 1 m1 2 dwa 12 dξ µ 1 pt 1 p nd kb dt proposition 2 ccadl thermostat 12 preserves modified gibbs stationary distribution ρ β θ p ξ z 1 exp βh θ p exp βµ ξ 2 2 13 proof fokker planck equation corresponding 12 ρt l ρ 1 p θ ρ u θ p ρ h 2 p σ θ p ρ h 2 β p σ θ pρ ξ p pρ aβ 1 p p ρ µ 1 pt 1 p nd kb ξ ρ insert ρ β 13 fokker planck operator l see vanishes incorporation parameter dependent covariance matrix σ θ 12 intended offset covariance matrix coming gradient approximation however practice one know σ θ priori thus instead one must estimate σ θ simulation task addressed section 3 1 procedure related method used sghmc method proposed chen et al 4 uses dynamics following form dθ 1 pdt p p 14 dp u θ dt hς θ m1 2 dw apdt 2β 1 ai βhς θ 2 m1 2 dwa shown sghmc method preserves gibbs canonical distribution ρβ θ p z 1 exp βh θ p 15 although ccadl 12 sghmc 14 preserve respective invariant distributions let us note several advantages former latter practice ccadl sghmc require estimation covariance matrix σ θ simulation costly high dimension numerical experiments found simply using diagonal covariance matrix significantly reduced computational cost works quite well ccadl contrast difficult find suitable value parameter sghmc since one make sure matrix ai βhς θ 2 positive semi definite one may attempt use large value effective friction small stepsize h however large friction would essentially reduce sghmc sgld desirable pointed 4 extremely small stepsize would significantly impact computational efficiency ii estimation covariance matrix σ θ unavoidably introduces additional noise ccadl sghmc nonetheless ccadl still effectively control system temperature e maintaining correct distribution momenta due use stabilizing nose hoover control sghmc poor estimation covariance matrix may lead significant deviations system temperature well distribution momenta resulting poor sampling parameters interest 3 1 covariance estimation noisy gradients assumption noise stochastic gradient follows normal distribution apply similar method 2 estimate covariance matrix associated noisy gradient let g θ x θ log π x θ assume size subset n large enough central limit theorem hold n 1 1x g θ xri n ex g θ x 16 n 1 n cov g θ x covariance gradient θ given noisy stochastic pn gradient based current subset u θ n 1 g θ xri log π θ clean n 4 algorithm 1 covariance controlled adaptive langevin ccadl thermostat 1 2 3 4 5 6 7 8 input h κt 1 initialize θ 0 p0 i0 ξ0 1 2 θ θ 1 pt 1 h estimate using eq 18 2 pt pt 1 u θ h h2 nn pt 1 h ξt 1 pt 1 h 2ahn 0 ξt ξt 1 ptt pt nd 1 h end full gradient u θ thus pn g θ xi log π θ ex u θ ex u θ n2 17 u θ u θ n 0 n e σ θ n 2 n assuming θ change dramatically time use moving average update estimate 1 κt 1 κt v θ 18 κt 1 n 1 x v θ g θ xri g θ g θ xri g θ 19 n 1 1 empirical covariance gradient g θ represents mean gradient log likelihood computed subset proved 2 estimator convergence order 1 n 1 already mentioned estimating full covariance matrix computationally infeasible high dimension however found employing diagonal approximation covariance matrix e estimating variance along dimension noisy gradient works quite well practice demonstrated section 4 procedure ccadl method summarized algorithm 1 simply used β 1 µ nd order consistent original implementation sgnht 5 note simple first order terms stepsize algorithm recent article 15 introduced higher order accuracy schemes improve accuracy interest direct comparison underlying machinery sghmc sgnht ccadl avoid modifications enhancements related timestepping stage following section compare newly established ccadl method sghmc sgnht various machine learning tasks demonstrate benefits ccadl bayesian sampling noisy gradient 4 4 1 numerical experiments bayesian inference gaussian distribution first compare performance newly established ccadl method sghmc sgnht simple task using synthetic data e bayesian inference mean variance one dimensional normal distribution apply experimental setting 5 generated n 100 samples standard normal distribution n 0 1 used likelihood function n xi µ γ 1 assigned normal gamma distribution prior distribution e µ γ n µ 0 γ gam γ 1 1 corresponding posterior distribution another normalgamma distribution e µ γ x n µ µn κn γ 1 gam γ αn βn n x n xi x 2 n x 2 n x κn 1 n αn 1 βn 1 µn n 1 2 2 2 1 n 1 pn x 1 xi n random subset size n 10 selected timestep approximate full gradient resulting following stochastic gradients n n γn x n 1 µ2 n x µ u n 1 µγ x ri γ u 1 xr µ 2 n 1 2γ 2 2n 1 5 seen variance stochastic gradient noise longer constant actually depends size subset n values µ γ iteration directly violates constant noise variance assumption sgnht 5 ccadl adjusts varying noise variance marginal distributions µ γ obtained various methods different combinations h compared plotted figure 1 table 1 consisting corresponding root mean square error rmse distribution autocorrelation time 106 samples cases sgnht ccadl easily outperform sghmc method possibly due presence nose hoover device sghmc showing superiority small value h large value neither desirable practice discussed section 3 sgnht newly proposed ccadl method latter achieves better performance cases investigated highlighting importance covariance control parameterdependent noise 3 3 2 2 1 1 1 0 0 5 0 0 5 0 0 5 2 0 µ 0 5 true sghmc sgnht ccadl 3 2 1 1 0 0 5 0 0 5 0 0 5 1 γ 1 5 h 0 001 1 1 γ 0 µ 1 5 b h 0 001 10 0 5 true sghmc sgnht ccadl 2 1 4 true sghmc sgnht ccadl 3 2 1 3 density true sghmc sgnht ccadl 3 density 0 5 density 0 µ true sghmc sgnht ccadl density 2 4 true sghmc sgnht ccadl 0 0 5 0 µ 0 5 true sghmc sgnht ccadl 3 density density 3 4 density true sghmc sgnht ccadl density 4 2 1 1 γ 1 5 c h 0 01 1 0 0 5 1 γ 1 5 h 0 01 10 figure 1 comparisons marginal distribution density µ top row γ bottom row various values h indicated column peak region highlighted inset table 1 comparisons rmse autocorrelation time µ γ various methods bayesian inference mean variance gaussian distribution methods h 0 001 1 h 0 001 10 h 0 01 1 h 0 01 10 sghmc 0 0148 236 12 0 0029 333 04 0 0531 29 78 0 0132 39 33 sgnht 0 0037 238 32 0 0035 406 71 0 0044 26 71 0 0043 55 00 ccadl 0 0034 238 06 0 0031 402 45 0 0021 26 71 0 0035 54 43 4 2 large scale bayesian logistic regression consider bayesian logistic regression model trained benchmark mnist dataset binary classification digits 7 9 using 12 214 training data points test set size 2037 100 dimensional random projection original features used used likeliqn hood function π xi yi n 1 w 1 1 1 exp yi w xi prior distribution π w exp w w 2 subset size n 500 used timestep since dimensionality problem high full covariance estimation used ccadl investigate figure 2 top row convergence speed method measuring test log likelihood using posterior mean number passes entire dataset ccadl displays significant improvements sghmc sgnht different values h 1 ccadl converges much faster two also indicates faster mixing speed shorter burn period 2 ccadl shows robustness different values effective friction sghmc sgnht relying relative large value especially sghmc method intended dominate gradient noise compare sample quality obtained method figure 2 bottom row plots twodimensional marginal posterior distribution randomly selected dimensions 2 5 based 106 samples method burn period e start collect samples test 6 log likelihood stabilizes true reference distribution obtained sufficiently long run standard hmc implemented 10 runs standard hmc found variation runs guarantees qualification true reference distribution ccadl shows much better performance sghmc sgnht note contour sghmc even fit region plot fact shows significant deviation even estimation mean 500 sghmc 1 sghmc 10 sgnht 1 sgnht 10 ccadl 1 ccadl 10 700 800 0 200 400 number passes 500 600 700 800 600 0 3 5 0 800 300 0 10 5 300 true hmc sghmc sgnht ccadl 15 10 5 0 5 0 03 0 035 0 04 0 045 0 05 0 055 w2 h 0 2 10 4 100 200 number passes x 10 true hmc sghmc sgnht ccadl 0 5 0 03 0 035 0 04 0 045 0 05 0 055 w2 sghmc 1 sghmc 10 sgnht 1 sgnht 10 ccadl 1 ccadl 10 700 3 15 w5 5 w 600 x 10 true hmc sghmc sgnht ccadl 10 100 200 number passes 500 3 x 10 15 sghmc 1 sghmc 10 sgnht 1 sgnht 10 ccadl 1 ccadl 10 400 w5 600 300 400 test log likelihood 300 400 test log likelihood test log likelihood 300 b h 0 5 10 4 5 0 03 0 035 0 04 0 045 0 05 0 055 w2 c h 1 10 4 figure 2 comparisons bayesian logistic regression various methods mnist dataset digits 7 9 various values h top row test log likelihood using posterior mean number passes entire dataset bottom row two dimensional marginal posterior distribution randomly selected dimensions 2 5 10 fixed based 106 samples method burn period e start collect samples test log likelihood stabilizes magenta circle true reference posterior mean obtained standard hmc crosses represent sample means computed various methods ellipses represent iso probability contours covering 95 probability mass note contour sghmc well beyond scale plot especially large stepsize regime case include 4 3 discriminative restricted boltzmann machine drbm drbm 11 self contained non linear classifier gradient discriminative objective explicitly computed due limited space refer readers 11 details trained drbm different large scale multi class datasets libsvm1 dataset collection including connect 4 letter sensit vehicle acoustic detailed information datasets presented table 2 selected number hidden units using cross validation achieve best results since dimension parameters nd relatively high used diagonal covariance matrix estimation ccadl significantly reduce computational cost e estimating variance along dimension size subset chosen 500 1000 obtain reasonable variance estimation dataset chose first 20 total number passes entire dataset burn period collected remaining samples prediction table 2 datasets used drbm corresponding parameter configurations datasets connect 4 letter acoustic training test set 54 046 13 511 10 500 5 000 78 823 19 705 classes 3 26 3 features 126 16 50 hidden units 20 100 20 total number parameters nd 2603 4326 1083 error rates computed various methods test set using posterior mean number passes entire dataset plotted figure 3 observed sghmc sgnht work well large value effective friction corresponds strong random walk effect thus slows convergence contrary ccadl works 1 http www csie ntu edu tw cjlin libsvmtools datasets multiclass html 7 reliably much better two wide range importantly large stepsize regime speeds convergence rate relation computational work performed easily seen performance sghmc heavily relies using small value h large value significantly limits usefulness practice 0 29 0 29 0 28 0 27 0 27 50 100 150 number passes 200 test error 0 15 0 1 sghmc 1 sghmc 10 sgnht 1 sgnht 10 ccadl 1 ccadl 10 100 100 150 number passes 0 3 50 100 150 number passes 3a acoustic h 0 2 10 200 3 100 150 number passes 200 sghmc 1 sghmc 10 sgnht 1 sgnht 10 ccadl 1 ccadl 10 0 25 0 2 0 15 0 1 200 300 number passes 0 4 400 100 0 3 50 100 150 number passes 3b acoustic h 0 5 10 200 300 number passes 400 2c letter h 5 10 3 sghmc 1 sghmc 10 sgnht 1 sgnht 10 ccadl 1 ccadl 10 0 35 0 25 50 1c connect 4 h 2 10 3 2b letter h 2 10 3 sghmc 1 sghmc 10 sgnht 1 sgnht 10 ccadl 1 ccadl 10 0 35 0 15 100 test error 0 4 0 3 0 29 0 27 200 sghmc 1 sghmc 10 sgnht 1 sgnht 10 ccadl 1 ccadl 10 0 2 400 2a letter h 1 10 3 0 25 50 0 1 200 300 number passes 0 31 0 28 0 25 0 2 sghmc 10 sghmc 50 sgnht 10 sgnht 50 ccadl 10 ccadl 50 0 32 1b connect 4 h 1 10 3 0 25 test error 0 3 0 28 1a connect 4 h 0 5 10 3 test error 0 31 0 33 test error 0 3 sghmc 10 sghmc 50 sgnht 10 sgnht 50 ccadl 10 ccadl 50 0 32 200 3 0 4 test error test error 0 31 0 33 test error sghmc 10 sghmc 50 sgnht 10 sgnht 50 ccadl 10 ccadl 50 0 32 test error 0 33 sghmc 1 sghmc 10 sgnht 1 sgnht 10 ccadl 1 ccadl 10 0 35 0 3 0 25 50 100 150 number passes 200 3 3c acoustic h 1 10 figure 3 comparisons drbm datasets connect 4 top row letter middle row acoustic bottom row various values h indicated test error rates various methods using posterior mean number passes entire dataset 5 conclusions future work article proposed novel ccadl formulation effectively dissipate parameter dependent noise maintaining desired invariant distribution ccadl combines ideas sghmc sgnht literature achieves significant improvements methods practice additional error introduced covariance estimation expected small relative sense e substantially smaller error arising noisy gradient findings verified large scale machine learning applications particular consistently observed sghmc relies small stepsize h large friction significantly reduces usefulness practice discussed techniques presented article could use general settings large scale bayesian sampling optimization leave future work naive nonsymmetric splitting method applied ccadl fair comparison article however point optimal design splitting methods ergodic sde systems explored recently mathematics community 1 13 14 moreover shown 15 certain type symmetric splitting method ad langevin sgnht method clean full gradient inherits superconvergence property e fourth order convergence invariant distribution configurational quantities recently demonstrated setting langevin dynamics 12 14 leave exploration direction context noisy gradients future work 8 references 1 abdulle g vilmart k c zygalakis long time accuracy lie trotter splitting methods langevin dynamics siam journal numerical analysis 53 1 1 16 2015 2 ahn korattikara welling bayesian posterior sampling via stochastic gradient fisher scoring proceedings 29th international conference machine learning pages 1591 1598 2012 3 brooks gelman g jones x l meng handbook markov chain monte carlo crc press 2011 4 chen e b fox c guestrin stochastic gradient hamiltonian monte carlo proceedings 31st international conference machine learning pages 1683 1691 2014 5 n ding fang r babbush c chen r skeel h neven bayesian sampling using stochastic gradient thermostats advances neural information processing systems 27 pages 3203 3211 2014 6 duane kennedy b j pendleton roweth hybrid monte carlo physics letters b 195 2 216 222 1987 7 frenkel b smit understanding molecular simulation algorithms applications second edition academic press 2001 8 w g hoover computational statistical mechanics studies modern thermodynamics elsevier science 1991 9 horowitz generalized guided monte carlo algorithm physics letters b 268 2 247 252 1991 10 jones b leimkuhler adaptive stochastic methods sampling driven molecular systems journal chemical physics 135 8 084125 2011 11 h larochelle bengio classification using discriminative restricted boltzmann machines proceedings 25th international conference machine learning pages 536 543 2008 12 b leimkuhler c matthews rational construction stochastic numerical methods molecular sampling applied mathematics research express 2013 1 34 56 2013 13 b leimkuhler c matthews molecular dynamics deterministic stochastic numerical methods springer 2015 14 b leimkuhler c matthews g stoltz computation averages equilibrium nonequilibrium langevin molecular dynamics ima journal numerical analysis 36 1 13 79 2016 15 b leimkuhler x shang adaptive thermostats noisy gradient systems siam journal scientific computing 2016 16 n metropolis w rosenbluth n rosenbluth h teller e teller equation state calculations fast computing machines journal chemical physics 21 6 1087 1953 17 nose unified formulation constant temperature molecular dynamics methods journal chemical physics 81 1 511 1984 18 h robbins monro stochastic approximation method annals mathematical statistics 22 2 400 407 1951 19 c robert g casella monte carlo statistical methods second edition springer 2004 20 j vollmer k c zygalakis w teh non asymptotic properties stochastic gradient langevin dynamics arxiv preprint arxiv 1501 00438 2015 21 welling w teh bayesian learning via stochastic gradient langevin dynamics proceedings 28th international conference machine learning pages 681 688 2011 9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Inspect the cleaned summaries and texts to ensure they have been cleaned well\n",
    "for i in range(5):\n",
    "    print(\"Clean Review #\",i+1)\n",
    "    print(clean_summaries[i]+'\\n')\n",
    "    print(clean_texts[i])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count the number of occurrences of each word in a set of text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(count_dict, text):\n",
    "    for sentence in text:\n",
    "        for word in sentence.split():\n",
    "            if word not in count_dict:\n",
    "                count_dict[word] = 1\n",
    "            else:\n",
    "                count_dict[word] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Give the function a try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'that': 1, 'is': 1, 'a': 2, 'great': 4, 'dog': 2, 'you': 1, 'have': 1}"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {}\n",
    "count_words(mydict, [\"that is a great great great dog\",\"you have a great dog\"])\n",
    "mydict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Vocabulary: 47962\n"
     ]
    }
   ],
   "source": [
    "word_counts = {}\n",
    "count_words(word_counts, clean_summaries)\n",
    "count_words(word_counts, clean_texts)\n",
    "print(\"Size of Vocabulary:\", len(word_counts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how may \"hero\" occurs in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "603"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_counts[\"computer\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Load Conceptnet Numberbatch's (CN) embeddings, similar to GloVe, but probably better \n",
    " (https://github.com/commonsense/conceptnet-numberbatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 417195\n"
     ]
    }
   ],
   "source": [
    "\n",
    "embeddings_index = {}\n",
    "with open('/Users/apple/Desktop/601Project-master/numberbatch-en-17.06.txt', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.split(' ')\n",
    "        word = values[0]\n",
    "        embedding = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = embedding\n",
    "\n",
    "print('Word embeddings:', len(embeddings_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at the CN embedding dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings_index[\"test\"].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the number of words that are missing from CN, and are used more than our threshold.\n",
    "\n",
    "I use a **threshold** of 20, so that words not in CN can be added to our **word_embedding_matrix**, but they need to be common enough in the reviews so that the model can understand their meaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words missing from CN: 1266\n",
      "Percent of words that are missing from vocabulary: 2.64%\n"
     ]
    }
   ],
   "source": [
    "missing_words = 0\n",
    "threshold = 20\n",
    "\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold:\n",
    "        if word not in embeddings_index:\n",
    "            missing_words += 1\n",
    "            \n",
    "missing_ratio = round(missing_words/len(word_counts),4)*100\n",
    "            \n",
    "print(\"Number of words missing from CN:\", missing_words)\n",
    "print(\"Percent of words that are missing from vocabulary: {}%\".format(missing_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are those missing words in the CN\n",
    "Looks mostly products' brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('servedio', 25),\n",
       " ('2008', 449),\n",
       " ('sln', 91),\n",
       " ('svm', 287),\n",
       " ('dirichlet', 245),\n",
       " ('gaussians', 117),\n",
       " ('hyperparameters', 149),\n",
       " ('subsampling', 76),\n",
       " ('langevin', 69),\n",
       " ('quantifies', 40),\n",
       " ('multiclass', 127),\n",
       " ('leverages', 30),\n",
       " ('convolutional', 748),\n",
       " ('crcn', 27),\n",
       " ('outperforms', 309),\n",
       " ('kwikcluster', 25),\n",
       " ('clusterwild', 49),\n",
       " ('speedups', 35),\n",
       " ('32', 575),\n",
       " ('objectness', 25),\n",
       " ('vgg', 90),\n",
       " ('16', 1615),\n",
       " ('2007', 449),\n",
       " ('73', 127),\n",
       " ('2012', 1035),\n",
       " ('70', 230),\n",
       " ('300', 258),\n",
       " ('github', 67),\n",
       " ('rcnn', 78),\n",
       " ('datasets', 1091)]"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "missing_words = []\n",
    "for word, count in word_counts.items():\n",
    "    if count > threshold and word not in embeddings_index:\n",
    "        missing_words.append((word,count))\n",
    "missing_words[:30]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Words to indexes, indexes to words dicts\n",
    "Limit the vocab that we will use to words that appear ≥ threshold or are in CN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of unique words: 47962\n",
      "Number of words we will use: 21556\n",
      "Percent of words we will use: 44.940000000000005%\n"
     ]
    }
   ],
   "source": [
    "#dictionary to convert words to integers\n",
    "vocab_to_int = {} \n",
    "# Index words from 0\n",
    "value = 0\n",
    "for word, count in word_counts.items():\n",
    "    if count >= threshold or word in embeddings_index:\n",
    "        vocab_to_int[word] = value\n",
    "        value += 1\n",
    "\n",
    "# Special tokens that will be added to our vocab\n",
    "codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]   \n",
    "\n",
    "# Add codes to vocab\n",
    "for code in codes:\n",
    "    vocab_to_int[code] = len(vocab_to_int)\n",
    "\n",
    "# Dictionary to convert integers to words\n",
    "int_to_vocab = {}\n",
    "for word, value in vocab_to_int.items():\n",
    "    int_to_vocab[value] = word\n",
    "\n",
    "usage_ratio = round(len(vocab_to_int) / len(word_counts),4)*100\n",
    "\n",
    "print(\"Total number of unique words:\", len(word_counts))\n",
    "print(\"Number of words we will use:\", len(vocab_to_int))\n",
    "print(\"Percent of words we will use: {}%\".format(usage_ratio))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create word embedding matrix\n",
    "It has shape (nb_words, embedding_dim) i.e. (59072, 300) in this case. 1st dim is word index, 2nd dim is from CN or random generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21556\n"
     ]
    }
   ],
   "source": [
    "# Need to use 300 for embedding dimensions to match CN's vectors.\n",
    "embedding_dim = 300\n",
    "nb_words = len(vocab_to_int)\n",
    "\n",
    "\n",
    "# Create matrix with default values of zero\n",
    "word_embedding_matrix = np.zeros((nb_words, embedding_dim), dtype=np.float32)\n",
    "for word, i in vocab_to_int.items():\n",
    "    if word in embeddings_index:\n",
    "        word_embedding_matrix[i] = embeddings_index[word]\n",
    "    else:\n",
    "        # If word not in CN, create a random embedding for it\n",
    "        new_embedding = np.array(np.random.uniform(-1.0, 1.0, embedding_dim))\n",
    "        embeddings_index[word] = new_embedding\n",
    "        word_embedding_matrix[i] = new_embedding\n",
    "\n",
    "# Check if value matches len(vocab_to_int)\n",
    "print(len(word_embedding_matrix))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to convert sentences to sequence of words indexes\n",
    "It also use `<UNK>` index to replace unknown words, append `<EOS>` (End of Sentence) to the sequences if eos is set True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_ints(text, word_count, unk_count, eos=False):\n",
    "    '''Convert words in text to an integer.\n",
    "       If word is not in vocab_to_int, use UNK's integer.\n",
    "       Total the number of words and UNKs.\n",
    "       Add EOS token to the end of texts'''\n",
    "    ints = []\n",
    "    for sentence in text:\n",
    "        sentence_ints = []\n",
    "        for word in sentence.split():\n",
    "            word_count += 1\n",
    "            if word in vocab_to_int:\n",
    "                sentence_ints.append(vocab_to_int[word])\n",
    "            else:\n",
    "                sentence_ints.append(vocab_to_int[\"<UNK>\"])\n",
    "                unk_count += 1\n",
    "        if eos:\n",
    "            sentence_ints.append(vocab_to_int[\"<EOS>\"])\n",
    "        ints.append(sentence_ints)\n",
    "    return ints, word_count, unk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apply convert_to_ints to clean_summaries and clean_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of words in headlines: 1623654\n",
      "Total number of UNKs in headlines: 73211\n",
      "Percent of words that are UNK: 4.51%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "word_count = 0\n",
    "unk_count = 0\n",
    "\n",
    "int_summaries, word_count, unk_count = convert_to_ints(clean_summaries, word_count, unk_count,  eos=True)\n",
    "int_texts, word_count, unk_count = convert_to_ints(clean_texts, word_count, unk_count, eos=True)\n",
    "\n",
    "unk_percent = round(unk_count/word_count,4)*100\n",
    "\n",
    "print(\"Total number of words in headlines:\", word_count)\n",
    "print(\"Total number of UNKs in headlines:\", unk_count)\n",
    "print(\"Percent of words that are UNK: {}%\".format(unk_percent))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Take a look at what the sequence looks like\n",
    "Each number here represents a word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "117"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(int_summaries[20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to get the length of each sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lengths(text):\n",
    "    '''Create a data frame of the sentence lengths from a text'''\n",
    "    lengths = []\n",
    "    for sentence in text:\n",
    "        lengths.append(len(sentence))\n",
    "    return pd.DataFrame(lengths, columns=['counts'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   counts\n",
       "0     159\n",
       "1     152\n",
       "2     168"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_lengths(int_summaries[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get statistic summary of the length of summaries and texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summaries:\n",
      "           counts\n",
      "count  403.000000\n",
      "mean   154.759305\n",
      "std     45.652038\n",
      "min     62.000000\n",
      "25%    123.000000\n",
      "50%    148.000000\n",
      "75%    184.000000\n",
      "max    349.000000\n",
      "\n",
      "Texts:\n",
      "            counts\n",
      "count   403.000000\n",
      "mean   3876.158809\n",
      "std     392.740745\n",
      "min    2846.000000\n",
      "25%    3604.500000\n",
      "50%    3869.000000\n",
      "75%    4161.000000\n",
      "max    6094.000000\n"
     ]
    }
   ],
   "source": [
    "lengths_summaries = create_lengths(int_summaries)\n",
    "lengths_texts = create_lengths(int_texts)\n",
    "\n",
    "print(\"Summaries:\")\n",
    "print(lengths_summaries.describe())\n",
    "print()\n",
    "print(\"Texts:\")\n",
    "print(lengths_texts.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### See what's the max squence length we can cover by percentile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4360.58\n",
      "4457.7\n",
      "4761.8\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of texts\n",
    "print(np.percentile(lengths_texts.counts, 89.5))\n",
    "print(np.percentile(lengths_texts.counts, 95))\n",
    "print(np.percentile(lengths_texts.counts, 99))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "210.60000000000002\n",
      "240.69999999999993\n",
      "273.9200000000001\n"
     ]
    }
   ],
   "source": [
    "# Inspect the length of summaries\n",
    "print(np.percentile(lengths_summaries.counts, 90))\n",
    "print(np.percentile(lengths_summaries.counts, 95))\n",
    "print(np.percentile(lengths_summaries.counts, 99))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function to counts the number of time `<UNK>` appears in a sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unk_counter(sentence):\n",
    "    '''Counts the number of time UNK appears in a sentence.'''\n",
    "    unk_count = 0\n",
    "    for word in sentence:\n",
    "        if word == vocab_to_int[\"<UNK>\"]:\n",
    "            unk_count += 1\n",
    "    return unk_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter** for length limit and number of `<UNK>`s\n",
    "\n",
    "**Sort** the summaries and texts by the length of the element in **texts** from shortest to longest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "314\n",
      "314\n",
      "314\n"
     ]
    }
   ],
   "source": [
    "max_text_length = 4361 # This will cover up to 89.5% lengthes\n",
    "max_summary_length = 274 # This will cover up to 99% lengthes\n",
    "min_length = 2\n",
    "unk_text_limit = 240 # text can contain up to 1 UNK word\n",
    "unk_summary_limit = 12 # Summary should not contain any UNK word\n",
    "\n",
    "def filter_condition(item):\n",
    "    int_summary = item[0]\n",
    "    int_text = item[1]\n",
    "    if(len(int_summary) >= min_length and \n",
    "       len(int_summary) <= max_summary_length and \n",
    "       len(int_text) >= min_length and \n",
    "       len(int_text) <= max_text_length and \n",
    "       unk_counter(int_summary) <= unk_summary_limit and \n",
    "       unk_counter(int_text) <= unk_text_limit):\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "int_text_summaries = list(zip(int_summaries , int_texts))\n",
    "int_text_summaries_filtered = list(filter(filter_condition, int_text_summaries))\n",
    "print(len(int_text_summaries_filtered))\n",
    "sorted_int_text_summaries = sorted(int_text_summaries_filtered, key=lambda item: len(item[1]))\n",
    "sorted_int_text_summaries = list(zip(*sorted_int_text_summaries))\n",
    "sorted_summaries = list(sorted_int_text_summaries[0])\n",
    "sorted_texts = list(sorted_int_text_summaries[1])\n",
    "#Delete those temporary varaibles\n",
    "del int_text_summaries, sorted_int_text_summaries, int_text_summaries_filtered\n",
    "#Compare lengths to ensure they match\n",
    "print(len(sorted_summaries))\n",
    "print(len(sorted_texts))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspect the length of text in sorted_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2846,\n",
       " 2956,\n",
       " 2979,\n",
       " 3008,\n",
       " 3033,\n",
       " 3034,\n",
       " 3042,\n",
       " 3077,\n",
       " 3128,\n",
       " 3138,\n",
       " 3186,\n",
       " 3190,\n",
       " 3190,\n",
       " 3206,\n",
       " 3221,\n",
       " 3230,\n",
       " 3236,\n",
       " 3237,\n",
       " 3264,\n",
       " 3267]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths_texts = [len(text) for text in sorted_texts]\n",
    "lengths_texts[:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data for later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "__pickleStuff(\"/Users/apple/Desktop/601Project-master/clean_summaries.p\",clean_summaries)\n",
    "__pickleStuff(\"/Users/apple/Desktop/601Project-master/clean_texts.p\",clean_texts)\n",
    "\n",
    "__pickleStuff(\"/Users/apple/Desktop/601Project-master/sorted_summaries.p\",sorted_summaries)\n",
    "__pickleStuff(\"/Users/apple/Desktop/601Project-master/sorted_texts.p\",sorted_texts)\n",
    "__pickleStuff(\"/Users/apple/Desktop/601Project-master/word_embedding_matrix.p\",word_embedding_matrix)\n",
    "\n",
    "__pickleStuff(\"/Users/apple/Desktop/601Project-master/vocab_to_int.p\",vocab_to_int)\n",
    "__pickleStuff(\"/Users/apple/Desktop/601Project-master/int_to_vocab.p\",int_to_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Building the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create palceholders for inputs to the model\n",
    "\n",
    "**summary_length** and **text_length** are the sentence lengths in a batch, and **max_summary_length** is the maximum length of a summary in a batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_inputs():\n",
    "    input_data = tf.placeholder(tf.int32, [None, None], name='input')\n",
    "    targets = tf.placeholder(tf.int32, [None, None], name='targets')\n",
    "    lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "    keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "    summary_length = tf.placeholder(tf.int32, (None,), name='summary_length')\n",
    "    max_summary_length = tf.reduce_max(summary_length, name='max_dec_len')\n",
    "    text_length = tf.placeholder(tf.int32, (None,), name='text_length')\n",
    "\n",
    "    return input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove the last word id from each batch and concatenate the id of `<GO>` to the begining of each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_encoding_input(target_data, vocab_to_int, batch_size):  \n",
    "    ending = tf.strided_slice(target_data, [0, 0], [batch_size, -1], [1, 1]) # slice it to target_data[0:batch_size, 0: -1]\n",
    "    dec_input = tf.concat([tf.fill([batch_size, 1], vocab_to_int['<GO>']), ending], 1)\n",
    "\n",
    "    return dec_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the encoding layers\n",
    "\n",
    "bidirectional_dynamic_rnn\n",
    "use **tf.variable_scope** so that variables are reused with each layer\n",
    "\n",
    "parameters\n",
    "- **rnn_size**: The number of units in the LSTM cell\n",
    "- **sequence_length**: size [batch_size], containing the actual lengths for each of the sequences in the batch\n",
    "- **num_layers**: number of bidirectional RNN layer\n",
    "- **rnn_inputs**: number of bidirectional RNN layer\n",
    "- **keep_prob**: RNN dropout input keep probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(rnn_size, sequence_length, num_layers, rnn_inputs, keep_prob):\n",
    "    for layer in range(num_layers):\n",
    "        with tf.variable_scope('encoder_{}'.format(layer)):\n",
    "            cell_fw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_fw = tf.contrib.rnn.DropoutWrapper(cell_fw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            cell_bw = tf.contrib.rnn.LSTMCell(rnn_size,\n",
    "                                              initializer=tf.random_uniform_initializer(-0.1, 0.1, seed=2))\n",
    "            cell_bw = tf.contrib.rnn.DropoutWrapper(cell_bw, \n",
    "                                                    input_keep_prob = keep_prob)\n",
    "\n",
    "            enc_output, enc_state = tf.nn.bidirectional_dynamic_rnn(cell_fw, \n",
    "                                                                    cell_bw, \n",
    "                                                                    rnn_inputs,\n",
    "                                                                    sequence_length,\n",
    "                                                                    dtype=tf.float32)\n",
    "            enc_output = tf.concat(enc_output,2)\n",
    "            # original code is missing this line below, that is how we connect layers \n",
    "            # by feeding the current layer's output to next layer's input\n",
    "            rnn_inputs = enc_output\n",
    "    return enc_output, enc_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the training decoding layer\n",
    "parameters\n",
    "- **dec_embed_input**: output of embedding_lookup for a batch of inputs\n",
    "- **summary_length**: length of each padded summary sequences in batch, since padded, all lengths should be same number \n",
    "- **dec_cell**: the decoder RNN cells' output with attention wapper\n",
    "- **output_layer**: fully connected layer to apply to the RNN output\n",
    "- **vocab_size**: vocabulary size i.e. len(vocab_to_int)+1\n",
    "- **max_summary_length**: the maximum length of a summary in a batch\n",
    "- **batch_size**: number of input sequences in a batch\n",
    "\n",
    "Three components\n",
    "\n",
    "- **TraingHelper** reads a sequence of integers from the encoding layer.\n",
    "- **BasicDecoder** processes the sequence with the decoding cell, and an output layer, which is a fully connected layer. **initial_state** set to zero state.\n",
    "- **dynamic_decode** creates our outputs that will be used for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_decoding_layer(dec_embed_input, summary_length, dec_cell, output_layer,\n",
    "                            vocab_size, max_summary_length,batch_size):\n",
    "    training_helper = tf.contrib.seq2seq.TrainingHelper(inputs=dec_embed_input,\n",
    "                                                        sequence_length=summary_length,\n",
    "                                                        time_major=False)\n",
    "\n",
    "    training_decoder = tf.contrib.seq2seq.BasicDecoder(cell=dec_cell,\n",
    "                                                       helper=training_helper,\n",
    "                                                       initial_state=dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                       output_layer = output_layer)\n",
    "\n",
    "    training_logits = tf.contrib.seq2seq.dynamic_decode(training_decoder,\n",
    "                                                           output_time_major=False,\n",
    "                                                           impute_finished=True,\n",
    "                                                           maximum_iterations=max_summary_length)\n",
    "    return training_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create infer decoding layer\n",
    "\n",
    "parameters\n",
    "- **embeddings**: the CN's word_embedding_matrix\n",
    "- **start_token**: the id of `<GO>`\n",
    "- **end_token**: the id of `<EOS>`\n",
    "- **dec_cell**: the decoder RNN cells' output with attention wapper\n",
    "- **output_layer**: fully connected layer to apply to the RNN output\n",
    "- **max_summary_length**: the maximum length of a summary in a batch\n",
    "- **batch_size**: number of input sequences in a batch\n",
    "\n",
    "**GreedyEmbeddingHelper** argument **start_tokens**: int32 vector shaped [batch_size], the start tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_decoding_layer(embeddings, start_token, end_token, dec_cell, output_layer,\n",
    "                             max_summary_length, batch_size):\n",
    "    '''Create the inference logits'''\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant([start_token], dtype=tf.int32), [batch_size], name='start_tokens')\n",
    "    \n",
    "    inference_helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                                start_tokens,\n",
    "                                                                end_token)\n",
    "                \n",
    "    inference_decoder = tf.contrib.seq2seq.BasicDecoder(dec_cell,\n",
    "                                                        inference_helper,\n",
    "                                                        dec_cell.zero_state(dtype=tf.float32, batch_size=batch_size),\n",
    "                                                        output_layer)\n",
    "                \n",
    "    inference_logits = tf.contrib.seq2seq.dynamic_decode(inference_decoder,\n",
    "                                                            output_time_major=False,\n",
    "                                                            impute_finished=True,\n",
    "                                                            maximum_iterations=max_summary_length)\n",
    "    \n",
    "    return inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Decoding layer\n",
    "3 parts: decoding cell, attention, and getting our logits.\n",
    "#### Decoding Cell: \n",
    "Just a two layer LSTM with dropout.\n",
    "#### Attention: \n",
    "Using Bhadanau, since trains faster than Luong. \n",
    "\n",
    "**AttentionWrapper** applies the attention mechanism to our decoding cell.\n",
    "\n",
    "parameters\n",
    "- **dec_embed_input**: output of embedding_lookup for a batch of inputs\n",
    "- **embeddings**: the CN's word_embedding_matrix\n",
    "- **enc_output**: encoder layer output, containing the forward and the backward rnn output\n",
    "- **enc_state**: encoder layer state, a tuple containing the forward and the backward final states of bidirectional rnn.\n",
    "- **vocab_size**: vocabulary size i.e. len(vocab_to_int)+1\n",
    "- **text_length**: the actual lengths for each of the input text sequences in the batch\n",
    "- **summary_length**: the actual lengths for each of the input summary sequences in the batch\n",
    "- **max_summary_length**: the maximum length of a summary in a batch\n",
    "- **rnn_size**: The number of units in the LSTM cell\n",
    "- **vocab_to_int**: vocab_to_int the dictionary\n",
    "- **keep_prob**: RNN dropout input keep probability\n",
    "- **batch_size**: number of input sequences in a batch\n",
    "- **num_layers**: number of decoder RNN layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lstm_cell(lstm_size, keep_prob):\n",
    "    cell = tf.contrib.rnn.BasicLSTMCell(lstm_size)\n",
    "    return tf.contrib.rnn.DropoutWrapper(cell, input_keep_prob = keep_prob)\n",
    "\n",
    "def decoding_layer(dec_embed_input, embeddings, enc_output, enc_state, vocab_size, text_length, summary_length,\n",
    "                   max_summary_length, rnn_size, vocab_to_int, keep_prob, batch_size, num_layers):\n",
    "    '''Create the decoding cell and attention for the training and inference decoding layers'''\n",
    "    dec_cell = tf.contrib.rnn.MultiRNNCell([lstm_cell(rnn_size, keep_prob) for _ in range(num_layers)])\n",
    "    output_layer = Dense(vocab_size,kernel_initializer=tf.truncated_normal_initializer(mean=0.0, stddev=0.1))\n",
    "    attn_mech = tf.contrib.seq2seq.BahdanauAttention(rnn_size,\n",
    "                                                     enc_output,\n",
    "                                                     text_length,\n",
    "                                                     normalize=False,\n",
    "                                                     name='BahdanauAttention')\n",
    "    dec_cell = tf.contrib.seq2seq.AttentionWrapper(dec_cell,attn_mech,rnn_size)\n",
    "    with tf.variable_scope(\"decode\"):\n",
    "        training_logits = training_decoding_layer(dec_embed_input,summary_length,dec_cell,\n",
    "                                                  output_layer,\n",
    "                                                  vocab_size,\n",
    "                                                  max_summary_length,\n",
    "                                                  batch_size)\n",
    "    with tf.variable_scope(\"decode\", reuse=True):\n",
    "        inference_logits = inference_decoding_layer(embeddings,\n",
    "                                                    vocab_to_int['<GO>'],\n",
    "                                                    vocab_to_int['<EOS>'],\n",
    "                                                    dec_cell,\n",
    "                                                    output_layer,\n",
    "                                                    max_summary_length,\n",
    "                                                    batch_size)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq2seq_model(input_data, target_data, keep_prob, text_length, summary_length, max_summary_length, \n",
    "                  vocab_size, rnn_size, num_layers, vocab_to_int, batch_size):\n",
    "    '''Use the previous functions to create the training and inference logits'''\n",
    "    \n",
    "    # Use Numberbatch's embeddings and the newly created ones as our embeddings\n",
    "    embeddings = word_embedding_matrix\n",
    "    enc_embed_input = tf.nn.embedding_lookup(embeddings, input_data)\n",
    "    enc_output, enc_state = encoding_layer(rnn_size, text_length, num_layers, enc_embed_input, keep_prob)\n",
    "    dec_input = process_encoding_input(target_data, vocab_to_int, batch_size) #shape=(batch_size, senquence length) each seq start with index of<GO>\n",
    "    dec_embed_input = tf.nn.embedding_lookup(embeddings, dec_input)\n",
    "    training_logits, inference_logits  = decoding_layer(dec_embed_input, \n",
    "                                                        embeddings,\n",
    "                                                        enc_output,\n",
    "                                                        enc_state, \n",
    "                                                        vocab_size, \n",
    "                                                        text_length, \n",
    "                                                        summary_length, \n",
    "                                                        max_summary_length,\n",
    "                                                        rnn_size, \n",
    "                                                        vocab_to_int, \n",
    "                                                        keep_prob, \n",
    "                                                        batch_size,\n",
    "                                                        num_layers)\n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pad sentences for batch\n",
    "Pad so the actual lengths for each of the sequences in the batch have the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_sentence_batch(sentence_batch):\n",
    "    \"\"\"Pad sentences with <PAD> so that each sentence of a batch has the same length\"\"\"\n",
    "    max_sentence = max([len(sentence) for sentence in sentence_batch])\n",
    "    return [sentence + [vocab_to_int['<PAD>']] * (max_sentence - len(sentence)) for sentence in sentence_batch]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to generate batch data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(summaries, texts, batch_size):\n",
    "    \"\"\"Batch summaries, texts, and the lengths of their sentences together\"\"\"\n",
    "    for batch_i in range(0, len(texts)//batch_size):\n",
    "        start_i = batch_i * batch_size\n",
    "        summaries_batch = summaries[start_i:start_i + batch_size]\n",
    "        texts_batch = texts[start_i:start_i + batch_size]\n",
    "        pad_summaries_batch = np.array(pad_sentence_batch(summaries_batch))\n",
    "        pad_texts_batch = np.array(pad_sentence_batch(texts_batch))\n",
    "        \n",
    "        # Need the lengths for the _lengths parameters\n",
    "        pad_summaries_lengths = []\n",
    "        for summary in pad_summaries_batch:\n",
    "            pad_summaries_lengths.append(len(summary))\n",
    "        \n",
    "        pad_texts_lengths = []\n",
    "        for text in pad_texts_batch:\n",
    "            pad_texts_lengths.append(len(text))\n",
    "        \n",
    "        yield pad_summaries_batch, pad_texts_batch, pad_summaries_lengths, pad_texts_lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Just to test \"get_batches\" function\n",
    "Here we generate a batch with size of 5\n",
    "\n",
    "Checkout those \"59069\" they are `<PAD>`s, also all sequences' lengths are the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'<PAD>' has id: 21553\n",
      "pad summaries batch samples:\n",
      "\r",
      " [[  723   275   724   725    27   726   727   350     5   412   728    18\n",
      "     27   729    23   727   730    13   731    22   732   495    32    33\n",
      "     34   733   511     9   734   301   735    13   727   736   737    22\n",
      "    279   738   739    18   495   145    23   740    13    70   733   511\n",
      "     16    34   741   652   742   743   277    80   592   346   664    13\n",
      "    727   730   629   744   619   741   745    18   746    32   119   729\n",
      "    301   747   730   444   748   749   750    13   751   170   752   753\n",
      "      5    23   652   742    27   754   755   610    13   473   183    32\n",
      "    269   454   512   756   473   757    27   727   730   758    34   759\n",
      "     13    23   733   187   444   624     7    32   760    70   511   761\n",
      "     18   762   133   277    34   763   764   207    35   765   766   767\n",
      "    768   581    49    43   761    16   769    27   729    23   727   730\n",
      "     13   731    18   726    23   770    13    34   771    13   727   350\n",
      "    207    80   583   772    27   773   774    70   389   775   776    80\n",
      "    777    13   773   778   207   733   727   779   145   780   740    18\n",
      "    193   781   705   255   782   783    27   784   186 21554]\n",
      " [  238  1273   325     7   268   219    45   426   784  2267    27  2195\n",
      "     23  2777  3298   735  2641   240   180    11   624   506  1041  1011\n",
      "    381    43   152  2718    43   160    13 21552   775  2641   240   180\n",
      "    795   506    45  3299   775    32  2200   718  1141    23  2264    13\n",
      "   2449  3028    13    23  1648  1502  1394  1044   263   478  3300   325\n",
      "      7   268   342    32    49    43    23   340  3301  3302     1  3303\n",
      "   2678     5    23  2264    13   379   688    18  2449  2974    18   245\n",
      "    404    18   376  1337    43    23   987   499  3304  3302    16  3305\n",
      "    478  3300    27   669    98   735    32   606    34   497   325     7\n",
      "    268  1465   499 21552   346   277    23  3304  3302    70    84   275\n",
      "     43   379   342  2910   623   541  1580  2261    20    43 21552  2068\n",
      "   1338  3306     5   415    13   371  1162  2091  3307  1438    86  3308\n",
      "    298   312   674 21554 21553 21553 21553 21553 21553 21553 21553 21553\n",
      "  21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553\n",
      "  21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553\n",
      "  21553 21553 21553 21553 21553 21553 21553 21553 21553 21553]\n",
      " [  502   821   506  1522   119   200  4589   189  3017   495    67   741\n",
      "    739    27  4590   976    44   420    34  1757   492   212    18   321\n",
      "   4591    41   578   509  1674    13    23   928    27   200  4592   255\n",
      "   4593    67  2565     5  1350  1084  1150   501  4594  4595  2457   119\n",
      "   4591    23  1268  4491  1195   903    13   409   899     5    34   702\n",
      "   4596   276    64   928  1999   240    23  1206    16    34   107   831\n",
      "   1207  1034  2374   942   301   404  1271   106  1427  1522   486  4595\n",
      "   2374  1816  3289  1402    27  4597   277  4598  1286    32  1801  4599\n",
      "     23  1235  4600   228    13  2392     5  4595  2374     5  4601  1295\n",
      "     23   943  3284  2374    16   879    27  4597  1999     9   741    15\n",
      "    795   255  3611    13  1566  4602   495  3284  2374  1408  1442  1382\n",
      "    899  3226  1566   456   457   581   277 21552    18  1961   581   277\n",
      "   1760 21552 21554 21553 21553 21553 21553 21553 21553 21553 21553 21553\n",
      "  21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553\n",
      "  21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553\n",
      "  21553 21553 21553 21553 21553 21553 21553 21553 21553 21553]\n",
      " [    7    13    25  1150   850     5  2486    15    16    34  3530    24\n",
      "      5     6     7    58   289   103    16    27  3531    43    23   893\n",
      "     15    45   812    27    34    95  1150   711   667   707    45    34\n",
      "    767   771    13   667     7   219   644   170   994   756    13    15\n",
      "    775    27    23   667   106   707    16    34  2071  1494    13   860\n",
      "    219    43   994     7    13    23   667  2054   207    23   733   271\n",
      "     13    23   893    15    23  1442  3532    16    23   274   187   747\n",
      "    857   511  3209   859    20  2041  3427  2159    27  2531   121    32\n",
      "    443   301  3427   135  3533    34   497  3534  3535   187  3536     9\n",
      "    598    13   775   277    23   667  1381   135   727   705    13  3537\n",
      "   1839    63  3538   999    29   187   207    34  3209   209     9    23\n",
      "    756   117   722    34   497  3537  3209  3539   187  3540   277    23\n",
      "     35  1091    13    34    58  1150   667    32   230   784   186   545\n",
      "     18  1181  1437   201   121     5    34   771    13    84   629  2454\n",
      "      5  2460  1197     5   763 21554 21553 21553 21553 21553 21553 21553\n",
      "  21553 21553 21553 21553 21553 21553 21553 21553 21553 21553]\n",
      " [  870   186    16    34   673  1367     9  1249   287   186  1246   870\n",
      "    186   545   556  1935   511  1273  1936    29    71   133  1461     9\n",
      "    669  1937    27  1178    32    33    80  1938   870   186   428  1938\n",
      "   1939   870   186 21552    32   974   133     5  1940   643   644    34\n",
      "    860  1406   627     5 21552    23   515   215    34   287   511    18\n",
      "     34   764  1941  1942    32   878    54  1943   365    18  1944    34\n",
      "   1933   118    13   705    23   428  1945  1946    80  1947   870   680\n",
      "     18  1948    23   870   407    32  1179 21552    27   751   286   189\n",
      "    435  1795   116   705 21552   690  1481    18    34  1371   511    32\n",
      "    394    23  1371   511   277    34  1949  1950   495   207 21552    32\n",
      "    119  1178   870   186   277   115   511    32  1951     5  1940 21554\n",
      "  21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553\n",
      "  21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553\n",
      "  21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553\n",
      "  21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553 21553\n",
      "  21553 21553 21553 21553 21553 21553 21553 21553 21553 21553]]\n"
     ]
    }
   ],
   "source": [
    "print(\"'<PAD>' has id: {}\".format(vocab_to_int['<PAD>']))\n",
    "sorted_summaries_samples = sorted_summaries[7:50]\n",
    "sorted_texts_samples = sorted_texts[7:50]\n",
    "pad_summaries_batch_samples, pad_texts_batch_samples, pad_summaries_lengths_samples, pad_texts_lengths_samples = next(get_batches(\n",
    "    sorted_summaries_samples, sorted_texts_samples, 5))\n",
    "print(\"pad summaries batch samples:\\n\\r {}\".format(pad_summaries_batch_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the Hyperparameters\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.005\n",
    "keep_probability = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "seq_dim is deprecated, use seq_axis instead\n",
      "WARNING:tensorflow:From /Users/apple/anaconda3/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "batch_dim is deprecated, use batch_axis instead\n",
      "Graph is built.\n",
      "./graph\n"
     ]
    }
   ],
   "source": [
    "# Build the graph\n",
    "train_graph = tf.Graph()\n",
    "# Set the graph to default to ensure that it is ready for training\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs    \n",
    "    input_data, targets, lr, keep_prob, summary_length, max_summary_length, text_length = model_inputs()\n",
    "\n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(tf.reverse(input_data, [-1]),\n",
    "                                                      targets, \n",
    "                                                      keep_prob,   \n",
    "                                                      text_length,\n",
    "                                                      summary_length,\n",
    "                                                      max_summary_length,\n",
    "                                                      len(vocab_to_int)+1,\n",
    "                                                      rnn_size, \n",
    "                                                      num_layers, \n",
    "                                                      vocab_to_int,\n",
    "                                                      batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits[0].rnn_output, 'logits')\n",
    "    inference_logits = tf.identity(inference_logits[0].sample_id, name='predictions')\n",
    "    \n",
    "    # Create the weights for sequence_loss, the sould be all True across since each batch is padded\n",
    "    masks = tf.sequence_mask(summary_length, max_summary_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        # Loss function\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(\n",
    "            training_logits,\n",
    "            targets,\n",
    "            masks)\n",
    "\n",
    "        # Optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "\n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -5., 5.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)\n",
    "print(\"Graph is built.\")\n",
    "graph_location = \"./graph\"\n",
    "print(graph_location)\n",
    "train_writer = tf.summary.FileWriter(graph_location)\n",
    "train_writer.add_graph(train_graph)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the Model\n",
    "\n",
    "Only going to use a subset of the data to reduce the traing time for this demo.\n",
    "\n",
    "We chose not use use the start of the subset because because those are shorter sequences and we don't want to make it too easy for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shortest text length: 3935\n",
      "The longest text length: 4094\n"
     ]
    }
   ],
   "source": [
    "# Subset the data for training\n",
    "start = 200\n",
    "end = start + 50\n",
    "sorted_summaries_short = sorted_summaries[start:end]\n",
    "sorted_texts_short = sorted_texts[start:end]\n",
    "print(\"The shortest text length:\", len(sorted_texts_short[0]))\n",
    "print(\"The longest text length:\",len(sorted_texts_short[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   1/100 Batch   20/781 - Loss:  4.931, Seconds: 2.67\n",
      "Epoch   1/100 Batch   40/781 - Loss:  2.845, Seconds: 2.23\n",
      "Epoch   1/100 Batch   60/781 - Loss:  2.871, Seconds: 2.77\n",
      "Epoch   1/100 Batch   80/781 - Loss:  2.816, Seconds: 2.67\n",
      "Epoch   1/100 Batch  100/781 - Loss:  2.682, Seconds: 2.66\n",
      "Epoch   1/100 Batch  120/781 - Loss:  2.695, Seconds: 2.49\n",
      "Epoch   1/100 Batch  140/781 - Loss:  2.583, Seconds: 2.59\n",
      "Epoch   1/100 Batch  160/781 - Loss:  2.839, Seconds: 2.20\n",
      "Epoch   1/100 Batch  180/781 - Loss:  2.695, Seconds: 2.52\n",
      "Epoch   1/100 Batch  200/781 - Loss:  2.667, Seconds: 2.84\n",
      "Epoch   1/100 Batch  220/781 - Loss:  2.603, Seconds: 2.63\n",
      "Epoch   1/100 Batch  240/781 - Loss:  2.456, Seconds: 2.62\n",
      "Average loss for this update: 2.866\n",
      "New Record!\n",
      "Epoch   1/100 Batch  260/781 - Loss:  2.558, Seconds: 2.74\n",
      "Epoch   1/100 Batch  280/781 - Loss:  2.639, Seconds: 2.73\n",
      "Epoch   1/100 Batch  300/781 - Loss:  2.651, Seconds: 2.87\n",
      "Epoch   1/100 Batch  320/781 - Loss:  2.672, Seconds: 2.67\n",
      "Epoch   1/100 Batch  340/781 - Loss:  2.475, Seconds: 2.69\n",
      "Epoch   1/100 Batch  360/781 - Loss:  2.551, Seconds: 2.78\n",
      "Epoch   1/100 Batch  380/781 - Loss:  2.356, Seconds: 2.91\n",
      "Epoch   1/100 Batch  400/781 - Loss:  2.538, Seconds: 2.91\n",
      "Epoch   1/100 Batch  420/781 - Loss:  2.530, Seconds: 2.55\n",
      "Epoch   1/100 Batch  440/781 - Loss:  2.573, Seconds: 2.98\n",
      "Epoch   1/100 Batch  460/781 - Loss:  2.599, Seconds: 2.82\n",
      "Epoch   1/100 Batch  480/781 - Loss:  2.377, Seconds: 2.59\n",
      "Epoch   1/100 Batch  500/781 - Loss:  2.438, Seconds: 2.72\n",
      "Average loss for this update: 2.523\n",
      "New Record!\n",
      "Epoch   1/100 Batch  520/781 - Loss:  2.412, Seconds: 2.56\n",
      "Epoch   1/100 Batch  540/781 - Loss:  2.411, Seconds: 2.86\n",
      "Epoch   1/100 Batch  560/781 - Loss:  2.331, Seconds: 2.63\n",
      "Epoch   1/100 Batch  580/781 - Loss:  2.423, Seconds: 2.75\n",
      "Epoch   1/100 Batch  600/781 - Loss:  2.541, Seconds: 2.80\n",
      "Epoch   1/100 Batch  620/781 - Loss:  2.449, Seconds: 2.67\n",
      "Epoch   1/100 Batch  640/781 - Loss:  2.331, Seconds: 2.78\n",
      "Epoch   1/100 Batch  660/781 - Loss:  2.274, Seconds: 2.89\n",
      "Epoch   1/100 Batch  680/781 - Loss:  2.193, Seconds: 2.75\n",
      "Epoch   1/100 Batch  700/781 - Loss:  2.360, Seconds: 2.83\n",
      "Epoch   1/100 Batch  720/781 - Loss:  2.525, Seconds: 2.91\n",
      "Epoch   1/100 Batch  740/781 - Loss:  2.353, Seconds: 3.04\n",
      "Epoch   1/100 Batch  760/781 - Loss:  2.364, Seconds: 2.80\n",
      "Average loss for this update: 2.364\n",
      "New Record!\n",
      "Epoch   1/100 Batch  780/781 - Loss:  2.146, Seconds: 2.60\n",
      "Epoch   2/100 Batch   20/781 - Loss:  2.324, Seconds: 2.72\n",
      "Epoch   2/100 Batch   40/781 - Loss:  2.177, Seconds: 2.26\n",
      "Epoch   2/100 Batch   60/781 - Loss:  2.227, Seconds: 2.81\n",
      "Epoch   2/100 Batch   80/781 - Loss:  2.212, Seconds: 2.73\n",
      "Epoch   2/100 Batch  100/781 - Loss:  2.102, Seconds: 2.71\n",
      "Epoch   2/100 Batch  120/781 - Loss:  2.139, Seconds: 2.61\n",
      "Epoch   2/100 Batch  140/781 - Loss:  2.005, Seconds: 2.62\n",
      "Epoch   2/100 Batch  160/781 - Loss:  2.305, Seconds: 2.20\n",
      "Epoch   2/100 Batch  180/781 - Loss:  2.164, Seconds: 2.41\n",
      "Epoch   2/100 Batch  200/781 - Loss:  2.159, Seconds: 2.75\n",
      "Epoch   2/100 Batch  220/781 - Loss:  2.088, Seconds: 2.59\n",
      "Epoch   2/100 Batch  240/781 - Loss:  1.927, Seconds: 2.62\n",
      "Average loss for this update: 2.144\n",
      "New Record!\n",
      "Epoch   2/100 Batch  260/781 - Loss:  2.028, Seconds: 2.80\n",
      "Epoch   2/100 Batch  280/781 - Loss:  2.077, Seconds: 2.71\n",
      "Epoch   2/100 Batch  300/781 - Loss:  2.165, Seconds: 2.67\n",
      "Epoch   2/100 Batch  320/781 - Loss:  2.228, Seconds: 2.63\n",
      "Epoch   2/100 Batch  340/781 - Loss:  2.053, Seconds: 2.69\n",
      "Epoch   2/100 Batch  360/781 - Loss:  2.124, Seconds: 2.75\n",
      "Epoch   2/100 Batch  380/781 - Loss:  1.923, Seconds: 2.88\n",
      "Epoch   2/100 Batch  400/781 - Loss:  2.064, Seconds: 2.86\n",
      "Epoch   2/100 Batch  420/781 - Loss:  2.067, Seconds: 2.54\n",
      "Epoch   2/100 Batch  440/781 - Loss:  2.140, Seconds: 2.95\n",
      "Epoch   2/100 Batch  460/781 - Loss:  2.200, Seconds: 2.84\n",
      "Epoch   2/100 Batch  480/781 - Loss:  2.008, Seconds: 2.61\n",
      "Epoch   2/100 Batch  500/781 - Loss:  2.046, Seconds: 2.75\n",
      "Average loss for this update: 2.082\n",
      "New Record!\n",
      "Epoch   2/100 Batch  520/781 - Loss:  1.991, Seconds: 2.65\n",
      "Epoch   2/100 Batch  540/781 - Loss:  1.997, Seconds: 2.82\n",
      "Epoch   2/100 Batch  560/781 - Loss:  1.935, Seconds: 2.75\n",
      "Epoch   2/100 Batch  580/781 - Loss:  2.104, Seconds: 2.76\n",
      "Epoch   2/100 Batch  600/781 - Loss:  2.195, Seconds: 2.74\n",
      "Epoch   2/100 Batch  620/781 - Loss:  2.108, Seconds: 2.69\n",
      "Epoch   2/100 Batch  640/781 - Loss:  1.995, Seconds: 2.71\n",
      "Epoch   2/100 Batch  660/781 - Loss:  1.920, Seconds: 3.00\n",
      "Epoch   2/100 Batch  680/781 - Loss:  1.875, Seconds: 2.82\n",
      "Epoch   2/100 Batch  700/781 - Loss:  2.040, Seconds: 2.84\n",
      "Epoch   2/100 Batch  720/781 - Loss:  2.199, Seconds: 2.99\n",
      "Epoch   2/100 Batch  740/781 - Loss:  2.049, Seconds: 3.04\n",
      "Epoch   2/100 Batch  760/781 - Loss:  2.055, Seconds: 2.78\n",
      "Average loss for this update: 2.026\n",
      "New Record!\n",
      "Epoch   2/100 Batch  780/781 - Loss:  1.846, Seconds: 2.59\n",
      "Epoch   3/100 Batch   20/781 - Loss:  2.079, Seconds: 2.67\n",
      "Epoch   3/100 Batch   40/781 - Loss:  1.931, Seconds: 2.26\n",
      "Epoch   3/100 Batch   60/781 - Loss:  1.957, Seconds: 2.79\n",
      "Epoch   3/100 Batch   80/781 - Loss:  1.949, Seconds: 2.70\n",
      "Epoch   3/100 Batch  100/781 - Loss:  1.813, Seconds: 2.72\n",
      "Epoch   3/100 Batch  120/781 - Loss:  1.863, Seconds: 2.49\n",
      "Epoch   3/100 Batch  140/781 - Loss:  1.710, Seconds: 2.57\n",
      "Epoch   3/100 Batch  160/781 - Loss:  2.038, Seconds: 2.20\n",
      "Epoch   3/100 Batch  180/781 - Loss:  1.914, Seconds: 2.43\n",
      "Epoch   3/100 Batch  200/781 - Loss:  1.902, Seconds: 2.72\n",
      "Epoch   3/100 Batch  220/781 - Loss:  1.822, Seconds: 2.63\n",
      "Epoch   3/100 Batch  240/781 - Loss:  1.648, Seconds: 2.65\n",
      "Average loss for this update: 1.876\n",
      "New Record!\n",
      "Epoch   3/100 Batch  260/781 - Loss:  1.744, Seconds: 2.78\n",
      "Epoch   3/100 Batch  280/781 - Loss:  1.792, Seconds: 2.73\n",
      "Epoch   3/100 Batch  300/781 - Loss:  1.912, Seconds: 2.67\n",
      "Epoch   3/100 Batch  320/781 - Loss:  2.000, Seconds: 2.65\n",
      "Epoch   3/100 Batch  340/781 - Loss:  1.819, Seconds: 2.65\n",
      "Epoch   3/100 Batch  360/781 - Loss:  1.885, Seconds: 2.79\n",
      "Epoch   3/100 Batch  380/781 - Loss:  1.658, Seconds: 2.93\n",
      "Epoch   3/100 Batch  400/781 - Loss:  1.810, Seconds: 2.86\n",
      "Epoch   3/100 Batch  420/781 - Loss:  1.789, Seconds: 2.57\n",
      "Epoch   3/100 Batch  440/781 - Loss:  1.897, Seconds: 2.94\n",
      "Epoch   3/100 Batch  460/781 - Loss:  1.971, Seconds: 2.87\n",
      "Epoch   3/100 Batch  480/781 - Loss:  1.771, Seconds: 2.59\n",
      "Epoch   3/100 Batch  500/781 - Loss:  1.808, Seconds: 2.74\n",
      "Average loss for this update: 1.831\n",
      "New Record!\n",
      "Epoch   3/100 Batch  520/781 - Loss:  1.701, Seconds: 2.56\n",
      "Epoch   3/100 Batch  540/781 - Loss:  1.742, Seconds: 2.84\n",
      "Epoch   3/100 Batch  560/781 - Loss:  1.683, Seconds: 2.62\n",
      "Epoch   3/100 Batch  580/781 - Loss:  1.902, Seconds: 2.74\n",
      "Epoch   3/100 Batch  600/781 - Loss:  1.959, Seconds: 2.76\n",
      "Epoch   3/100 Batch  620/781 - Loss:  1.877, Seconds: 2.68\n",
      "Epoch   3/100 Batch  640/781 - Loss:  1.771, Seconds: 2.75\n",
      "Epoch   3/100 Batch  660/781 - Loss:  1.675, Seconds: 2.88\n",
      "Epoch   3/100 Batch  680/781 - Loss:  1.663, Seconds: 2.78\n",
      "Epoch   3/100 Batch  700/781 - Loss:  1.821, Seconds: 2.85\n",
      "Epoch   3/100 Batch  720/781 - Loss:  1.989, Seconds: 2.95\n",
      "Epoch   3/100 Batch  740/781 - Loss:  1.856, Seconds: 3.02\n",
      "Epoch   3/100 Batch  760/781 - Loss:  1.846, Seconds: 2.82\n",
      "Average loss for this update: 1.802\n",
      "New Record!\n",
      "Epoch   3/100 Batch  780/781 - Loss:  1.617, Seconds: 2.62\n",
      "Epoch   4/100 Batch   20/781 - Loss:  1.901, Seconds: 2.67\n",
      "Epoch   4/100 Batch   40/781 - Loss:  1.740, Seconds: 2.23\n",
      "Epoch   4/100 Batch   60/781 - Loss:  1.743, Seconds: 2.77\n",
      "Epoch   4/100 Batch   80/781 - Loss:  1.751, Seconds: 2.67\n",
      "Epoch   4/100 Batch  100/781 - Loss:  1.587, Seconds: 2.71\n",
      "Epoch   4/100 Batch  120/781 - Loss:  1.657, Seconds: 2.47\n",
      "Epoch   4/100 Batch  140/781 - Loss:  1.515, Seconds: 2.59\n",
      "Epoch   4/100 Batch  160/781 - Loss:  1.834, Seconds: 2.19\n",
      "Epoch   4/100 Batch  180/781 - Loss:  1.736, Seconds: 2.42\n",
      "Epoch   4/100 Batch  200/781 - Loss:  1.707, Seconds: 2.74\n",
      "Epoch   4/100 Batch  220/781 - Loss:  1.641, Seconds: 2.65\n",
      "Epoch   4/100 Batch  240/781 - Loss:  1.450, Seconds: 2.63\n",
      "Average loss for this update: 1.679\n",
      "New Record!\n",
      "Epoch   4/100 Batch  260/781 - Loss:  1.556, Seconds: 2.92\n",
      "Epoch   4/100 Batch  280/781 - Loss:  1.605, Seconds: 2.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   4/100 Batch  300/781 - Loss:  1.737, Seconds: 2.70\n",
      "Epoch   4/100 Batch  320/781 - Loss:  1.818, Seconds: 2.71\n",
      "Epoch   4/100 Batch  340/781 - Loss:  1.626, Seconds: 2.66\n",
      "Epoch   4/100 Batch  360/781 - Loss:  1.688, Seconds: 2.80\n",
      "Epoch   4/100 Batch  380/781 - Loss:  1.477, Seconds: 2.91\n",
      "Epoch   4/100 Batch  400/781 - Loss:  1.614, Seconds: 2.88\n",
      "Epoch   4/100 Batch  420/781 - Loss:  1.591, Seconds: 2.59\n",
      "Epoch   4/100 Batch  440/781 - Loss:  1.705, Seconds: 2.94\n",
      "Epoch   4/100 Batch  460/781 - Loss:  1.781, Seconds: 2.85\n",
      "Epoch   4/100 Batch  480/781 - Loss:  1.582, Seconds: 2.66\n",
      "Epoch   4/100 Batch  500/781 - Loss:  1.619, Seconds: 2.70\n",
      "Average loss for this update: 1.64\n",
      "New Record!\n",
      "Epoch   4/100 Batch  520/781 - Loss:  1.492, Seconds: 2.54\n",
      "Epoch   4/100 Batch  540/781 - Loss:  1.559, Seconds: 2.80\n",
      "Epoch   4/100 Batch  560/781 - Loss:  1.491, Seconds: 2.61\n",
      "Epoch   4/100 Batch  580/781 - Loss:  1.728, Seconds: 2.79\n",
      "Epoch   4/100 Batch  600/781 - Loss:  1.766, Seconds: 2.76\n",
      "Epoch   4/100 Batch  620/781 - Loss:  1.665, Seconds: 2.66\n",
      "Epoch   4/100 Batch  640/781 - Loss:  1.583, Seconds: 2.73\n",
      "Epoch   4/100 Batch  660/781 - Loss:  1.461, Seconds: 2.89\n",
      "Epoch   4/100 Batch  680/781 - Loss:  1.487, Seconds: 2.74\n",
      "Epoch   4/100 Batch  700/781 - Loss:  1.648, Seconds: 2.78\n",
      "Epoch   4/100 Batch  720/781 - Loss:  1.830, Seconds: 2.88\n",
      "Epoch   4/100 Batch  740/781 - Loss:  1.689, Seconds: 3.08\n",
      "Epoch   4/100 Batch  760/781 - Loss:  1.663, Seconds: 2.77\n",
      "Average loss for this update: 1.617\n",
      "New Record!\n",
      "Epoch   4/100 Batch  780/781 - Loss:  1.431, Seconds: 2.57\n",
      "Epoch   5/100 Batch   20/781 - Loss:  1.746, Seconds: 2.64\n",
      "Epoch   5/100 Batch   40/781 - Loss:  1.577, Seconds: 2.23\n",
      "Epoch   5/100 Batch   60/781 - Loss:  1.584, Seconds: 2.81\n",
      "Epoch   5/100 Batch   80/781 - Loss:  1.581, Seconds: 2.64\n",
      "Epoch   5/100 Batch  100/781 - Loss:  1.411, Seconds: 2.68\n",
      "Epoch   5/100 Batch  120/781 - Loss:  1.499, Seconds: 2.46\n",
      "Epoch   5/100 Batch  140/781 - Loss:  1.360, Seconds: 2.57\n",
      "Epoch   5/100 Batch  160/781 - Loss:  1.669, Seconds: 2.22\n",
      "Epoch   5/100 Batch  180/781 - Loss:  1.592, Seconds: 2.42\n",
      "Epoch   5/100 Batch  200/781 - Loss:  1.541, Seconds: 2.83\n",
      "Epoch   5/100 Batch  220/781 - Loss:  1.492, Seconds: 2.60\n",
      "Epoch   5/100 Batch  240/781 - Loss:  1.278, Seconds: 2.65\n",
      "Average loss for this update: 1.519\n",
      "New Record!\n",
      "Epoch   5/100 Batch  260/781 - Loss:  1.406, Seconds: 2.80\n",
      "Epoch   5/100 Batch  280/781 - Loss:  1.448, Seconds: 2.73\n",
      "Epoch   5/100 Batch  300/781 - Loss:  1.599, Seconds: 2.65\n",
      "Epoch   5/100 Batch  320/781 - Loss:  1.658, Seconds: 2.69\n",
      "Epoch   5/100 Batch  340/781 - Loss:  1.476, Seconds: 2.67\n",
      "Epoch   5/100 Batch  360/781 - Loss:  1.506, Seconds: 2.79\n",
      "Epoch   5/100 Batch  380/781 - Loss:  1.309, Seconds: 2.89\n",
      "Epoch   5/100 Batch  400/781 - Loss:  1.445, Seconds: 2.90\n",
      "Epoch   5/100 Batch  420/781 - Loss:  1.426, Seconds: 2.54\n",
      "Epoch   5/100 Batch  440/781 - Loss:  1.547, Seconds: 2.98\n",
      "Epoch   5/100 Batch  460/781 - Loss:  1.618, Seconds: 2.83\n",
      "Epoch   5/100 Batch  480/781 - Loss:  1.435, Seconds: 2.61\n",
      "Epoch   5/100 Batch  500/781 - Loss:  1.475, Seconds: 2.73\n",
      "Average loss for this update: 1.481\n",
      "New Record!\n",
      "Epoch   5/100 Batch  520/781 - Loss:  1.322, Seconds: 2.51\n",
      "Epoch   5/100 Batch  540/781 - Loss:  1.417, Seconds: 2.95\n",
      "Epoch   5/100 Batch  560/781 - Loss:  1.344, Seconds: 2.59\n",
      "Epoch   5/100 Batch  580/781 - Loss:  1.565, Seconds: 2.89\n",
      "Epoch   5/100 Batch  600/781 - Loss:  1.599, Seconds: 2.80\n",
      "Epoch   5/100 Batch  620/781 - Loss:  1.495, Seconds: 2.69\n",
      "Epoch   5/100 Batch  640/781 - Loss:  1.425, Seconds: 2.82\n",
      "Epoch   5/100 Batch  660/781 - Loss:  1.283, Seconds: 2.85\n",
      "Epoch   5/100 Batch  680/781 - Loss:  1.345, Seconds: 2.79\n",
      "Epoch   5/100 Batch  700/781 - Loss:  1.493, Seconds: 2.94\n",
      "Epoch   5/100 Batch  720/781 - Loss:  1.689, Seconds: 2.91\n",
      "Epoch   5/100 Batch  740/781 - Loss:  1.540, Seconds: 3.02\n",
      "Epoch   5/100 Batch  760/781 - Loss:  1.515, Seconds: 2.79\n",
      "Average loss for this update: 1.463\n",
      "New Record!\n",
      "Epoch   5/100 Batch  780/781 - Loss:  1.292, Seconds: 2.59\n",
      "Epoch   6/100 Batch   20/781 - Loss:  1.596, Seconds: 2.66\n",
      "Epoch   6/100 Batch   40/781 - Loss:  1.460, Seconds: 2.29\n",
      "Epoch   6/100 Batch   60/781 - Loss:  1.434, Seconds: 2.77\n",
      "Epoch   6/100 Batch   80/781 - Loss:  1.438, Seconds: 2.68\n",
      "Epoch   6/100 Batch  100/781 - Loss:  1.278, Seconds: 2.72\n",
      "Epoch   6/100 Batch  120/781 - Loss:  1.363, Seconds: 2.48\n",
      "Epoch   6/100 Batch  140/781 - Loss:  1.249, Seconds: 2.58\n",
      "Epoch   6/100 Batch  160/781 - Loss:  1.526, Seconds: 2.21\n",
      "Epoch   6/100 Batch  180/781 - Loss:  1.459, Seconds: 2.40\n",
      "Epoch   6/100 Batch  200/781 - Loss:  1.399, Seconds: 2.78\n",
      "Epoch   6/100 Batch  220/781 - Loss:  1.368, Seconds: 2.64\n",
      "Epoch   6/100 Batch  240/781 - Loss:  1.152, Seconds: 2.66\n",
      "Average loss for this update: 1.386\n",
      "New Record!\n",
      "Epoch   6/100 Batch  260/781 - Loss:  1.284, Seconds: 2.78\n",
      "Epoch   6/100 Batch  280/781 - Loss:  1.312, Seconds: 2.73\n",
      "Epoch   6/100 Batch  300/781 - Loss:  1.479, Seconds: 2.67\n",
      "Epoch   6/100 Batch  320/781 - Loss:  1.532, Seconds: 2.66\n",
      "Epoch   6/100 Batch  340/781 - Loss:  1.356, Seconds: 2.65\n",
      "Epoch   6/100 Batch  360/781 - Loss:  1.390, Seconds: 2.85\n",
      "Epoch   6/100 Batch  380/781 - Loss:  1.194, Seconds: 2.86\n",
      "Epoch   6/100 Batch  400/781 - Loss:  1.318, Seconds: 2.88\n",
      "Epoch   6/100 Batch  420/781 - Loss:  1.291, Seconds: 2.57\n",
      "Epoch   6/100 Batch  440/781 - Loss:  1.408, Seconds: 2.94\n",
      "Epoch   6/100 Batch  460/781 - Loss:  1.469, Seconds: 2.85\n",
      "Epoch   6/100 Batch  480/781 - Loss:  1.303, Seconds: 2.62\n",
      "Epoch   6/100 Batch  500/781 - Loss:  1.355, Seconds: 2.70\n",
      "Average loss for this update: 1.353\n",
      "New Record!\n",
      "Epoch   6/100 Batch  520/781 - Loss:  1.191, Seconds: 2.52\n",
      "Epoch   6/100 Batch  540/781 - Loss:  1.276, Seconds: 2.93\n",
      "Epoch   6/100 Batch  560/781 - Loss:  1.211, Seconds: 2.60\n",
      "Epoch   6/100 Batch  580/781 - Loss:  1.431, Seconds: 2.73\n",
      "Epoch   6/100 Batch  600/781 - Loss:  1.445, Seconds: 2.74\n",
      "Epoch   6/100 Batch  620/781 - Loss:  1.355, Seconds: 2.65\n",
      "Epoch   6/100 Batch  640/781 - Loss:  1.298, Seconds: 2.76\n",
      "Epoch   6/100 Batch  660/781 - Loss:  1.150, Seconds: 2.86\n",
      "Epoch   6/100 Batch  680/781 - Loss:  1.232, Seconds: 2.80\n",
      "Epoch   6/100 Batch  700/781 - Loss:  1.369, Seconds: 2.83\n",
      "Epoch   6/100 Batch  720/781 - Loss:  1.562, Seconds: 2.94\n",
      "Epoch   6/100 Batch  740/781 - Loss:  1.403, Seconds: 3.07\n",
      "Epoch   6/100 Batch  760/781 - Loss:  1.377, Seconds: 2.79\n",
      "Average loss for this update: 1.33\n",
      "New Record!\n",
      "Epoch   6/100 Batch  780/781 - Loss:  1.171, Seconds: 2.62\n",
      "Epoch   7/100 Batch   20/781 - Loss:  1.467, Seconds: 2.71\n",
      "Epoch   7/100 Batch   40/781 - Loss:  1.326, Seconds: 2.24\n",
      "Epoch   7/100 Batch   60/781 - Loss:  1.306, Seconds: 2.78\n",
      "Epoch   7/100 Batch   80/781 - Loss:  1.322, Seconds: 2.66\n",
      "Epoch   7/100 Batch  100/781 - Loss:  1.144, Seconds: 2.67\n",
      "Epoch   7/100 Batch  120/781 - Loss:  1.242, Seconds: 2.47\n",
      "Epoch   7/100 Batch  140/781 - Loss:  1.150, Seconds: 2.58\n",
      "Epoch   7/100 Batch  160/781 - Loss:  1.399, Seconds: 2.29\n",
      "Epoch   7/100 Batch  180/781 - Loss:  1.348, Seconds: 2.50\n",
      "Epoch   7/100 Batch  200/781 - Loss:  1.294, Seconds: 2.69\n",
      "Epoch   7/100 Batch  220/781 - Loss:  1.263, Seconds: 2.58\n",
      "Epoch   7/100 Batch  240/781 - Loss:  1.055, Seconds: 2.62\n",
      "Average loss for this update: 1.27\n",
      "New Record!\n",
      "Epoch   7/100 Batch  260/781 - Loss:  1.179, Seconds: 2.93\n",
      "Epoch   7/100 Batch  280/781 - Loss:  1.205, Seconds: 2.73\n",
      "Epoch   7/100 Batch  300/781 - Loss:  1.367, Seconds: 2.70\n",
      "Epoch   7/100 Batch  320/781 - Loss:  1.409, Seconds: 2.65\n",
      "Epoch   7/100 Batch  340/781 - Loss:  1.248, Seconds: 2.70\n",
      "Epoch   7/100 Batch  360/781 - Loss:  1.264, Seconds: 2.80\n",
      "Epoch   7/100 Batch  380/781 - Loss:  1.086, Seconds: 2.89\n",
      "Epoch   7/100 Batch  400/781 - Loss:  1.224, Seconds: 2.89\n",
      "Epoch   7/100 Batch  420/781 - Loss:  1.161, Seconds: 2.58\n",
      "Epoch   7/100 Batch  440/781 - Loss:  1.275, Seconds: 2.93\n",
      "Epoch   7/100 Batch  460/781 - Loss:  1.356, Seconds: 3.02\n",
      "Epoch   7/100 Batch  480/781 - Loss:  1.193, Seconds: 2.63\n",
      "Epoch   7/100 Batch  500/781 - Loss:  1.245, Seconds: 2.74\n",
      "Average loss for this update: 1.24\n",
      "New Record!\n",
      "Epoch   7/100 Batch  520/781 - Loss:  1.094, Seconds: 2.45\n",
      "Epoch   7/100 Batch  540/781 - Loss:  1.174, Seconds: 2.96\n",
      "Epoch   7/100 Batch  560/781 - Loss:  1.108, Seconds: 2.63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   7/100 Batch  580/781 - Loss:  1.312, Seconds: 2.74\n",
      "Epoch   7/100 Batch  600/781 - Loss:  1.317, Seconds: 2.76\n",
      "Epoch   7/100 Batch  620/781 - Loss:  1.245, Seconds: 2.68\n",
      "Epoch   7/100 Batch  640/781 - Loss:  1.183, Seconds: 2.76\n",
      "Epoch   7/100 Batch  660/781 - Loss:  1.045, Seconds: 2.87\n",
      "Epoch   7/100 Batch  680/781 - Loss:  1.133, Seconds: 2.78\n",
      "Epoch   7/100 Batch  700/781 - Loss:  1.268, Seconds: 2.83\n",
      "Epoch   7/100 Batch  720/781 - Loss:  1.444, Seconds: 3.06\n",
      "Epoch   7/100 Batch  740/781 - Loss:  1.289, Seconds: 3.03\n",
      "Epoch   7/100 Batch  760/781 - Loss:  1.277, Seconds: 2.78\n",
      "Average loss for this update: 1.222\n",
      "New Record!\n",
      "Epoch   7/100 Batch  780/781 - Loss:  1.091, Seconds: 2.56\n",
      "Epoch   8/100 Batch   20/781 - Loss:  1.353, Seconds: 2.67\n",
      "Epoch   8/100 Batch   40/781 - Loss:  1.241, Seconds: 2.28\n",
      "Epoch   8/100 Batch   60/781 - Loss:  1.210, Seconds: 2.86\n",
      "Epoch   8/100 Batch   80/781 - Loss:  1.214, Seconds: 2.67\n",
      "Epoch   8/100 Batch  100/781 - Loss:  1.058, Seconds: 2.69\n",
      "Epoch   8/100 Batch  120/781 - Loss:  1.151, Seconds: 2.45\n",
      "Epoch   8/100 Batch  140/781 - Loss:  1.057, Seconds: 2.59\n",
      "Epoch   8/100 Batch  160/781 - Loss:  1.280, Seconds: 2.50\n",
      "Epoch   8/100 Batch  180/781 - Loss:  1.233, Seconds: 2.44\n",
      "Epoch   8/100 Batch  200/781 - Loss:  1.171, Seconds: 2.74\n",
      "Epoch   8/100 Batch  220/781 - Loss:  1.143, Seconds: 2.62\n",
      "Epoch   8/100 Batch  240/781 - Loss:  0.968, Seconds: 2.62\n",
      "Average loss for this update: 1.167\n",
      "New Record!\n",
      "Epoch   8/100 Batch  260/781 - Loss:  1.081, Seconds: 2.79\n",
      "Epoch   8/100 Batch  280/781 - Loss:  1.109, Seconds: 2.73\n",
      "Epoch   8/100 Batch  300/781 - Loss:  1.244, Seconds: 2.69\n",
      "Epoch   8/100 Batch  320/781 - Loss:  1.294, Seconds: 2.70\n",
      "Epoch   8/100 Batch  340/781 - Loss:  1.151, Seconds: 2.71\n",
      "Epoch   8/100 Batch  360/781 - Loss:  1.157, Seconds: 2.82\n",
      "Epoch   8/100 Batch  380/781 - Loss:  0.998, Seconds: 2.90\n",
      "Epoch   8/100 Batch  400/781 - Loss:  1.139, Seconds: 2.90\n",
      "Epoch   8/100 Batch  420/781 - Loss:  1.089, Seconds: 2.57\n",
      "Epoch   8/100 Batch  440/781 - Loss:  1.175, Seconds: 2.97\n",
      "Epoch   8/100 Batch  460/781 - Loss:  1.234, Seconds: 2.82\n",
      "Epoch   8/100 Batch  480/781 - Loss:  1.104, Seconds: 2.63\n",
      "Epoch   8/100 Batch  500/781 - Loss:  1.178, Seconds: 2.76\n",
      "Average loss for this update: 1.146\n",
      "New Record!\n",
      "Epoch   8/100 Batch  520/781 - Loss:  1.033, Seconds: 2.54\n",
      "Epoch   8/100 Batch  540/781 - Loss:  1.096, Seconds: 2.82\n",
      "Epoch   8/100 Batch  560/781 - Loss:  1.040, Seconds: 2.63\n",
      "Epoch   8/100 Batch  580/781 - Loss:  1.229, Seconds: 2.76\n",
      "Epoch   8/100 Batch  600/781 - Loss:  1.233, Seconds: 2.73\n",
      "Epoch   8/100 Batch  620/781 - Loss:  1.160, Seconds: 2.62\n",
      "Epoch   8/100 Batch  640/781 - Loss:  1.112, Seconds: 2.79\n",
      "Epoch   8/100 Batch  660/781 - Loss:  0.960, Seconds: 2.89\n",
      "Epoch   8/100 Batch  680/781 - Loss:  1.060, Seconds: 2.93\n",
      "Epoch   8/100 Batch  700/781 - Loss:  1.168, Seconds: 2.82\n",
      "Epoch   8/100 Batch  720/781 - Loss:  1.328, Seconds: 2.94\n",
      "Epoch   8/100 Batch  740/781 - Loss:  1.201, Seconds: 3.02\n",
      "Epoch   8/100 Batch  760/781 - Loss:  1.196, Seconds: 2.97\n",
      "Average loss for this update: 1.138\n",
      "New Record!\n",
      "Epoch   8/100 Batch  780/781 - Loss:  1.009, Seconds: 2.62\n",
      "Epoch   9/100 Batch   20/781 - Loss:  1.254, Seconds: 2.70\n",
      "Epoch   9/100 Batch   40/781 - Loss:  1.143, Seconds: 2.28\n",
      "Epoch   9/100 Batch   60/781 - Loss:  1.116, Seconds: 2.80\n",
      "Epoch   9/100 Batch   80/781 - Loss:  1.123, Seconds: 2.73\n",
      "Epoch   9/100 Batch  100/781 - Loss:  0.969, Seconds: 2.72\n",
      "Epoch   9/100 Batch  120/781 - Loss:  1.054, Seconds: 2.49\n",
      "Epoch   9/100 Batch  140/781 - Loss:  0.979, Seconds: 2.59\n",
      "Epoch   9/100 Batch  160/781 - Loss:  1.166, Seconds: 2.20\n",
      "Epoch   9/100 Batch  180/781 - Loss:  1.126, Seconds: 2.41\n",
      "Epoch   9/100 Batch  200/781 - Loss:  1.085, Seconds: 2.73\n",
      "Epoch   9/100 Batch  220/781 - Loss:  1.062, Seconds: 2.63\n",
      "Epoch   9/100 Batch  240/781 - Loss:  0.878, Seconds: 2.59\n",
      "Average loss for this update: 1.074\n",
      "New Record!\n",
      "Epoch   9/100 Batch  260/781 - Loss:  1.002, Seconds: 2.87\n",
      "Epoch   9/100 Batch  280/781 - Loss:  1.024, Seconds: 2.75\n",
      "Epoch   9/100 Batch  300/781 - Loss:  1.140, Seconds: 2.84\n",
      "Epoch   9/100 Batch  320/781 - Loss:  1.193, Seconds: 2.67\n",
      "Epoch   9/100 Batch  340/781 - Loss:  1.056, Seconds: 2.68\n",
      "Epoch   9/100 Batch  360/781 - Loss:  1.063, Seconds: 2.91\n",
      "Epoch   9/100 Batch  380/781 - Loss:  0.920, Seconds: 2.89\n",
      "Epoch   9/100 Batch  400/781 - Loss:  1.045, Seconds: 2.86\n",
      "Epoch   9/100 Batch  420/781 - Loss:  0.999, Seconds: 2.53\n",
      "Epoch   9/100 Batch  440/781 - Loss:  1.078, Seconds: 2.94\n",
      "Epoch   9/100 Batch  460/781 - Loss:  1.133, Seconds: 2.80\n",
      "Epoch   9/100 Batch  480/781 - Loss:  1.011, Seconds: 2.60\n",
      "Epoch   9/100 Batch  500/781 - Loss:  1.079, Seconds: 2.74\n",
      "Average loss for this update: 1.052\n",
      "New Record!\n",
      "Epoch   9/100 Batch  520/781 - Loss:  0.939, Seconds: 2.53\n",
      "Epoch   9/100 Batch  540/781 - Loss:  0.991, Seconds: 2.83\n",
      "Epoch   9/100 Batch  560/781 - Loss:  0.950, Seconds: 2.64\n",
      "Epoch   9/100 Batch  580/781 - Loss:  1.116, Seconds: 2.78\n",
      "Epoch   9/100 Batch  600/781 - Loss:  1.110, Seconds: 2.76\n",
      "Epoch   9/100 Batch  620/781 - Loss:  1.053, Seconds: 2.65\n",
      "Epoch   9/100 Batch  640/781 - Loss:  1.014, Seconds: 2.77\n",
      "Epoch   9/100 Batch  660/781 - Loss:  0.885, Seconds: 2.86\n",
      "Epoch   9/100 Batch  680/781 - Loss:  0.978, Seconds: 2.89\n",
      "Epoch   9/100 Batch  700/781 - Loss:  1.070, Seconds: 2.83\n",
      "Epoch   9/100 Batch  720/781 - Loss:  1.222, Seconds: 2.94\n",
      "Epoch   9/100 Batch  740/781 - Loss:  1.100, Seconds: 3.06\n",
      "Epoch   9/100 Batch  760/781 - Loss:  1.092, Seconds: 3.00\n",
      "Average loss for this update: 1.041\n",
      "New Record!\n",
      "Epoch   9/100 Batch  780/781 - Loss:  0.961, Seconds: 2.74\n",
      "Epoch  10/100 Batch   20/781 - Loss:  1.167, Seconds: 2.71\n",
      "Epoch  10/100 Batch   40/781 - Loss:  1.052, Seconds: 2.23\n",
      "Epoch  10/100 Batch   60/781 - Loss:  1.026, Seconds: 2.79\n",
      "Epoch  10/100 Batch   80/781 - Loss:  1.032, Seconds: 2.72\n",
      "Epoch  10/100 Batch  100/781 - Loss:  0.885, Seconds: 2.80\n",
      "Epoch  10/100 Batch  120/781 - Loss:  0.972, Seconds: 2.46\n",
      "Epoch  10/100 Batch  140/781 - Loss:  0.910, Seconds: 2.56\n",
      "Epoch  10/100 Batch  160/781 - Loss:  1.071, Seconds: 2.19\n",
      "Epoch  10/100 Batch  180/781 - Loss:  1.034, Seconds: 2.39\n",
      "Epoch  10/100 Batch  200/781 - Loss:  0.990, Seconds: 2.75\n",
      "Epoch  10/100 Batch  220/781 - Loss:  0.990, Seconds: 2.63\n",
      "Epoch  10/100 Batch  240/781 - Loss:  0.824, Seconds: 2.61\n",
      "Average loss for this update: 0.991\n",
      "New Record!\n",
      "Epoch  10/100 Batch  260/781 - Loss:  0.919, Seconds: 2.80\n",
      "Epoch  10/100 Batch  280/781 - Loss:  0.950, Seconds: 2.74\n",
      "Epoch  10/100 Batch  300/781 - Loss:  1.057, Seconds: 2.68\n",
      "Epoch  10/100 Batch  320/781 - Loss:  1.092, Seconds: 2.69\n",
      "Epoch  10/100 Batch  340/781 - Loss:  0.977, Seconds: 2.69\n",
      "Epoch  10/100 Batch  360/781 - Loss:  0.977, Seconds: 2.79\n",
      "Epoch  10/100 Batch  380/781 - Loss:  0.849, Seconds: 2.88\n",
      "Epoch  10/100 Batch  400/781 - Loss:  0.962, Seconds: 3.04\n",
      "Epoch  10/100 Batch  420/781 - Loss:  0.904, Seconds: 2.56\n",
      "Epoch  10/100 Batch  440/781 - Loss:  0.979, Seconds: 2.93\n",
      "Epoch  10/100 Batch  460/781 - Loss:  1.031, Seconds: 2.86\n",
      "Epoch  10/100 Batch  480/781 - Loss:  0.916, Seconds: 2.60\n",
      "Epoch  10/100 Batch  500/781 - Loss:  0.995, Seconds: 2.70\n",
      "Average loss for this update: 0.965\n",
      "New Record!\n",
      "Epoch  10/100 Batch  520/781 - Loss:  0.853, Seconds: 2.47\n",
      "Epoch  10/100 Batch  540/781 - Loss:  0.912, Seconds: 2.85\n",
      "Epoch  10/100 Batch  560/781 - Loss:  0.868, Seconds: 2.63\n",
      "Epoch  10/100 Batch  580/781 - Loss:  1.008, Seconds: 2.76\n",
      "Epoch  10/100 Batch  600/781 - Loss:  1.019, Seconds: 2.81\n",
      "Epoch  10/100 Batch  620/781 - Loss:  0.959, Seconds: 2.66\n",
      "Epoch  10/100 Batch  640/781 - Loss:  0.938, Seconds: 2.79\n",
      "Epoch  10/100 Batch  660/781 - Loss:  0.796, Seconds: 2.90\n",
      "Epoch  10/100 Batch  680/781 - Loss:  0.907, Seconds: 2.73\n",
      "Epoch  10/100 Batch  700/781 - Loss:  0.976, Seconds: 2.84\n",
      "Epoch  10/100 Batch  720/781 - Loss:  1.114, Seconds: 2.89\n",
      "Epoch  10/100 Batch  740/781 - Loss:  0.998, Seconds: 3.03\n",
      "Epoch  10/100 Batch  760/781 - Loss:  0.995, Seconds: 2.80\n",
      "Average loss for this update: 0.95\n",
      "New Record!\n",
      "Epoch  10/100 Batch  780/781 - Loss:  0.860, Seconds: 2.65\n",
      "Epoch  11/100 Batch   20/781 - Loss:  1.061, Seconds: 2.72\n",
      "Epoch  11/100 Batch   40/781 - Loss:  0.951, Seconds: 2.25\n",
      "Epoch  11/100 Batch   60/781 - Loss:  0.933, Seconds: 2.90\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  11/100 Batch   80/781 - Loss:  0.932, Seconds: 2.71\n",
      "Epoch  11/100 Batch  100/781 - Loss:  0.815, Seconds: 2.81\n",
      "Epoch  11/100 Batch  120/781 - Loss:  0.893, Seconds: 2.48\n",
      "Epoch  11/100 Batch  140/781 - Loss:  0.853, Seconds: 2.59\n",
      "Epoch  11/100 Batch  160/781 - Loss:  0.985, Seconds: 2.20\n",
      "Epoch  11/100 Batch  180/781 - Loss:  0.957, Seconds: 2.50\n",
      "Epoch  11/100 Batch  200/781 - Loss:  0.909, Seconds: 2.77\n",
      "Epoch  11/100 Batch  220/781 - Loss:  0.913, Seconds: 2.63\n",
      "Epoch  11/100 Batch  240/781 - Loss:  0.765, Seconds: 2.64\n",
      "Average loss for this update: 0.91\n",
      "New Record!\n",
      "Epoch  11/100 Batch  260/781 - Loss:  0.857, Seconds: 2.73\n",
      "Epoch  11/100 Batch  280/781 - Loss:  0.877, Seconds: 2.78\n",
      "Epoch  11/100 Batch  300/781 - Loss:  0.974, Seconds: 2.67\n",
      "Epoch  11/100 Batch  320/781 - Loss:  1.015, Seconds: 3.00\n",
      "Epoch  11/100 Batch  340/781 - Loss:  0.900, Seconds: 2.71\n",
      "Epoch  11/100 Batch  360/781 - Loss:  0.896, Seconds: 2.80\n",
      "Epoch  11/100 Batch  380/781 - Loss:  0.788, Seconds: 2.90\n",
      "Epoch  11/100 Batch  400/781 - Loss:  0.888, Seconds: 2.92\n",
      "Epoch  11/100 Batch  420/781 - Loss:  0.843, Seconds: 2.59\n",
      "Epoch  11/100 Batch  440/781 - Loss:  0.880, Seconds: 3.13\n",
      "Epoch  11/100 Batch  460/781 - Loss:  0.939, Seconds: 2.83\n",
      "Epoch  11/100 Batch  480/781 - Loss:  0.857, Seconds: 2.58\n",
      "Epoch  11/100 Batch  500/781 - Loss:  0.926, Seconds: 2.72\n",
      "Average loss for this update: 0.891\n",
      "New Record!\n",
      "Epoch  11/100 Batch  520/781 - Loss:  0.806, Seconds: 2.50\n",
      "Epoch  11/100 Batch  540/781 - Loss:  0.845, Seconds: 2.95\n",
      "Epoch  11/100 Batch  560/781 - Loss:  0.812, Seconds: 2.75\n",
      "Epoch  11/100 Batch  580/781 - Loss:  0.944, Seconds: 2.81\n",
      "Epoch  11/100 Batch  600/781 - Loss:  0.939, Seconds: 2.75\n",
      "Epoch  11/100 Batch  620/781 - Loss:  0.895, Seconds: 2.66\n",
      "Epoch  11/100 Batch  640/781 - Loss:  0.887, Seconds: 2.88\n",
      "Epoch  11/100 Batch  660/781 - Loss:  0.741, Seconds: 2.91\n",
      "Epoch  11/100 Batch  680/781 - Loss:  0.847, Seconds: 2.92\n",
      "Epoch  11/100 Batch  700/781 - Loss:  0.905, Seconds: 2.94\n",
      "Epoch  11/100 Batch  720/781 - Loss:  1.035, Seconds: 2.90\n",
      "Epoch  11/100 Batch  740/781 - Loss:  0.938, Seconds: 3.01\n",
      "Epoch  11/100 Batch  760/781 - Loss:  0.951, Seconds: 2.97\n",
      "Average loss for this update: 0.89\n",
      "New Record!\n",
      "Epoch  11/100 Batch  780/781 - Loss:  0.826, Seconds: 2.58\n",
      "Epoch  12/100 Batch   20/781 - Loss:  0.997, Seconds: 2.72\n",
      "Epoch  12/100 Batch   40/781 - Loss:  0.868, Seconds: 2.27\n",
      "Epoch  12/100 Batch   60/781 - Loss:  0.864, Seconds: 2.80\n",
      "Epoch  12/100 Batch   80/781 - Loss:  0.875, Seconds: 2.86\n",
      "Epoch  12/100 Batch  100/781 - Loss:  0.754, Seconds: 2.65\n",
      "Epoch  12/100 Batch  120/781 - Loss:  0.813, Seconds: 2.49\n",
      "Epoch  12/100 Batch  140/781 - Loss:  0.793, Seconds: 2.54\n",
      "Epoch  12/100 Batch  160/781 - Loss:  0.904, Seconds: 2.22\n",
      "Epoch  12/100 Batch  180/781 - Loss:  0.882, Seconds: 2.44\n",
      "Epoch  12/100 Batch  200/781 - Loss:  0.840, Seconds: 2.89\n",
      "Epoch  12/100 Batch  220/781 - Loss:  0.851, Seconds: 2.67\n",
      "Epoch  12/100 Batch  240/781 - Loss:  0.707, Seconds: 2.63\n",
      "Average loss for this update: 0.842\n",
      "New Record!\n",
      "Epoch  12/100 Batch  260/781 - Loss:  0.793, Seconds: 2.84\n",
      "Epoch  12/100 Batch  280/781 - Loss:  0.798, Seconds: 2.73\n",
      "Epoch  12/100 Batch  300/781 - Loss:  0.889, Seconds: 2.67\n",
      "Epoch  12/100 Batch  320/781 - Loss:  0.930, Seconds: 2.69\n",
      "Epoch  12/100 Batch  340/781 - Loss:  0.832, Seconds: 2.68\n",
      "Epoch  12/100 Batch  360/781 - Loss:  0.830, Seconds: 2.82\n",
      "Epoch  12/100 Batch  380/781 - Loss:  0.738, Seconds: 2.87\n",
      "Epoch  12/100 Batch  400/781 - Loss:  0.829, Seconds: 2.88\n",
      "Epoch  12/100 Batch  420/781 - Loss:  0.790, Seconds: 2.57\n",
      "Epoch  12/100 Batch  440/781 - Loss:  0.827, Seconds: 2.92\n",
      "Epoch  12/100 Batch  460/781 - Loss:  0.885, Seconds: 2.85\n",
      "Epoch  12/100 Batch  480/781 - Loss:  0.811, Seconds: 2.61\n",
      "Epoch  12/100 Batch  500/781 - Loss:  0.863, Seconds: 2.74\n",
      "Average loss for this update: 0.829\n",
      "New Record!\n",
      "Epoch  12/100 Batch  520/781 - Loss:  0.751, Seconds: 2.54\n",
      "Epoch  12/100 Batch  540/781 - Loss:  0.791, Seconds: 2.87\n",
      "Epoch  12/100 Batch  560/781 - Loss:  0.745, Seconds: 2.59\n",
      "Epoch  12/100 Batch  580/781 - Loss:  0.855, Seconds: 2.77\n",
      "Epoch  12/100 Batch  600/781 - Loss:  0.859, Seconds: 2.78\n",
      "Epoch  12/100 Batch  620/781 - Loss:  0.818, Seconds: 2.82\n",
      "Epoch  12/100 Batch  640/781 - Loss:  0.815, Seconds: 2.79\n",
      "Epoch  12/100 Batch  660/781 - Loss:  0.676, Seconds: 2.88\n",
      "Epoch  12/100 Batch  680/781 - Loss:  0.776, Seconds: 2.76\n",
      "Epoch  12/100 Batch  700/781 - Loss:  0.844, Seconds: 2.85\n",
      "Epoch  12/100 Batch  720/781 - Loss:  0.951, Seconds: 2.97\n",
      "Epoch  12/100 Batch  740/781 - Loss:  0.864, Seconds: 3.11\n",
      "Epoch  12/100 Batch  760/781 - Loss:  0.883, Seconds: 2.82\n",
      "Average loss for this update: 0.819\n",
      "New Record!\n",
      "Epoch  12/100 Batch  780/781 - Loss:  0.763, Seconds: 2.59\n",
      "Epoch  13/100 Batch   20/781 - Loss:  0.941, Seconds: 2.67\n",
      "Epoch  13/100 Batch   40/781 - Loss:  0.791, Seconds: 2.25\n",
      "Epoch  13/100 Batch   60/781 - Loss:  0.802, Seconds: 2.81\n",
      "Epoch  13/100 Batch   80/781 - Loss:  0.801, Seconds: 2.71\n",
      "Epoch  13/100 Batch  100/781 - Loss:  0.698, Seconds: 2.81\n",
      "Epoch  13/100 Batch  120/781 - Loss:  0.759, Seconds: 2.48\n",
      "Epoch  13/100 Batch  140/781 - Loss:  0.732, Seconds: 2.58\n",
      "Epoch  13/100 Batch  160/781 - Loss:  0.814, Seconds: 2.19\n",
      "Epoch  13/100 Batch  180/781 - Loss:  0.802, Seconds: 2.44\n",
      "Epoch  13/100 Batch  200/781 - Loss:  0.770, Seconds: 2.77\n",
      "Epoch  13/100 Batch  220/781 - Loss:  0.791, Seconds: 2.65\n",
      "Epoch  13/100 Batch  240/781 - Loss:  0.654, Seconds: 2.61\n",
      "Average loss for this update: 0.777\n",
      "New Record!\n",
      "Epoch  13/100 Batch  260/781 - Loss:  0.744, Seconds: 2.77\n",
      "Epoch  13/100 Batch  280/781 - Loss:  0.757, Seconds: 2.75\n",
      "Epoch  13/100 Batch  300/781 - Loss:  0.822, Seconds: 2.70\n",
      "Epoch  13/100 Batch  320/781 - Loss:  0.860, Seconds: 2.66\n",
      "Epoch  13/100 Batch  340/781 - Loss:  0.772, Seconds: 2.67\n",
      "Epoch  13/100 Batch  360/781 - Loss:  0.768, Seconds: 2.77\n",
      "Epoch  13/100 Batch  380/781 - Loss:  0.668, Seconds: 2.93\n",
      "Epoch  13/100 Batch  400/781 - Loss:  0.772, Seconds: 2.87\n",
      "Epoch  13/100 Batch  420/781 - Loss:  0.727, Seconds: 2.55\n",
      "Epoch  13/100 Batch  440/781 - Loss:  0.752, Seconds: 2.95\n",
      "Epoch  13/100 Batch  460/781 - Loss:  0.804, Seconds: 2.86\n",
      "Epoch  13/100 Batch  480/781 - Loss:  0.750, Seconds: 2.62\n",
      "Epoch  13/100 Batch  500/781 - Loss:  0.810, Seconds: 2.73\n",
      "Average loss for this update: 0.766\n",
      "New Record!\n",
      "Epoch  13/100 Batch  520/781 - Loss:  0.703, Seconds: 2.72\n",
      "Epoch  13/100 Batch  540/781 - Loss:  0.749, Seconds: 2.85\n",
      "Epoch  13/100 Batch  560/781 - Loss:  0.705, Seconds: 2.64\n",
      "Epoch  13/100 Batch  580/781 - Loss:  0.779, Seconds: 2.81\n",
      "Epoch  13/100 Batch  600/781 - Loss:  0.809, Seconds: 2.80\n",
      "Epoch  13/100 Batch  620/781 - Loss:  0.757, Seconds: 2.66\n",
      "Epoch  13/100 Batch  640/781 - Loss:  0.762, Seconds: 2.77\n",
      "Epoch  13/100 Batch  660/781 - Loss:  0.636, Seconds: 2.84\n",
      "Epoch  13/100 Batch  680/781 - Loss:  0.722, Seconds: 2.78\n",
      "Epoch  13/100 Batch  700/781 - Loss:  0.788, Seconds: 2.83\n",
      "Epoch  13/100 Batch  720/781 - Loss:  0.871, Seconds: 2.96\n",
      "Epoch  13/100 Batch  740/781 - Loss:  0.789, Seconds: 3.09\n",
      "Epoch  13/100 Batch  760/781 - Loss:  0.822, Seconds: 2.83\n",
      "Average loss for this update: 0.761\n",
      "New Record!\n",
      "Epoch  13/100 Batch  780/781 - Loss:  0.701, Seconds: 2.63\n",
      "Epoch  14/100 Batch   20/781 - Loss:  0.856, Seconds: 2.73\n",
      "Epoch  14/100 Batch   40/781 - Loss:  0.737, Seconds: 2.28\n",
      "Epoch  14/100 Batch   60/781 - Loss:  0.744, Seconds: 2.83\n",
      "Epoch  14/100 Batch   80/781 - Loss:  0.740, Seconds: 2.71\n",
      "Epoch  14/100 Batch  100/781 - Loss:  0.654, Seconds: 2.69\n",
      "Epoch  14/100 Batch  120/781 - Loss:  0.711, Seconds: 2.59\n",
      "Epoch  14/100 Batch  140/781 - Loss:  0.680, Seconds: 2.58\n",
      "Epoch  14/100 Batch  160/781 - Loss:  0.756, Seconds: 2.20\n",
      "Epoch  14/100 Batch  180/781 - Loss:  0.747, Seconds: 2.40\n",
      "Epoch  14/100 Batch  200/781 - Loss:  0.721, Seconds: 2.76\n",
      "Epoch  14/100 Batch  220/781 - Loss:  0.750, Seconds: 2.64\n",
      "Epoch  14/100 Batch  240/781 - Loss:  0.620, Seconds: 2.60\n",
      "Average loss for this update: 0.724\n",
      "New Record!\n",
      "Epoch  14/100 Batch  260/781 - Loss:  0.688, Seconds: 2.77\n",
      "Epoch  14/100 Batch  280/781 - Loss:  0.706, Seconds: 2.74\n",
      "Epoch  14/100 Batch  300/781 - Loss:  0.784, Seconds: 2.67\n",
      "Epoch  14/100 Batch  320/781 - Loss:  0.801, Seconds: 2.80\n",
      "Epoch  14/100 Batch  340/781 - Loss:  0.726, Seconds: 2.65\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  14/100 Batch  360/781 - Loss:  0.725, Seconds: 2.76\n",
      "Epoch  14/100 Batch  380/781 - Loss:  0.631, Seconds: 2.90\n",
      "Epoch  14/100 Batch  400/781 - Loss:  0.717, Seconds: 2.89\n",
      "Epoch  14/100 Batch  420/781 - Loss:  0.661, Seconds: 2.54\n",
      "Epoch  14/100 Batch  440/781 - Loss:  0.690, Seconds: 2.96\n",
      "Epoch  14/100 Batch  460/781 - Loss:  0.747, Seconds: 2.84\n",
      "Epoch  14/100 Batch  480/781 - Loss:  0.705, Seconds: 2.62\n",
      "Epoch  14/100 Batch  500/781 - Loss:  0.745, Seconds: 2.75\n",
      "Average loss for this update: 0.714\n",
      "New Record!\n",
      "Epoch  14/100 Batch  520/781 - Loss:  0.648, Seconds: 2.47\n",
      "Epoch  14/100 Batch  540/781 - Loss:  0.701, Seconds: 2.95\n",
      "Epoch  14/100 Batch  560/781 - Loss:  0.647, Seconds: 2.61\n",
      "Epoch  14/100 Batch  580/781 - Loss:  0.720, Seconds: 2.76\n",
      "Epoch  14/100 Batch  600/781 - Loss:  0.733, Seconds: 2.95\n",
      "Epoch  14/100 Batch  620/781 - Loss:  0.690, Seconds: 2.65\n",
      "Epoch  14/100 Batch  640/781 - Loss:  0.708, Seconds: 2.83\n",
      "Epoch  14/100 Batch  660/781 - Loss:  0.583, Seconds: 2.86\n",
      "Epoch  14/100 Batch  680/781 - Loss:  0.677, Seconds: 2.78\n",
      "Epoch  14/100 Batch  700/781 - Loss:  0.744, Seconds: 2.81\n",
      "Epoch  14/100 Batch  720/781 - Loss:  0.825, Seconds: 2.94\n",
      "Epoch  14/100 Batch  740/781 - Loss:  0.756, Seconds: 3.06\n",
      "Epoch  14/100 Batch  760/781 - Loss:  0.800, Seconds: 3.06\n",
      "Average loss for this update: 0.712\n",
      "New Record!\n",
      "Epoch  14/100 Batch  780/781 - Loss:  0.676, Seconds: 2.61\n",
      "Epoch  15/100 Batch   20/781 - Loss:  0.808, Seconds: 2.71\n",
      "Epoch  15/100 Batch   40/781 - Loss:  0.702, Seconds: 2.29\n",
      "Epoch  15/100 Batch   60/781 - Loss:  0.717, Seconds: 2.78\n",
      "Epoch  15/100 Batch   80/781 - Loss:  0.701, Seconds: 2.70\n",
      "Epoch  15/100 Batch  100/781 - Loss:  0.622, Seconds: 2.68\n",
      "Epoch  15/100 Batch  120/781 - Loss:  0.662, Seconds: 2.45\n",
      "Epoch  15/100 Batch  140/781 - Loss:  0.631, Seconds: 2.57\n",
      "Epoch  15/100 Batch  160/781 - Loss:  0.704, Seconds: 2.17\n",
      "Epoch  15/100 Batch  180/781 - Loss:  0.699, Seconds: 2.37\n",
      "Epoch  15/100 Batch  200/781 - Loss:  0.664, Seconds: 2.76\n",
      "Epoch  15/100 Batch  220/781 - Loss:  0.684, Seconds: 2.62\n",
      "Epoch  15/100 Batch  240/781 - Loss:  0.572, Seconds: 2.61\n",
      "Average loss for this update: 0.678\n",
      "New Record!\n",
      "Epoch  15/100 Batch  260/781 - Loss:  0.640, Seconds: 2.73\n",
      "Epoch  15/100 Batch  280/781 - Loss:  0.647, Seconds: 2.75\n",
      "Epoch  15/100 Batch  300/781 - Loss:  0.719, Seconds: 2.62\n",
      "Epoch  15/100 Batch  320/781 - Loss:  0.752, Seconds: 2.69\n",
      "Epoch  15/100 Batch  340/781 - Loss:  0.664, Seconds: 2.71\n",
      "Epoch  15/100 Batch  360/781 - Loss:  0.657, Seconds: 2.81\n",
      "Epoch  15/100 Batch  380/781 - Loss:  0.593, Seconds: 2.85\n",
      "Epoch  15/100 Batch  400/781 - Loss:  0.669, Seconds: 2.84\n",
      "Epoch  15/100 Batch  420/781 - Loss:  0.611, Seconds: 2.59\n",
      "Epoch  15/100 Batch  440/781 - Loss:  0.646, Seconds: 2.94\n",
      "Epoch  15/100 Batch  460/781 - Loss:  0.707, Seconds: 2.84\n",
      "Epoch  15/100 Batch  480/781 - Loss:  0.659, Seconds: 2.62\n",
      "Epoch  15/100 Batch  500/781 - Loss:  0.704, Seconds: 2.72\n",
      "Average loss for this update: 0.663\n",
      "New Record!\n",
      "Epoch  15/100 Batch  520/781 - Loss:  0.602, Seconds: 2.58\n",
      "Epoch  15/100 Batch  540/781 - Loss:  0.639, Seconds: 2.79\n",
      "Epoch  15/100 Batch  560/781 - Loss:  0.597, Seconds: 2.59\n",
      "Epoch  15/100 Batch  580/781 - Loss:  0.676, Seconds: 2.78\n",
      "Epoch  15/100 Batch  600/781 - Loss:  0.682, Seconds: 2.72\n",
      "Epoch  15/100 Batch  620/781 - Loss:  0.662, Seconds: 2.72\n",
      "Epoch  15/100 Batch  640/781 - Loss:  0.683, Seconds: 2.78\n",
      "Epoch  15/100 Batch  660/781 - Loss:  0.570, Seconds: 2.87\n",
      "Epoch  15/100 Batch  680/781 - Loss:  0.640, Seconds: 2.81\n",
      "Epoch  15/100 Batch  700/781 - Loss:  0.699, Seconds: 2.83\n",
      "Epoch  15/100 Batch  720/781 - Loss:  0.778, Seconds: 2.94\n",
      "Epoch  15/100 Batch  740/781 - Loss:  0.709, Seconds: 3.07\n",
      "Epoch  15/100 Batch  760/781 - Loss:  0.726, Seconds: 2.76\n",
      "Average loss for this update: 0.669\n",
      "No Improvement.\n",
      "Epoch  15/100 Batch  780/781 - Loss:  0.632, Seconds: 2.59\n",
      "Epoch  16/100 Batch   20/781 - Loss:  0.747, Seconds: 2.68\n",
      "Epoch  16/100 Batch   40/781 - Loss:  0.644, Seconds: 2.21\n",
      "Epoch  16/100 Batch   60/781 - Loss:  0.665, Seconds: 2.79\n",
      "Epoch  16/100 Batch   80/781 - Loss:  0.644, Seconds: 2.83\n",
      "Epoch  16/100 Batch  100/781 - Loss:  0.567, Seconds: 2.82\n",
      "Epoch  16/100 Batch  120/781 - Loss:  0.623, Seconds: 2.48\n",
      "Epoch  16/100 Batch  140/781 - Loss:  0.587, Seconds: 2.56\n",
      "Epoch  16/100 Batch  160/781 - Loss:  0.652, Seconds: 2.29\n",
      "Epoch  16/100 Batch  180/781 - Loss:  0.650, Seconds: 2.43\n",
      "Epoch  16/100 Batch  200/781 - Loss:  0.631, Seconds: 2.73\n",
      "Epoch  16/100 Batch  220/781 - Loss:  0.640, Seconds: 2.65\n",
      "Epoch  16/100 Batch  240/781 - Loss:  0.529, Seconds: 2.64\n",
      "Average loss for this update: 0.63\n",
      "New Record!\n",
      "Epoch  16/100 Batch  260/781 - Loss:  0.604, Seconds: 2.80\n",
      "Epoch  16/100 Batch  280/781 - Loss:  0.609, Seconds: 2.75\n",
      "Epoch  16/100 Batch  300/781 - Loss:  0.666, Seconds: 2.69\n",
      "Epoch  16/100 Batch  320/781 - Loss:  0.693, Seconds: 2.67\n",
      "Epoch  16/100 Batch  340/781 - Loss:  0.625, Seconds: 2.65\n",
      "Epoch  16/100 Batch  360/781 - Loss:  0.619, Seconds: 2.81\n",
      "Epoch  16/100 Batch  380/781 - Loss:  0.566, Seconds: 2.94\n",
      "Epoch  16/100 Batch  400/781 - Loss:  0.630, Seconds: 2.91\n",
      "Epoch  16/100 Batch  420/781 - Loss:  0.592, Seconds: 2.54\n",
      "Epoch  16/100 Batch  440/781 - Loss:  0.617, Seconds: 2.94\n",
      "Epoch  16/100 Batch  460/781 - Loss:  0.655, Seconds: 2.83\n",
      "Epoch  16/100 Batch  480/781 - Loss:  0.615, Seconds: 2.61\n",
      "Epoch  16/100 Batch  500/781 - Loss:  0.665, Seconds: 2.71\n",
      "Average loss for this update: 0.625\n",
      "New Record!\n",
      "Epoch  16/100 Batch  520/781 - Loss:  0.572, Seconds: 2.51\n",
      "Epoch  16/100 Batch  540/781 - Loss:  0.610, Seconds: 2.98\n",
      "Epoch  16/100 Batch  560/781 - Loss:  0.572, Seconds: 2.61\n",
      "Epoch  16/100 Batch  580/781 - Loss:  0.632, Seconds: 2.79\n",
      "Epoch  16/100 Batch  600/781 - Loss:  0.647, Seconds: 2.78\n",
      "Epoch  16/100 Batch  620/781 - Loss:  0.620, Seconds: 2.63\n",
      "Epoch  16/100 Batch  640/781 - Loss:  0.622, Seconds: 2.76\n",
      "Epoch  16/100 Batch  660/781 - Loss:  0.530, Seconds: 2.87\n",
      "Epoch  16/100 Batch  680/781 - Loss:  0.600, Seconds: 2.75\n",
      "Epoch  16/100 Batch  700/781 - Loss:  0.651, Seconds: 2.83\n",
      "Epoch  16/100 Batch  720/781 - Loss:  0.731, Seconds: 2.97\n",
      "Epoch  16/100 Batch  740/781 - Loss:  0.641, Seconds: 3.05\n",
      "Epoch  16/100 Batch  760/781 - Loss:  0.661, Seconds: 2.78\n",
      "Average loss for this update: 0.622\n",
      "New Record!\n",
      "Epoch  16/100 Batch  780/781 - Loss:  0.566, Seconds: 2.59\n",
      "Epoch  17/100 Batch   20/781 - Loss:  0.690, Seconds: 2.79\n",
      "Epoch  17/100 Batch   40/781 - Loss:  0.604, Seconds: 2.27\n",
      "Epoch  17/100 Batch   60/781 - Loss:  0.618, Seconds: 2.82\n",
      "Epoch  17/100 Batch   80/781 - Loss:  0.611, Seconds: 2.74\n",
      "Epoch  17/100 Batch  100/781 - Loss:  0.546, Seconds: 2.72\n",
      "Epoch  17/100 Batch  120/781 - Loss:  0.572, Seconds: 2.59\n",
      "Epoch  17/100 Batch  140/781 - Loss:  0.554, Seconds: 2.60\n",
      "Epoch  17/100 Batch  160/781 - Loss:  0.602, Seconds: 2.18\n",
      "Epoch  17/100 Batch  180/781 - Loss:  0.594, Seconds: 2.43\n",
      "Epoch  17/100 Batch  200/781 - Loss:  0.600, Seconds: 2.75\n",
      "Epoch  17/100 Batch  220/781 - Loss:  0.599, Seconds: 2.61\n",
      "Epoch  17/100 Batch  240/781 - Loss:  0.491, Seconds: 2.60\n",
      "Average loss for this update: 0.589\n",
      "New Record!\n",
      "Epoch  17/100 Batch  260/781 - Loss:  0.569, Seconds: 2.84\n",
      "Epoch  17/100 Batch  280/781 - Loss:  0.563, Seconds: 2.75\n",
      "Epoch  17/100 Batch  300/781 - Loss:  0.624, Seconds: 2.71\n",
      "Epoch  17/100 Batch  320/781 - Loss:  0.648, Seconds: 2.64\n",
      "Epoch  17/100 Batch  340/781 - Loss:  0.584, Seconds: 2.64\n",
      "Epoch  17/100 Batch  360/781 - Loss:  0.575, Seconds: 2.77\n",
      "Epoch  17/100 Batch  380/781 - Loss:  0.516, Seconds: 2.92\n",
      "Epoch  17/100 Batch  400/781 - Loss:  0.591, Seconds: 2.89\n",
      "Epoch  17/100 Batch  420/781 - Loss:  0.552, Seconds: 2.58\n",
      "Epoch  17/100 Batch  440/781 - Loss:  0.559, Seconds: 2.93\n",
      "Epoch  17/100 Batch  460/781 - Loss:  0.592, Seconds: 2.95\n",
      "Epoch  17/100 Batch  480/781 - Loss:  0.571, Seconds: 2.61\n",
      "Epoch  17/100 Batch  500/781 - Loss:  0.627, Seconds: 2.73\n",
      "Average loss for this update: 0.58\n",
      "New Record!\n",
      "Epoch  17/100 Batch  520/781 - Loss:  0.532, Seconds: 2.55\n",
      "Epoch  17/100 Batch  540/781 - Loss:  0.564, Seconds: 2.82\n",
      "Epoch  17/100 Batch  560/781 - Loss:  0.515, Seconds: 2.61\n",
      "Epoch  17/100 Batch  580/781 - Loss:  0.581, Seconds: 2.80\n",
      "Epoch  17/100 Batch  600/781 - Loss:  0.589, Seconds: 2.78\n",
      "Epoch  17/100 Batch  620/781 - Loss:  0.559, Seconds: 2.67\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  17/100 Batch  640/781 - Loss:  0.583, Seconds: 2.74\n",
      "Epoch  17/100 Batch  660/781 - Loss:  0.495, Seconds: 2.87\n",
      "Epoch  17/100 Batch  680/781 - Loss:  0.552, Seconds: 2.76\n",
      "Epoch  17/100 Batch  700/781 - Loss:  0.587, Seconds: 2.84\n",
      "Epoch  17/100 Batch  720/781 - Loss:  0.647, Seconds: 2.97\n",
      "Epoch  17/100 Batch  740/781 - Loss:  0.607, Seconds: 3.06\n",
      "Epoch  17/100 Batch  760/781 - Loss:  0.618, Seconds: 2.85\n",
      "Average loss for this update: 0.572\n",
      "New Record!\n",
      "Epoch  17/100 Batch  780/781 - Loss:  0.539, Seconds: 2.59\n",
      "Epoch  18/100 Batch   20/781 - Loss:  0.644, Seconds: 2.66\n",
      "Epoch  18/100 Batch   40/781 - Loss:  0.547, Seconds: 2.24\n",
      "Epoch  18/100 Batch   60/781 - Loss:  0.576, Seconds: 2.83\n",
      "Epoch  18/100 Batch   80/781 - Loss:  0.564, Seconds: 2.72\n",
      "Epoch  18/100 Batch  100/781 - Loss:  0.500, Seconds: 2.67\n",
      "Epoch  18/100 Batch  120/781 - Loss:  0.527, Seconds: 2.46\n",
      "Epoch  18/100 Batch  140/781 - Loss:  0.508, Seconds: 2.59\n",
      "Epoch  18/100 Batch  160/781 - Loss:  0.554, Seconds: 2.21\n",
      "Epoch  18/100 Batch  180/781 - Loss:  0.559, Seconds: 2.42\n",
      "Epoch  18/100 Batch  200/781 - Loss:  0.541, Seconds: 2.97\n",
      "Epoch  18/100 Batch  220/781 - Loss:  0.560, Seconds: 2.62\n",
      "Epoch  18/100 Batch  240/781 - Loss:  0.461, Seconds: 2.62\n",
      "Average loss for this update: 0.544\n",
      "New Record!\n",
      "Epoch  18/100 Batch  260/781 - Loss:  0.527, Seconds: 2.77\n",
      "Epoch  18/100 Batch  280/781 - Loss:  0.522, Seconds: 2.71\n",
      "Epoch  18/100 Batch  300/781 - Loss:  0.583, Seconds: 2.63\n",
      "Epoch  18/100 Batch  320/781 - Loss:  0.590, Seconds: 2.70\n",
      "Epoch  18/100 Batch  340/781 - Loss:  0.548, Seconds: 2.70\n",
      "Epoch  18/100 Batch  360/781 - Loss:  0.544, Seconds: 2.88\n",
      "Epoch  18/100 Batch  380/781 - Loss:  0.491, Seconds: 2.88\n",
      "Epoch  18/100 Batch  400/781 - Loss:  0.550, Seconds: 2.91\n",
      "Epoch  18/100 Batch  420/781 - Loss:  0.516, Seconds: 2.57\n",
      "Epoch  18/100 Batch  440/781 - Loss:  0.521, Seconds: 2.94\n",
      "Epoch  18/100 Batch  460/781 - Loss:  0.558, Seconds: 2.87\n",
      "Epoch  18/100 Batch  480/781 - Loss:  0.542, Seconds: 2.60\n",
      "Epoch  18/100 Batch  500/781 - Loss:  0.572, Seconds: 2.72\n",
      "Average loss for this update: 0.541\n",
      "New Record!\n",
      "Epoch  18/100 Batch  520/781 - Loss:  0.501, Seconds: 2.56\n",
      "Epoch  18/100 Batch  540/781 - Loss:  0.534, Seconds: 2.85\n",
      "Epoch  18/100 Batch  560/781 - Loss:  0.488, Seconds: 2.60\n",
      "Epoch  18/100 Batch  580/781 - Loss:  0.606, Seconds: 2.74\n",
      "Epoch  18/100 Batch  600/781 - Loss:  0.570, Seconds: 2.78\n",
      "Epoch  18/100 Batch  620/781 - Loss:  0.539, Seconds: 2.71\n",
      "Epoch  18/100 Batch  640/781 - Loss:  0.563, Seconds: 2.77\n",
      "Epoch  18/100 Batch  660/781 - Loss:  0.477, Seconds: 2.86\n",
      "Epoch  18/100 Batch  680/781 - Loss:  0.514, Seconds: 2.79\n",
      "Epoch  18/100 Batch  700/781 - Loss:  0.579, Seconds: 2.82\n",
      "Epoch  18/100 Batch  720/781 - Loss:  0.613, Seconds: 2.92\n",
      "Epoch  18/100 Batch  740/781 - Loss:  0.576, Seconds: 3.05\n",
      "Epoch  18/100 Batch  760/781 - Loss:  0.575, Seconds: 2.81\n",
      "Average loss for this update: 0.548\n",
      "No Improvement.\n",
      "Epoch  18/100 Batch  780/781 - Loss:  0.501, Seconds: 2.59\n",
      "Epoch  19/100 Batch   20/781 - Loss:  0.615, Seconds: 2.71\n",
      "Epoch  19/100 Batch   40/781 - Loss:  0.525, Seconds: 2.23\n",
      "Epoch  19/100 Batch   60/781 - Loss:  0.529, Seconds: 2.84\n",
      "Epoch  19/100 Batch   80/781 - Loss:  0.541, Seconds: 2.69\n",
      "Epoch  19/100 Batch  100/781 - Loss:  0.486, Seconds: 2.68\n",
      "Epoch  19/100 Batch  120/781 - Loss:  0.507, Seconds: 2.48\n",
      "Epoch  19/100 Batch  140/781 - Loss:  0.485, Seconds: 2.60\n",
      "Epoch  19/100 Batch  160/781 - Loss:  0.523, Seconds: 2.19\n",
      "Epoch  19/100 Batch  180/781 - Loss:  0.533, Seconds: 2.43\n",
      "Epoch  19/100 Batch  200/781 - Loss:  0.512, Seconds: 2.74\n",
      "Epoch  19/100 Batch  220/781 - Loss:  0.534, Seconds: 2.62\n",
      "Epoch  19/100 Batch  240/781 - Loss:  0.439, Seconds: 2.65\n",
      "Average loss for this update: 0.518\n",
      "New Record!\n",
      "Epoch  19/100 Batch  260/781 - Loss:  0.499, Seconds: 2.82\n",
      "Epoch  19/100 Batch  280/781 - Loss:  0.497, Seconds: 2.78\n",
      "Epoch  19/100 Batch  300/781 - Loss:  0.555, Seconds: 2.68\n",
      "Epoch  19/100 Batch  320/781 - Loss:  0.567, Seconds: 2.65\n",
      "Epoch  19/100 Batch  340/781 - Loss:  0.515, Seconds: 2.66\n",
      "Epoch  19/100 Batch  360/781 - Loss:  0.504, Seconds: 2.79\n",
      "Epoch  19/100 Batch  380/781 - Loss:  0.450, Seconds: 2.87\n",
      "Epoch  19/100 Batch  400/781 - Loss:  0.506, Seconds: 3.01\n",
      "Epoch  19/100 Batch  420/781 - Loss:  0.471, Seconds: 2.55\n",
      "Epoch  19/100 Batch  440/781 - Loss:  0.496, Seconds: 2.95\n",
      "Epoch  19/100 Batch  460/781 - Loss:  0.539, Seconds: 2.84\n",
      "Epoch  19/100 Batch  480/781 - Loss:  0.519, Seconds: 2.62\n",
      "Epoch  19/100 Batch  500/781 - Loss:  0.566, Seconds: 2.73\n",
      "Average loss for this update: 0.513\n",
      "New Record!\n",
      "Epoch  19/100 Batch  520/781 - Loss:  0.478, Seconds: 2.52\n",
      "Epoch  19/100 Batch  540/781 - Loss:  0.505, Seconds: 2.88\n",
      "Epoch  19/100 Batch  560/781 - Loss:  0.465, Seconds: 2.63\n",
      "Epoch  19/100 Batch  580/781 - Loss:  0.515, Seconds: 2.80\n",
      "Epoch  19/100 Batch  600/781 - Loss:  0.522, Seconds: 2.81\n",
      "Epoch  19/100 Batch  620/781 - Loss:  0.486, Seconds: 2.72\n",
      "Epoch  19/100 Batch  640/781 - Loss:  0.498, Seconds: 2.74\n",
      "Epoch  19/100 Batch  660/781 - Loss:  0.432, Seconds: 2.83\n",
      "Epoch  19/100 Batch  680/781 - Loss:  0.481, Seconds: 2.78\n",
      "Epoch  19/100 Batch  700/781 - Loss:  0.530, Seconds: 2.84\n",
      "Epoch  19/100 Batch  720/781 - Loss:  0.574, Seconds: 3.07\n",
      "Epoch  19/100 Batch  740/781 - Loss:  0.514, Seconds: 3.05\n",
      "Epoch  19/100 Batch  760/781 - Loss:  0.556, Seconds: 2.82\n",
      "Average loss for this update: 0.504\n",
      "New Record!\n",
      "Epoch  19/100 Batch  780/781 - Loss:  0.478, Seconds: 2.58\n",
      "Epoch  20/100 Batch   20/781 - Loss:  0.580, Seconds: 2.71\n",
      "Epoch  20/100 Batch   40/781 - Loss:  0.487, Seconds: 2.23\n",
      "Epoch  20/100 Batch   60/781 - Loss:  0.509, Seconds: 2.76\n",
      "Epoch  20/100 Batch   80/781 - Loss:  0.497, Seconds: 2.72\n",
      "Epoch  20/100 Batch  100/781 - Loss:  0.438, Seconds: 2.68\n",
      "Epoch  20/100 Batch  120/781 - Loss:  0.466, Seconds: 2.47\n",
      "Epoch  20/100 Batch  140/781 - Loss:  0.453, Seconds: 2.57\n",
      "Epoch  20/100 Batch  160/781 - Loss:  0.492, Seconds: 2.32\n",
      "Epoch  20/100 Batch  180/781 - Loss:  0.489, Seconds: 2.43\n",
      "Epoch  20/100 Batch  200/781 - Loss:  0.469, Seconds: 2.83\n",
      "Epoch  20/100 Batch  220/781 - Loss:  0.497, Seconds: 2.65\n",
      "Epoch  20/100 Batch  240/781 - Loss:  0.409, Seconds: 2.65\n",
      "Average loss for this update: 0.481\n",
      "New Record!\n",
      "Epoch  20/100 Batch  260/781 - Loss:  0.462, Seconds: 2.77\n",
      "Epoch  20/100 Batch  280/781 - Loss:  0.461, Seconds: 2.75\n",
      "Epoch  20/100 Batch  300/781 - Loss:  0.519, Seconds: 2.73\n",
      "Epoch  20/100 Batch  320/781 - Loss:  0.539, Seconds: 2.77\n",
      "Epoch  20/100 Batch  340/781 - Loss:  0.485, Seconds: 2.70\n",
      "Epoch  20/100 Batch  360/781 - Loss:  0.485, Seconds: 2.95\n",
      "Epoch  20/100 Batch  380/781 - Loss:  0.432, Seconds: 2.87\n",
      "Epoch  20/100 Batch  400/781 - Loss:  0.478, Seconds: 2.94\n",
      "Epoch  20/100 Batch  420/781 - Loss:  0.435, Seconds: 2.60\n",
      "Epoch  20/100 Batch  440/781 - Loss:  0.461, Seconds: 2.96\n",
      "Epoch  20/100 Batch  460/781 - Loss:  0.489, Seconds: 2.85\n",
      "Epoch  20/100 Batch  480/781 - Loss:  0.463, Seconds: 2.65\n",
      "Epoch  20/100 Batch  500/781 - Loss:  0.509, Seconds: 2.73\n",
      "Average loss for this update: 0.476\n",
      "New Record!\n",
      "Epoch  20/100 Batch  520/781 - Loss:  0.427, Seconds: 2.67\n",
      "Epoch  20/100 Batch  540/781 - Loss:  0.477, Seconds: 2.89\n",
      "Epoch  20/100 Batch  560/781 - Loss:  0.427, Seconds: 2.60\n",
      "Epoch  20/100 Batch  580/781 - Loss:  0.486, Seconds: 2.78\n",
      "Epoch  20/100 Batch  600/781 - Loss:  0.482, Seconds: 2.77\n",
      "Epoch  20/100 Batch  620/781 - Loss:  0.461, Seconds: 2.70\n",
      "Epoch  20/100 Batch  640/781 - Loss:  0.481, Seconds: 2.78\n",
      "Epoch  20/100 Batch  660/781 - Loss:  0.397, Seconds: 2.86\n",
      "Epoch  20/100 Batch  680/781 - Loss:  0.458, Seconds: 2.81\n",
      "Epoch  20/100 Batch  700/781 - Loss:  0.506, Seconds: 2.82\n",
      "Epoch  20/100 Batch  720/781 - Loss:  0.535, Seconds: 2.93\n",
      "Epoch  20/100 Batch  740/781 - Loss:  0.495, Seconds: 3.05\n",
      "Epoch  20/100 Batch  760/781 - Loss:  0.498, Seconds: 2.79\n",
      "Average loss for this update: 0.473\n",
      "New Record!\n",
      "Epoch  20/100 Batch  780/781 - Loss:  0.438, Seconds: 2.77\n",
      "Epoch  21/100 Batch   20/781 - Loss:  0.543, Seconds: 2.69\n",
      "Epoch  21/100 Batch   40/781 - Loss:  0.465, Seconds: 2.24\n",
      "Epoch  21/100 Batch   60/781 - Loss:  0.460, Seconds: 2.82\n",
      "Epoch  21/100 Batch   80/781 - Loss:  0.462, Seconds: 2.71\n",
      "Epoch  21/100 Batch  100/781 - Loss:  0.412, Seconds: 2.68\n",
      "Epoch  21/100 Batch  120/781 - Loss:  0.438, Seconds: 2.45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  21/100 Batch  140/781 - Loss:  0.427, Seconds: 2.57\n",
      "Epoch  21/100 Batch  160/781 - Loss:  0.458, Seconds: 2.16\n",
      "Epoch  21/100 Batch  180/781 - Loss:  0.463, Seconds: 2.42\n",
      "Epoch  21/100 Batch  200/781 - Loss:  0.445, Seconds: 2.76\n",
      "Epoch  21/100 Batch  220/781 - Loss:  0.470, Seconds: 2.60\n",
      "Epoch  21/100 Batch  240/781 - Loss:  0.375, Seconds: 2.63\n",
      "Average loss for this update: 0.451\n",
      "New Record!\n",
      "Epoch  21/100 Batch  260/781 - Loss:  0.436, Seconds: 2.87\n",
      "Epoch  21/100 Batch  280/781 - Loss:  0.421, Seconds: 2.75\n",
      "Epoch  21/100 Batch  300/781 - Loss:  0.486, Seconds: 2.65\n",
      "Epoch  21/100 Batch  320/781 - Loss:  0.483, Seconds: 2.68\n",
      "Epoch  21/100 Batch  340/781 - Loss:  0.449, Seconds: 2.69\n",
      "Epoch  21/100 Batch  360/781 - Loss:  0.436, Seconds: 2.80\n",
      "Epoch  21/100 Batch  380/781 - Loss:  0.396, Seconds: 2.89\n",
      "Epoch  21/100 Batch  400/781 - Loss:  0.453, Seconds: 2.89\n",
      "Epoch  21/100 Batch  420/781 - Loss:  0.408, Seconds: 2.61\n",
      "Epoch  21/100 Batch  440/781 - Loss:  0.447, Seconds: 3.08\n",
      "Epoch  21/100 Batch  460/781 - Loss:  0.454, Seconds: 2.82\n",
      "Epoch  21/100 Batch  480/781 - Loss:  0.454, Seconds: 2.65\n",
      "Epoch  21/100 Batch  500/781 - Loss:  0.492, Seconds: 2.75\n",
      "Average loss for this update: 0.446\n",
      "New Record!\n",
      "Epoch  21/100 Batch  520/781 - Loss:  0.419, Seconds: 2.47\n",
      "Epoch  21/100 Batch  540/781 - Loss:  0.450, Seconds: 2.85\n",
      "Epoch  21/100 Batch  560/781 - Loss:  0.405, Seconds: 2.62\n",
      "Epoch  21/100 Batch  580/781 - Loss:  0.462, Seconds: 2.80\n",
      "Epoch  21/100 Batch  600/781 - Loss:  0.451, Seconds: 2.75\n",
      "Epoch  21/100 Batch  620/781 - Loss:  0.441, Seconds: 2.80\n",
      "Epoch  21/100 Batch  640/781 - Loss:  0.457, Seconds: 2.75\n",
      "Epoch  21/100 Batch  660/781 - Loss:  0.382, Seconds: 2.86\n",
      "Epoch  21/100 Batch  680/781 - Loss:  0.438, Seconds: 2.78\n",
      "Epoch  21/100 Batch  700/781 - Loss:  0.479, Seconds: 2.85\n",
      "Epoch  21/100 Batch  720/781 - Loss:  0.511, Seconds: 2.93\n",
      "Epoch  21/100 Batch  740/781 - Loss:  0.472, Seconds: 3.00\n",
      "Epoch  21/100 Batch  760/781 - Loss:  0.489, Seconds: 2.83\n",
      "Average loss for this update: 0.451\n",
      "No Improvement.\n",
      "Epoch  21/100 Batch  780/781 - Loss:  0.429, Seconds: 2.61\n",
      "Epoch  22/100 Batch   20/781 - Loss:  0.507, Seconds: 2.71\n",
      "Epoch  22/100 Batch   40/781 - Loss:  0.432, Seconds: 2.26\n",
      "Epoch  22/100 Batch   60/781 - Loss:  0.452, Seconds: 2.84\n",
      "Epoch  22/100 Batch   80/781 - Loss:  0.451, Seconds: 2.69\n",
      "Epoch  22/100 Batch  100/781 - Loss:  0.393, Seconds: 2.71\n",
      "Epoch  22/100 Batch  120/781 - Loss:  0.427, Seconds: 2.47\n",
      "Epoch  22/100 Batch  140/781 - Loss:  0.400, Seconds: 2.60\n",
      "Epoch  22/100 Batch  160/781 - Loss:  0.433, Seconds: 2.20\n",
      "Epoch  22/100 Batch  180/781 - Loss:  0.433, Seconds: 2.38\n",
      "Epoch  22/100 Batch  200/781 - Loss:  0.433, Seconds: 2.73\n",
      "Epoch  22/100 Batch  220/781 - Loss:  0.449, Seconds: 2.65\n",
      "Epoch  22/100 Batch  240/781 - Loss:  0.360, Seconds: 2.67\n",
      "Average loss for this update: 0.43\n",
      "New Record!\n",
      "Epoch  22/100 Batch  260/781 - Loss:  0.418, Seconds: 2.73\n",
      "Epoch  22/100 Batch  280/781 - Loss:  0.412, Seconds: 2.71\n",
      "Epoch  22/100 Batch  300/781 - Loss:  0.461, Seconds: 2.66\n",
      "Epoch  22/100 Batch  320/781 - Loss:  0.469, Seconds: 2.67\n",
      "Epoch  22/100 Batch  340/781 - Loss:  0.429, Seconds: 2.68\n",
      "Epoch  22/100 Batch  360/781 - Loss:  0.432, Seconds: 2.79\n",
      "Epoch  22/100 Batch  380/781 - Loss:  0.369, Seconds: 2.86\n",
      "Epoch  22/100 Batch  400/781 - Loss:  0.426, Seconds: 2.91\n",
      "Epoch  22/100 Batch  420/781 - Loss:  0.385, Seconds: 2.71\n",
      "Epoch  22/100 Batch  440/781 - Loss:  0.402, Seconds: 2.97\n",
      "Epoch  22/100 Batch  460/781 - Loss:  0.440, Seconds: 2.88\n",
      "Epoch  22/100 Batch  480/781 - Loss:  0.435, Seconds: 2.65\n",
      "Epoch  22/100 Batch  500/781 - Loss:  0.460, Seconds: 2.74\n",
      "Average loss for this update: 0.424\n",
      "New Record!\n",
      "Epoch  22/100 Batch  520/781 - Loss:  0.387, Seconds: 2.53\n",
      "Epoch  22/100 Batch  540/781 - Loss:  0.426, Seconds: 2.84\n",
      "Epoch  22/100 Batch  560/781 - Loss:  0.396, Seconds: 2.59\n",
      "Epoch  22/100 Batch  580/781 - Loss:  0.432, Seconds: 2.75\n",
      "Epoch  22/100 Batch  600/781 - Loss:  0.432, Seconds: 2.79\n",
      "Epoch  22/100 Batch  620/781 - Loss:  0.420, Seconds: 2.68\n",
      "Epoch  22/100 Batch  640/781 - Loss:  0.439, Seconds: 2.75\n",
      "Epoch  22/100 Batch  660/781 - Loss:  0.370, Seconds: 2.88\n",
      "Epoch  22/100 Batch  680/781 - Loss:  0.401, Seconds: 2.78\n",
      "Epoch  22/100 Batch  700/781 - Loss:  0.454, Seconds: 2.78\n",
      "Epoch  22/100 Batch  720/781 - Loss:  0.488, Seconds: 2.92\n",
      "Epoch  22/100 Batch  740/781 - Loss:  0.434, Seconds: 3.01\n",
      "Epoch  22/100 Batch  760/781 - Loss:  0.463, Seconds: 2.84\n",
      "Average loss for this update: 0.427\n",
      "No Improvement.\n",
      "Epoch  22/100 Batch  780/781 - Loss:  0.400, Seconds: 2.61\n",
      "Epoch  23/100 Batch   20/781 - Loss:  0.472, Seconds: 2.70\n",
      "Epoch  23/100 Batch   40/781 - Loss:  0.407, Seconds: 2.27\n",
      "Epoch  23/100 Batch   60/781 - Loss:  0.417, Seconds: 2.80\n",
      "Epoch  23/100 Batch   80/781 - Loss:  0.412, Seconds: 2.71\n",
      "Epoch  23/100 Batch  100/781 - Loss:  0.382, Seconds: 2.71\n",
      "Epoch  23/100 Batch  120/781 - Loss:  0.406, Seconds: 2.47\n",
      "Epoch  23/100 Batch  140/781 - Loss:  0.386, Seconds: 2.74\n",
      "Epoch  23/100 Batch  160/781 - Loss:  0.412, Seconds: 2.21\n",
      "Epoch  23/100 Batch  180/781 - Loss:  0.417, Seconds: 2.46\n",
      "Epoch  23/100 Batch  200/781 - Loss:  0.394, Seconds: 2.75\n",
      "Epoch  23/100 Batch  220/781 - Loss:  0.416, Seconds: 2.63\n",
      "Epoch  23/100 Batch  240/781 - Loss:  0.334, Seconds: 2.78\n",
      "Average loss for this update: 0.404\n",
      "New Record!\n",
      "Epoch  23/100 Batch  260/781 - Loss:  0.391, Seconds: 2.81\n",
      "Epoch  23/100 Batch  280/781 - Loss:  0.378, Seconds: 2.75\n",
      "Epoch  23/100 Batch  300/781 - Loss:  0.443, Seconds: 2.81\n",
      "Epoch  23/100 Batch  320/781 - Loss:  0.442, Seconds: 2.70\n",
      "Epoch  23/100 Batch  340/781 - Loss:  0.411, Seconds: 2.66\n",
      "Epoch  23/100 Batch  360/781 - Loss:  0.408, Seconds: 2.79\n",
      "Epoch  23/100 Batch  380/781 - Loss:  0.366, Seconds: 3.05\n",
      "Epoch  23/100 Batch  400/781 - Loss:  0.410, Seconds: 2.91\n",
      "Epoch  23/100 Batch  420/781 - Loss:  0.380, Seconds: 2.53\n",
      "Epoch  23/100 Batch  440/781 - Loss:  0.395, Seconds: 2.94\n",
      "Epoch  23/100 Batch  460/781 - Loss:  0.417, Seconds: 2.81\n",
      "Epoch  23/100 Batch  480/781 - Loss:  0.413, Seconds: 2.65\n",
      "Epoch  23/100 Batch  500/781 - Loss:  0.443, Seconds: 2.88\n",
      "Average loss for this update: 0.406\n",
      "No Improvement.\n",
      "Epoch  23/100 Batch  520/781 - Loss:  0.369, Seconds: 2.51\n",
      "Epoch  23/100 Batch  540/781 - Loss:  0.399, Seconds: 2.85\n",
      "Epoch  23/100 Batch  560/781 - Loss:  0.362, Seconds: 2.66\n",
      "Epoch  23/100 Batch  580/781 - Loss:  0.415, Seconds: 2.77\n",
      "Epoch  23/100 Batch  600/781 - Loss:  0.415, Seconds: 2.82\n",
      "Epoch  23/100 Batch  620/781 - Loss:  0.397, Seconds: 2.65\n",
      "Epoch  23/100 Batch  640/781 - Loss:  0.401, Seconds: 2.76\n",
      "Epoch  23/100 Batch  660/781 - Loss:  0.345, Seconds: 2.91\n",
      "Epoch  23/100 Batch  680/781 - Loss:  0.387, Seconds: 2.76\n",
      "Epoch  23/100 Batch  700/781 - Loss:  0.439, Seconds: 2.85\n",
      "Epoch  23/100 Batch  720/781 - Loss:  0.462, Seconds: 2.95\n",
      "Epoch  23/100 Batch  740/781 - Loss:  0.423, Seconds: 3.02\n",
      "Epoch  23/100 Batch  760/781 - Loss:  0.438, Seconds: 2.83\n",
      "Average loss for this update: 0.405\n",
      "No Improvement.\n",
      "Epoch  23/100 Batch  780/781 - Loss:  0.388, Seconds: 2.61\n",
      "Epoch  24/100 Batch   20/781 - Loss:  0.452, Seconds: 2.84\n",
      "Epoch  24/100 Batch   40/781 - Loss:  0.382, Seconds: 2.29\n",
      "Epoch  24/100 Batch   60/781 - Loss:  0.399, Seconds: 2.78\n",
      "Epoch  24/100 Batch   80/781 - Loss:  0.394, Seconds: 2.75\n",
      "Epoch  24/100 Batch  100/781 - Loss:  0.358, Seconds: 2.72\n",
      "Epoch  24/100 Batch  120/781 - Loss:  0.376, Seconds: 2.46\n",
      "Epoch  24/100 Batch  140/781 - Loss:  0.356, Seconds: 2.59\n",
      "Epoch  24/100 Batch  160/781 - Loss:  0.392, Seconds: 2.19\n",
      "Epoch  24/100 Batch  180/781 - Loss:  0.388, Seconds: 2.40\n",
      "Epoch  24/100 Batch  200/781 - Loss:  0.385, Seconds: 2.71\n",
      "Epoch  24/100 Batch  220/781 - Loss:  0.389, Seconds: 2.64\n",
      "Epoch  24/100 Batch  240/781 - Loss:  0.313, Seconds: 2.66\n",
      "Average loss for this update: 0.382\n",
      "New Record!\n",
      "Epoch  24/100 Batch  260/781 - Loss:  0.377, Seconds: 2.82\n",
      "Epoch  24/100 Batch  280/781 - Loss:  0.365, Seconds: 2.72\n",
      "Epoch  24/100 Batch  300/781 - Loss:  0.417, Seconds: 2.67\n",
      "Epoch  24/100 Batch  320/781 - Loss:  0.413, Seconds: 2.75\n",
      "Epoch  24/100 Batch  340/781 - Loss:  0.383, Seconds: 2.68\n",
      "Epoch  24/100 Batch  360/781 - Loss:  0.386, Seconds: 2.89\n",
      "Epoch  24/100 Batch  380/781 - Loss:  0.343, Seconds: 2.91\n",
      "Epoch  24/100 Batch  400/781 - Loss:  0.384, Seconds: 2.85\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  24/100 Batch  420/781 - Loss:  0.357, Seconds: 2.55\n",
      "Epoch  24/100 Batch  440/781 - Loss:  0.390, Seconds: 2.97\n",
      "Epoch  24/100 Batch  460/781 - Loss:  0.393, Seconds: 2.86\n",
      "Epoch  24/100 Batch  480/781 - Loss:  0.383, Seconds: 2.63\n",
      "Epoch  24/100 Batch  500/781 - Loss:  0.423, Seconds: 2.76\n",
      "Average loss for this update: 0.382\n",
      "No Improvement.\n",
      "Epoch  24/100 Batch  520/781 - Loss:  0.334, Seconds: 2.50\n",
      "Epoch  24/100 Batch  540/781 - Loss:  0.373, Seconds: 2.84\n",
      "Epoch  24/100 Batch  560/781 - Loss:  0.362, Seconds: 2.55\n",
      "Epoch  24/100 Batch  580/781 - Loss:  0.386, Seconds: 2.80\n",
      "Epoch  24/100 Batch  600/781 - Loss:  0.392, Seconds: 3.01\n",
      "Epoch  24/100 Batch  620/781 - Loss:  0.388, Seconds: 2.62\n",
      "Epoch  24/100 Batch  640/781 - Loss:  0.383, Seconds: 2.80\n",
      "Epoch  24/100 Batch  660/781 - Loss:  0.325, Seconds: 2.88\n",
      "Epoch  24/100 Batch  680/781 - Loss:  0.375, Seconds: 2.77\n",
      "Epoch  24/100 Batch  700/781 - Loss:  0.409, Seconds: 2.78\n",
      "Epoch  24/100 Batch  720/781 - Loss:  0.450, Seconds: 3.10\n",
      "Epoch  24/100 Batch  740/781 - Loss:  0.404, Seconds: 3.00\n",
      "Epoch  24/100 Batch  760/781 - Loss:  0.419, Seconds: 2.84\n",
      "Average loss for this update: 0.387\n",
      "No Improvement.\n",
      "Epoch  24/100 Batch  780/781 - Loss:  0.363, Seconds: 2.62\n",
      "Epoch  25/100 Batch   20/781 - Loss:  0.429, Seconds: 2.71\n",
      "Epoch  25/100 Batch   40/781 - Loss:  0.373, Seconds: 2.25\n",
      "Epoch  25/100 Batch   60/781 - Loss:  0.384, Seconds: 2.86\n",
      "Epoch  25/100 Batch   80/781 - Loss:  0.385, Seconds: 2.68\n",
      "Epoch  25/100 Batch  100/781 - Loss:  0.342, Seconds: 2.74\n",
      "Epoch  25/100 Batch  120/781 - Loss:  0.355, Seconds: 2.48\n",
      "Epoch  25/100 Batch  140/781 - Loss:  0.346, Seconds: 2.58\n",
      "Epoch  25/100 Batch  160/781 - Loss:  0.365, Seconds: 2.17\n",
      "Epoch  25/100 Batch  180/781 - Loss:  0.368, Seconds: 2.46\n",
      "Epoch  25/100 Batch  200/781 - Loss:  0.365, Seconds: 2.77\n",
      "Epoch  25/100 Batch  220/781 - Loss:  0.372, Seconds: 2.63\n",
      "Epoch  25/100 Batch  240/781 - Loss:  0.303, Seconds: 2.68\n",
      "Average loss for this update: 0.365\n",
      "New Record!\n",
      "Epoch  25/100 Batch  260/781 - Loss:  0.361, Seconds: 2.85\n",
      "Epoch  25/100 Batch  280/781 - Loss:  0.337, Seconds: 2.88\n",
      "Epoch  25/100 Batch  300/781 - Loss:  0.409, Seconds: 2.65\n",
      "Epoch  25/100 Batch  320/781 - Loss:  0.391, Seconds: 2.68\n",
      "Epoch  25/100 Batch  340/781 - Loss:  0.365, Seconds: 2.81\n",
      "Epoch  25/100 Batch  360/781 - Loss:  0.363, Seconds: 2.81\n",
      "Epoch  25/100 Batch  380/781 - Loss:  0.329, Seconds: 2.89\n",
      "Epoch  25/100 Batch  400/781 - Loss:  0.356, Seconds: 2.93\n",
      "Epoch  25/100 Batch  420/781 - Loss:  0.336, Seconds: 2.59\n",
      "Epoch  25/100 Batch  440/781 - Loss:  0.364, Seconds: 2.96\n",
      "Epoch  25/100 Batch  460/781 - Loss:  0.380, Seconds: 2.83\n",
      "Epoch  25/100 Batch  480/781 - Loss:  0.374, Seconds: 2.62\n",
      "Epoch  25/100 Batch  500/781 - Loss:  0.403, Seconds: 2.77\n",
      "Average loss for this update: 0.364\n",
      "New Record!\n",
      "Epoch  25/100 Batch  520/781 - Loss:  0.332, Seconds: 2.51\n",
      "Epoch  25/100 Batch  540/781 - Loss:  0.357, Seconds: 2.83\n",
      "Epoch  25/100 Batch  560/781 - Loss:  0.331, Seconds: 2.61\n",
      "Epoch  25/100 Batch  580/781 - Loss:  0.376, Seconds: 2.80\n",
      "Epoch  25/100 Batch  600/781 - Loss:  0.363, Seconds: 2.74\n",
      "Epoch  25/100 Batch  620/781 - Loss:  0.355, Seconds: 2.67\n",
      "Epoch  25/100 Batch  640/781 - Loss:  0.361, Seconds: 2.75\n",
      "Epoch  25/100 Batch  660/781 - Loss:  0.298, Seconds: 2.89\n",
      "Epoch  25/100 Batch  680/781 - Loss:  0.353, Seconds: 2.78\n",
      "Epoch  25/100 Batch  700/781 - Loss:  0.383, Seconds: 2.82\n",
      "Epoch  25/100 Batch  720/781 - Loss:  0.416, Seconds: 2.93\n",
      "Epoch  25/100 Batch  740/781 - Loss:  0.381, Seconds: 3.04\n",
      "Epoch  25/100 Batch  760/781 - Loss:  0.396, Seconds: 2.86\n",
      "Average loss for this update: 0.363\n",
      "New Record!\n",
      "Epoch  25/100 Batch  780/781 - Loss:  0.349, Seconds: 2.60\n",
      "Epoch  26/100 Batch   20/781 - Loss:  0.405, Seconds: 2.81\n",
      "Epoch  26/100 Batch   40/781 - Loss:  0.348, Seconds: 2.24\n",
      "Epoch  26/100 Batch   60/781 - Loss:  0.360, Seconds: 2.77\n",
      "Epoch  26/100 Batch   80/781 - Loss:  0.347, Seconds: 2.69\n",
      "Epoch  26/100 Batch  100/781 - Loss:  0.331, Seconds: 2.69\n",
      "Epoch  26/100 Batch  120/781 - Loss:  0.343, Seconds: 2.49\n",
      "Epoch  26/100 Batch  140/781 - Loss:  0.333, Seconds: 2.61\n",
      "Epoch  26/100 Batch  160/781 - Loss:  0.346, Seconds: 2.17\n",
      "Epoch  26/100 Batch  180/781 - Loss:  0.356, Seconds: 2.46\n",
      "Epoch  26/100 Batch  200/781 - Loss:  0.339, Seconds: 2.79\n",
      "Epoch  26/100 Batch  220/781 - Loss:  0.363, Seconds: 2.70\n",
      "Epoch  26/100 Batch  240/781 - Loss:  0.293, Seconds: 2.63\n",
      "Average loss for this update: 0.347\n",
      "New Record!\n",
      "Epoch  26/100 Batch  260/781 - Loss:  0.342, Seconds: 2.79\n",
      "Epoch  26/100 Batch  280/781 - Loss:  0.328, Seconds: 2.76\n",
      "Epoch  26/100 Batch  300/781 - Loss:  0.374, Seconds: 2.82\n",
      "Epoch  26/100 Batch  320/781 - Loss:  0.392, Seconds: 2.66\n",
      "Epoch  26/100 Batch  340/781 - Loss:  0.349, Seconds: 2.64\n",
      "Epoch  26/100 Batch  360/781 - Loss:  0.338, Seconds: 2.82\n",
      "Epoch  26/100 Batch  380/781 - Loss:  0.330, Seconds: 2.87\n",
      "Epoch  26/100 Batch  400/781 - Loss:  0.369, Seconds: 2.86\n",
      "Epoch  26/100 Batch  420/781 - Loss:  0.337, Seconds: 2.55\n",
      "Epoch  26/100 Batch  440/781 - Loss:  0.345, Seconds: 2.90\n",
      "Epoch  26/100 Batch  460/781 - Loss:  0.357, Seconds: 2.83\n",
      "Epoch  26/100 Batch  480/781 - Loss:  0.355, Seconds: 2.76\n",
      "Epoch  26/100 Batch  500/781 - Loss:  0.383, Seconds: 2.71\n",
      "Average loss for this update: 0.352\n",
      "No Improvement.\n",
      "Epoch  26/100 Batch  520/781 - Loss:  0.314, Seconds: 2.54\n",
      "Epoch  26/100 Batch  540/781 - Loss:  0.350, Seconds: 2.86\n",
      "Epoch  26/100 Batch  560/781 - Loss:  0.305, Seconds: 2.60\n",
      "Epoch  26/100 Batch  580/781 - Loss:  0.352, Seconds: 2.80\n",
      "Epoch  26/100 Batch  600/781 - Loss:  0.346, Seconds: 2.80\n",
      "Epoch  26/100 Batch  620/781 - Loss:  0.339, Seconds: 2.67\n",
      "Epoch  26/100 Batch  640/781 - Loss:  0.349, Seconds: 2.78\n",
      "Epoch  26/100 Batch  660/781 - Loss:  0.300, Seconds: 2.90\n",
      "Epoch  26/100 Batch  680/781 - Loss:  0.333, Seconds: 2.78\n",
      "Epoch  26/100 Batch  700/781 - Loss:  0.367, Seconds: 2.83\n",
      "Epoch  26/100 Batch  720/781 - Loss:  0.390, Seconds: 2.94\n",
      "Epoch  26/100 Batch  740/781 - Loss:  0.361, Seconds: 3.01\n",
      "Epoch  26/100 Batch  760/781 - Loss:  0.383, Seconds: 2.78\n",
      "Average loss for this update: 0.348\n",
      "No Improvement.\n",
      "Epoch  26/100 Batch  780/781 - Loss:  0.347, Seconds: 2.61\n",
      "Epoch  27/100 Batch   20/781 - Loss:  0.407, Seconds: 2.70\n",
      "Epoch  27/100 Batch   40/781 - Loss:  0.330, Seconds: 2.31\n",
      "Epoch  27/100 Batch   60/781 - Loss:  0.344, Seconds: 2.82\n",
      "Epoch  27/100 Batch   80/781 - Loss:  0.346, Seconds: 2.72\n",
      "Epoch  27/100 Batch  100/781 - Loss:  0.316, Seconds: 2.70\n",
      "Epoch  27/100 Batch  120/781 - Loss:  0.324, Seconds: 2.46\n",
      "Epoch  27/100 Batch  140/781 - Loss:  0.310, Seconds: 2.69\n",
      "Epoch  27/100 Batch  160/781 - Loss:  0.333, Seconds: 2.20\n",
      "Epoch  27/100 Batch  180/781 - Loss:  0.341, Seconds: 2.45\n",
      "Epoch  27/100 Batch  200/781 - Loss:  0.334, Seconds: 2.77\n",
      "Epoch  27/100 Batch  220/781 - Loss:  0.347, Seconds: 2.67\n",
      "Epoch  27/100 Batch  240/781 - Loss:  0.273, Seconds: 2.62\n",
      "Average loss for this update: 0.334\n",
      "New Record!\n",
      "Epoch  27/100 Batch  260/781 - Loss:  0.340, Seconds: 2.82\n",
      "Epoch  27/100 Batch  280/781 - Loss:  0.316, Seconds: 2.76\n",
      "Epoch  27/100 Batch  300/781 - Loss:  0.349, Seconds: 2.69\n",
      "Epoch  27/100 Batch  320/781 - Loss:  0.359, Seconds: 2.83\n",
      "Epoch  27/100 Batch  340/781 - Loss:  0.319, Seconds: 2.69\n",
      "Epoch  27/100 Batch  360/781 - Loss:  0.337, Seconds: 2.79\n",
      "Epoch  27/100 Batch  380/781 - Loss:  0.307, Seconds: 2.90\n",
      "Epoch  27/100 Batch  400/781 - Loss:  0.333, Seconds: 2.89\n",
      "Epoch  27/100 Batch  420/781 - Loss:  0.309, Seconds: 2.54\n",
      "Epoch  27/100 Batch  440/781 - Loss:  0.326, Seconds: 2.97\n",
      "Epoch  27/100 Batch  460/781 - Loss:  0.348, Seconds: 2.80\n",
      "Epoch  27/100 Batch  480/781 - Loss:  0.326, Seconds: 2.61\n",
      "Epoch  27/100 Batch  500/781 - Loss:  0.363, Seconds: 2.74\n",
      "Average loss for this update: 0.33\n",
      "New Record!\n",
      "Epoch  27/100 Batch  520/781 - Loss:  0.304, Seconds: 2.56\n",
      "Epoch  27/100 Batch  540/781 - Loss:  0.318, Seconds: 2.86\n",
      "Epoch  27/100 Batch  560/781 - Loss:  0.316, Seconds: 2.60\n",
      "Epoch  27/100 Batch  580/781 - Loss:  0.347, Seconds: 2.92\n",
      "Epoch  27/100 Batch  600/781 - Loss:  0.322, Seconds: 2.78\n",
      "Epoch  27/100 Batch  620/781 - Loss:  0.330, Seconds: 2.64\n",
      "Epoch  27/100 Batch  640/781 - Loss:  0.329, Seconds: 2.81\n",
      "Epoch  27/100 Batch  660/781 - Loss:  0.284, Seconds: 2.90\n",
      "Epoch  27/100 Batch  680/781 - Loss:  0.321, Seconds: 2.74\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  27/100 Batch  700/781 - Loss:  0.342, Seconds: 2.82\n",
      "Epoch  27/100 Batch  720/781 - Loss:  0.374, Seconds: 2.89\n",
      "Epoch  27/100 Batch  740/781 - Loss:  0.338, Seconds: 3.06\n",
      "Epoch  27/100 Batch  760/781 - Loss:  0.348, Seconds: 2.82\n",
      "Average loss for this update: 0.329\n",
      "New Record!\n",
      "Epoch  27/100 Batch  780/781 - Loss:  0.313, Seconds: 2.62\n",
      "Epoch  28/100 Batch   20/781 - Loss:  0.385, Seconds: 2.67\n",
      "Epoch  28/100 Batch   40/781 - Loss:  0.315, Seconds: 2.25\n",
      "Epoch  28/100 Batch   60/781 - Loss:  0.332, Seconds: 2.83\n",
      "Epoch  28/100 Batch   80/781 - Loss:  0.312, Seconds: 2.79\n",
      "Epoch  28/100 Batch  100/781 - Loss:  0.293, Seconds: 2.72\n",
      "Epoch  28/100 Batch  120/781 - Loss:  0.315, Seconds: 2.49\n",
      "Epoch  28/100 Batch  140/781 - Loss:  0.302, Seconds: 2.60\n",
      "Epoch  28/100 Batch  160/781 - Loss:  0.325, Seconds: 2.18\n",
      "Epoch  28/100 Batch  180/781 - Loss:  0.318, Seconds: 2.42\n",
      "Epoch  28/100 Batch  200/781 - Loss:  0.315, Seconds: 2.74\n",
      "Epoch  28/100 Batch  220/781 - Loss:  0.327, Seconds: 2.66\n",
      "Epoch  28/100 Batch  240/781 - Loss:  0.258, Seconds: 2.63\n",
      "Average loss for this update: 0.316\n",
      "New Record!\n",
      "Epoch  28/100 Batch  260/781 - Loss:  0.312, Seconds: 2.79\n",
      "Epoch  28/100 Batch  280/781 - Loss:  0.297, Seconds: 2.77\n",
      "Epoch  28/100 Batch  300/781 - Loss:  0.323, Seconds: 2.66\n",
      "Epoch  28/100 Batch  320/781 - Loss:  0.330, Seconds: 2.63\n",
      "Epoch  28/100 Batch  340/781 - Loss:  0.306, Seconds: 2.69\n",
      "Epoch  28/100 Batch  360/781 - Loss:  0.299, Seconds: 2.77\n",
      "Epoch  28/100 Batch  380/781 - Loss:  0.275, Seconds: 2.89\n",
      "Epoch  28/100 Batch  400/781 - Loss:  0.317, Seconds: 2.94\n",
      "Epoch  28/100 Batch  420/781 - Loss:  0.290, Seconds: 2.53\n",
      "Epoch  28/100 Batch  440/781 - Loss:  0.308, Seconds: 2.96\n",
      "Epoch  28/100 Batch  460/781 - Loss:  0.326, Seconds: 2.86\n",
      "Epoch  28/100 Batch  480/781 - Loss:  0.312, Seconds: 2.62\n",
      "Epoch  28/100 Batch  500/781 - Loss:  0.343, Seconds: 2.72\n",
      "Average loss for this update: 0.309\n",
      "New Record!\n",
      "Epoch  28/100 Batch  520/781 - Loss:  0.286, Seconds: 2.53\n",
      "Epoch  28/100 Batch  540/781 - Loss:  0.304, Seconds: 2.84\n",
      "Epoch  28/100 Batch  560/781 - Loss:  0.278, Seconds: 2.58\n",
      "Epoch  28/100 Batch  580/781 - Loss:  0.326, Seconds: 2.81\n",
      "Epoch  28/100 Batch  600/781 - Loss:  0.326, Seconds: 2.78\n",
      "Epoch  28/100 Batch  620/781 - Loss:  0.320, Seconds: 2.65\n",
      "Epoch  28/100 Batch  640/781 - Loss:  0.331, Seconds: 2.77\n",
      "Epoch  28/100 Batch  660/781 - Loss:  0.275, Seconds: 3.02\n",
      "Epoch  28/100 Batch  680/781 - Loss:  0.309, Seconds: 2.77\n",
      "Epoch  28/100 Batch  700/781 - Loss:  0.334, Seconds: 2.82\n",
      "Epoch  28/100 Batch  720/781 - Loss:  0.368, Seconds: 2.93\n",
      "Epoch  28/100 Batch  740/781 - Loss:  0.323, Seconds: 3.02\n",
      "Epoch  28/100 Batch  760/781 - Loss:  0.340, Seconds: 2.81\n",
      "Average loss for this update: 0.318\n",
      "No Improvement.\n",
      "Epoch  28/100 Batch  780/781 - Loss:  0.305, Seconds: 2.58\n",
      "Epoch  29/100 Batch   20/781 - Loss:  0.353, Seconds: 2.76\n",
      "Epoch  29/100 Batch   40/781 - Loss:  0.326, Seconds: 2.29\n",
      "Epoch  29/100 Batch   60/781 - Loss:  0.324, Seconds: 2.79\n",
      "Epoch  29/100 Batch   80/781 - Loss:  0.313, Seconds: 2.70\n",
      "Epoch  29/100 Batch  100/781 - Loss:  0.283, Seconds: 2.71\n",
      "Epoch  29/100 Batch  120/781 - Loss:  0.292, Seconds: 2.50\n",
      "Epoch  29/100 Batch  140/781 - Loss:  0.289, Seconds: 2.63\n",
      "Epoch  29/100 Batch  160/781 - Loss:  0.316, Seconds: 2.17\n",
      "Epoch  29/100 Batch  180/781 - Loss:  0.312, Seconds: 2.42\n",
      "Epoch  29/100 Batch  200/781 - Loss:  0.301, Seconds: 2.72\n",
      "Epoch  29/100 Batch  220/781 - Loss:  0.327, Seconds: 2.60\n",
      "Epoch  29/100 Batch  240/781 - Loss:  0.247, Seconds: 2.64\n",
      "Average loss for this update: 0.307\n",
      "New Record!\n",
      "Epoch  29/100 Batch  260/781 - Loss:  0.302, Seconds: 2.89\n",
      "Epoch  29/100 Batch  280/781 - Loss:  0.292, Seconds: 2.72\n",
      "Epoch  29/100 Batch  300/781 - Loss:  0.330, Seconds: 2.68\n",
      "Epoch  29/100 Batch  320/781 - Loss:  0.336, Seconds: 2.71\n",
      "Epoch  29/100 Batch  340/781 - Loss:  0.307, Seconds: 2.70\n",
      "Epoch  29/100 Batch  360/781 - Loss:  0.308, Seconds: 2.81\n",
      "Epoch  29/100 Batch  380/781 - Loss:  0.264, Seconds: 2.90\n",
      "Epoch  29/100 Batch  400/781 - Loss:  0.308, Seconds: 2.89\n",
      "Epoch  29/100 Batch  420/781 - Loss:  0.271, Seconds: 2.75\n",
      "Epoch  29/100 Batch  440/781 - Loss:  0.292, Seconds: 3.03\n",
      "Epoch  29/100 Batch  460/781 - Loss:  0.306, Seconds: 2.82\n",
      "Epoch  29/100 Batch  480/781 - Loss:  0.307, Seconds: 2.64\n",
      "Epoch  29/100 Batch  500/781 - Loss:  0.340, Seconds: 2.77\n",
      "Average loss for this update: 0.302\n",
      "New Record!\n",
      "Epoch  29/100 Batch  520/781 - Loss:  0.273, Seconds: 2.52\n",
      "Epoch  29/100 Batch  540/781 - Loss:  0.295, Seconds: 2.86\n",
      "Epoch  29/100 Batch  560/781 - Loss:  0.281, Seconds: 2.65\n",
      "Epoch  29/100 Batch  580/781 - Loss:  0.306, Seconds: 2.75\n",
      "Epoch  29/100 Batch  600/781 - Loss:  0.305, Seconds: 2.80\n",
      "Epoch  29/100 Batch  620/781 - Loss:  0.301, Seconds: 2.63\n",
      "Epoch  29/100 Batch  640/781 - Loss:  0.317, Seconds: 2.91\n",
      "Epoch  29/100 Batch  660/781 - Loss:  0.272, Seconds: 2.87\n",
      "Epoch  29/100 Batch  680/781 - Loss:  0.301, Seconds: 2.88\n",
      "Epoch  29/100 Batch  700/781 - Loss:  0.320, Seconds: 2.82\n",
      "Epoch  29/100 Batch  720/781 - Loss:  0.352, Seconds: 2.91\n",
      "Epoch  29/100 Batch  740/781 - Loss:  0.310, Seconds: 3.02\n",
      "Epoch  29/100 Batch  760/781 - Loss:  0.324, Seconds: 2.90\n",
      "Average loss for this update: 0.306\n",
      "No Improvement.\n",
      "Epoch  29/100 Batch  780/781 - Loss:  0.293, Seconds: 2.61\n",
      "Epoch  30/100 Batch   20/781 - Loss:  0.349, Seconds: 2.69\n",
      "Epoch  30/100 Batch   40/781 - Loss:  0.310, Seconds: 2.25\n",
      "Epoch  30/100 Batch   60/781 - Loss:  0.300, Seconds: 2.80\n",
      "Epoch  30/100 Batch   80/781 - Loss:  0.303, Seconds: 2.70\n",
      "Epoch  30/100 Batch  100/781 - Loss:  0.297, Seconds: 2.72\n",
      "Epoch  30/100 Batch  120/781 - Loss:  0.290, Seconds: 2.49\n",
      "Epoch  30/100 Batch  140/781 - Loss:  0.285, Seconds: 2.61\n",
      "Epoch  30/100 Batch  160/781 - Loss:  0.304, Seconds: 2.22\n",
      "Epoch  30/100 Batch  180/781 - Loss:  0.291, Seconds: 2.41\n",
      "Epoch  30/100 Batch  200/781 - Loss:  0.290, Seconds: 2.72\n",
      "Epoch  30/100 Batch  220/781 - Loss:  0.330, Seconds: 2.66\n",
      "Epoch  30/100 Batch  240/781 - Loss:  0.243, Seconds: 2.66\n",
      "Average loss for this update: 0.299\n",
      "New Record!\n",
      "Epoch  30/100 Batch  260/781 - Loss:  0.297, Seconds: 2.84\n",
      "Epoch  30/100 Batch  280/781 - Loss:  0.284, Seconds: 2.75\n",
      "Epoch  30/100 Batch  300/781 - Loss:  0.310, Seconds: 2.67\n",
      "Epoch  30/100 Batch  320/781 - Loss:  0.315, Seconds: 2.68\n",
      "Epoch  30/100 Batch  340/781 - Loss:  0.280, Seconds: 2.67\n",
      "Epoch  30/100 Batch  360/781 - Loss:  0.283, Seconds: 2.78\n",
      "Epoch  30/100 Batch  380/781 - Loss:  0.258, Seconds: 2.87\n",
      "Epoch  30/100 Batch  400/781 - Loss:  0.283, Seconds: 2.99\n",
      "Epoch  30/100 Batch  420/781 - Loss:  0.268, Seconds: 2.54\n",
      "Epoch  30/100 Batch  440/781 - Loss:  0.290, Seconds: 2.97\n",
      "Epoch  30/100 Batch  460/781 - Loss:  0.288, Seconds: 2.83\n",
      "Epoch  30/100 Batch  480/781 - Loss:  0.289, Seconds: 2.58\n",
      "Epoch  30/100 Batch  500/781 - Loss:  0.320, Seconds: 2.73\n",
      "Average loss for this update: 0.287\n",
      "New Record!\n",
      "Epoch  30/100 Batch  520/781 - Loss:  0.268, Seconds: 2.49\n",
      "Epoch  30/100 Batch  540/781 - Loss:  0.299, Seconds: 2.85\n",
      "Epoch  30/100 Batch  560/781 - Loss:  0.269, Seconds: 2.81\n",
      "Epoch  30/100 Batch  580/781 - Loss:  0.286, Seconds: 2.78\n",
      "Epoch  30/100 Batch  600/781 - Loss:  0.286, Seconds: 2.77\n",
      "Epoch  30/100 Batch  620/781 - Loss:  0.291, Seconds: 2.75\n",
      "Epoch  30/100 Batch  640/781 - Loss:  0.293, Seconds: 2.79\n",
      "Epoch  30/100 Batch  660/781 - Loss:  0.246, Seconds: 2.91\n",
      "Epoch  30/100 Batch  680/781 - Loss:  0.271, Seconds: 2.78\n",
      "Epoch  30/100 Batch  700/781 - Loss:  0.306, Seconds: 2.84\n",
      "Epoch  30/100 Batch  720/781 - Loss:  0.320, Seconds: 2.94\n",
      "Epoch  30/100 Batch  740/781 - Loss:  0.298, Seconds: 3.07\n",
      "Epoch  30/100 Batch  760/781 - Loss:  0.318, Seconds: 2.84\n",
      "Average loss for this update: 0.289\n",
      "No Improvement.\n",
      "Epoch  30/100 Batch  780/781 - Loss:  0.273, Seconds: 2.56\n",
      "Epoch  31/100 Batch   20/781 - Loss:  0.332, Seconds: 2.69\n",
      "Epoch  31/100 Batch   40/781 - Loss:  0.288, Seconds: 2.27\n",
      "Epoch  31/100 Batch   60/781 - Loss:  0.298, Seconds: 2.83\n",
      "Epoch  31/100 Batch   80/781 - Loss:  0.285, Seconds: 2.66\n",
      "Epoch  31/100 Batch  100/781 - Loss:  0.264, Seconds: 2.71\n",
      "Epoch  31/100 Batch  120/781 - Loss:  0.281, Seconds: 2.49\n",
      "Epoch  31/100 Batch  140/781 - Loss:  0.274, Seconds: 2.60\n",
      "Epoch  31/100 Batch  160/781 - Loss:  0.282, Seconds: 2.16\n",
      "Epoch  31/100 Batch  180/781 - Loss:  0.283, Seconds: 2.40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  31/100 Batch  200/781 - Loss:  0.271, Seconds: 2.76\n",
      "Epoch  31/100 Batch  220/781 - Loss:  0.290, Seconds: 2.69\n",
      "Epoch  31/100 Batch  240/781 - Loss:  0.230, Seconds: 2.63\n",
      "Average loss for this update: 0.282\n",
      "New Record!\n",
      "Epoch  31/100 Batch  260/781 - Loss:  0.280, Seconds: 2.81\n",
      "Epoch  31/100 Batch  280/781 - Loss:  0.262, Seconds: 2.74\n",
      "Epoch  31/100 Batch  300/781 - Loss:  0.290, Seconds: 2.72\n",
      "Epoch  31/100 Batch  320/781 - Loss:  0.304, Seconds: 2.83\n",
      "Epoch  31/100 Batch  340/781 - Loss:  0.272, Seconds: 2.71\n",
      "Epoch  31/100 Batch  360/781 - Loss:  0.283, Seconds: 2.81\n",
      "Epoch  31/100 Batch  380/781 - Loss:  0.232, Seconds: 2.89\n",
      "Epoch  31/100 Batch  400/781 - Loss:  0.272, Seconds: 2.87\n",
      "Epoch  31/100 Batch  420/781 - Loss:  0.243, Seconds: 2.61\n",
      "Epoch  31/100 Batch  440/781 - Loss:  0.268, Seconds: 2.95\n",
      "Epoch  31/100 Batch  460/781 - Loss:  0.288, Seconds: 2.90\n",
      "Epoch  31/100 Batch  480/781 - Loss:  0.283, Seconds: 2.67\n",
      "Epoch  31/100 Batch  500/781 - Loss:  0.319, Seconds: 2.69\n",
      "Average loss for this update: 0.275\n",
      "New Record!\n",
      "Epoch  31/100 Batch  520/781 - Loss:  0.256, Seconds: 2.54\n",
      "Epoch  31/100 Batch  540/781 - Loss:  0.272, Seconds: 2.85\n",
      "Epoch  31/100 Batch  560/781 - Loss:  0.250, Seconds: 2.63\n",
      "Epoch  31/100 Batch  580/781 - Loss:  0.292, Seconds: 2.75\n",
      "Epoch  31/100 Batch  600/781 - Loss:  0.280, Seconds: 2.77\n",
      "Epoch  31/100 Batch  620/781 - Loss:  0.288, Seconds: 2.67\n",
      "Epoch  31/100 Batch  640/781 - Loss:  0.283, Seconds: 2.79\n",
      "Epoch  31/100 Batch  660/781 - Loss:  0.241, Seconds: 2.82\n",
      "Epoch  31/100 Batch  680/781 - Loss:  0.268, Seconds: 2.77\n",
      "Epoch  31/100 Batch  700/781 - Loss:  0.303, Seconds: 2.83\n",
      "Epoch  31/100 Batch  720/781 - Loss:  0.333, Seconds: 2.93\n",
      "Epoch  31/100 Batch  740/781 - Loss:  0.292, Seconds: 3.05\n",
      "Epoch  31/100 Batch  760/781 - Loss:  0.303, Seconds: 2.82\n",
      "Average loss for this update: 0.283\n",
      "No Improvement.\n",
      "Epoch  31/100 Batch  780/781 - Loss:  0.276, Seconds: 2.58\n",
      "Epoch  32/100 Batch   20/781 - Loss:  0.319, Seconds: 2.74\n",
      "Epoch  32/100 Batch   40/781 - Loss:  0.278, Seconds: 2.25\n",
      "Epoch  32/100 Batch   60/781 - Loss:  0.281, Seconds: 2.82\n",
      "Epoch  32/100 Batch   80/781 - Loss:  0.284, Seconds: 2.65\n",
      "Epoch  32/100 Batch  100/781 - Loss:  0.255, Seconds: 2.73\n",
      "Epoch  32/100 Batch  120/781 - Loss:  0.258, Seconds: 2.52\n",
      "Epoch  32/100 Batch  140/781 - Loss:  0.242, Seconds: 2.58\n",
      "Epoch  32/100 Batch  160/781 - Loss:  0.267, Seconds: 2.20\n",
      "Epoch  32/100 Batch  180/781 - Loss:  0.269, Seconds: 2.39\n",
      "Epoch  32/100 Batch  200/781 - Loss:  0.272, Seconds: 2.78\n",
      "Epoch  32/100 Batch  220/781 - Loss:  0.276, Seconds: 2.64\n",
      "Epoch  32/100 Batch  240/781 - Loss:  0.226, Seconds: 2.62\n",
      "Average loss for this update: 0.269\n",
      "New Record!\n",
      "Epoch  32/100 Batch  260/781 - Loss:  0.270, Seconds: 2.84\n",
      "Epoch  32/100 Batch  280/781 - Loss:  0.253, Seconds: 2.71\n",
      "Epoch  32/100 Batch  300/781 - Loss:  0.292, Seconds: 2.67\n",
      "Epoch  32/100 Batch  320/781 - Loss:  0.291, Seconds: 2.70\n",
      "Epoch  32/100 Batch  340/781 - Loss:  0.263, Seconds: 2.67\n",
      "Epoch  32/100 Batch  360/781 - Loss:  0.263, Seconds: 2.79\n",
      "Epoch  32/100 Batch  380/781 - Loss:  0.236, Seconds: 2.89\n",
      "Epoch  32/100 Batch  400/781 - Loss:  0.274, Seconds: 2.87\n",
      "Epoch  32/100 Batch  420/781 - Loss:  0.263, Seconds: 2.60\n",
      "Epoch  32/100 Batch  440/781 - Loss:  0.264, Seconds: 2.95\n",
      "Epoch  32/100 Batch  460/781 - Loss:  0.280, Seconds: 2.84\n",
      "Epoch  32/100 Batch  480/781 - Loss:  0.273, Seconds: 2.64\n",
      "Epoch  32/100 Batch  500/781 - Loss:  0.290, Seconds: 2.76\n",
      "Average loss for this update: 0.269\n",
      "New Record!\n",
      "Epoch  32/100 Batch  520/781 - Loss:  0.251, Seconds: 2.52\n",
      "Epoch  32/100 Batch  540/781 - Loss:  0.263, Seconds: 2.85\n",
      "Epoch  32/100 Batch  560/781 - Loss:  0.247, Seconds: 2.59\n",
      "Epoch  32/100 Batch  580/781 - Loss:  0.277, Seconds: 2.76\n",
      "Epoch  32/100 Batch  600/781 - Loss:  0.277, Seconds: 2.77\n",
      "Epoch  32/100 Batch  620/781 - Loss:  0.257, Seconds: 2.66\n",
      "Epoch  32/100 Batch  640/781 - Loss:  0.275, Seconds: 2.77\n",
      "Epoch  32/100 Batch  660/781 - Loss:  0.234, Seconds: 2.89\n",
      "Epoch  32/100 Batch  680/781 - Loss:  0.252, Seconds: 2.75\n",
      "Epoch  32/100 Batch  700/781 - Loss:  0.279, Seconds: 2.80\n",
      "Epoch  32/100 Batch  720/781 - Loss:  0.311, Seconds: 2.95\n",
      "Epoch  32/100 Batch  740/781 - Loss:  0.278, Seconds: 3.08\n",
      "Epoch  32/100 Batch  760/781 - Loss:  0.298, Seconds: 2.89\n",
      "Average loss for this update: 0.27\n",
      "No Improvement.\n",
      "Epoch  32/100 Batch  780/781 - Loss:  0.258, Seconds: 2.60\n",
      "Epoch  33/100 Batch   20/781 - Loss:  0.294, Seconds: 2.70\n",
      "Epoch  33/100 Batch   40/781 - Loss:  0.265, Seconds: 2.27\n",
      "Epoch  33/100 Batch   60/781 - Loss:  0.268, Seconds: 2.77\n",
      "Epoch  33/100 Batch   80/781 - Loss:  0.262, Seconds: 2.68\n",
      "Epoch  33/100 Batch  100/781 - Loss:  0.231, Seconds: 2.70\n",
      "Epoch  33/100 Batch  120/781 - Loss:  0.258, Seconds: 2.51\n",
      "Epoch  33/100 Batch  140/781 - Loss:  0.238, Seconds: 2.57\n",
      "Epoch  33/100 Batch  160/781 - Loss:  0.266, Seconds: 2.18\n",
      "Epoch  33/100 Batch  180/781 - Loss:  0.266, Seconds: 2.39\n",
      "Epoch  33/100 Batch  200/781 - Loss:  0.252, Seconds: 2.76\n",
      "Epoch  33/100 Batch  220/781 - Loss:  0.270, Seconds: 2.64\n",
      "Epoch  33/100 Batch  240/781 - Loss:  0.217, Seconds: 2.66\n",
      "Average loss for this update: 0.257\n",
      "New Record!\n",
      "Epoch  33/100 Batch  260/781 - Loss:  0.256, Seconds: 2.84\n",
      "Epoch  33/100 Batch  280/781 - Loss:  0.244, Seconds: 2.76\n",
      "Epoch  33/100 Batch  300/781 - Loss:  0.263, Seconds: 2.64\n",
      "Epoch  33/100 Batch  320/781 - Loss:  0.298, Seconds: 2.65\n",
      "Epoch  33/100 Batch  340/781 - Loss:  0.264, Seconds: 2.71\n",
      "Epoch  33/100 Batch  360/781 - Loss:  0.253, Seconds: 2.83\n",
      "Epoch  33/100 Batch  380/781 - Loss:  0.240, Seconds: 2.92\n",
      "Epoch  33/100 Batch  400/781 - Loss:  0.238, Seconds: 2.88\n",
      "Epoch  33/100 Batch  420/781 - Loss:  0.243, Seconds: 2.57\n",
      "Epoch  33/100 Batch  440/781 - Loss:  0.252, Seconds: 2.95\n",
      "Epoch  33/100 Batch  460/781 - Loss:  0.270, Seconds: 2.95\n",
      "Epoch  33/100 Batch  480/781 - Loss:  0.264, Seconds: 2.60\n",
      "Epoch  33/100 Batch  500/781 - Loss:  0.282, Seconds: 2.74\n",
      "Average loss for this update: 0.257\n",
      "New Record!\n",
      "Epoch  33/100 Batch  520/781 - Loss:  0.232, Seconds: 2.51\n",
      "Epoch  33/100 Batch  540/781 - Loss:  0.254, Seconds: 2.84\n",
      "Epoch  33/100 Batch  560/781 - Loss:  0.230, Seconds: 2.66\n",
      "Epoch  33/100 Batch  580/781 - Loss:  0.276, Seconds: 2.75\n",
      "Epoch  33/100 Batch  600/781 - Loss:  0.253, Seconds: 2.77\n",
      "Epoch  33/100 Batch  620/781 - Loss:  0.260, Seconds: 2.65\n",
      "Epoch  33/100 Batch  640/781 - Loss:  0.267, Seconds: 2.81\n",
      "Epoch  33/100 Batch  660/781 - Loss:  0.230, Seconds: 2.89\n",
      "Epoch  33/100 Batch  680/781 - Loss:  0.240, Seconds: 2.80\n",
      "Epoch  33/100 Batch  700/781 - Loss:  0.277, Seconds: 3.02\n",
      "Epoch  33/100 Batch  720/781 - Loss:  0.305, Seconds: 2.89\n",
      "Epoch  33/100 Batch  740/781 - Loss:  0.275, Seconds: 3.09\n",
      "Epoch  33/100 Batch  760/781 - Loss:  0.283, Seconds: 2.85\n",
      "Average loss for this update: 0.261\n",
      "No Improvement.\n",
      "Epoch  33/100 Batch  780/781 - Loss:  0.245, Seconds: 2.60\n",
      "Epoch  34/100 Batch   20/781 - Loss:  0.292, Seconds: 2.69\n",
      "Epoch  34/100 Batch   40/781 - Loss:  0.257, Seconds: 2.25\n",
      "Epoch  34/100 Batch   60/781 - Loss:  0.267, Seconds: 2.78\n",
      "Epoch  34/100 Batch   80/781 - Loss:  0.266, Seconds: 2.69\n",
      "Epoch  34/100 Batch  100/781 - Loss:  0.245, Seconds: 2.79\n",
      "Epoch  34/100 Batch  120/781 - Loss:  0.246, Seconds: 2.48\n",
      "Epoch  34/100 Batch  140/781 - Loss:  0.234, Seconds: 2.60\n",
      "Epoch  34/100 Batch  160/781 - Loss:  0.257, Seconds: 2.21\n",
      "Epoch  34/100 Batch  180/781 - Loss:  0.259, Seconds: 2.38\n",
      "Epoch  34/100 Batch  200/781 - Loss:  0.258, Seconds: 2.75\n",
      "Epoch  34/100 Batch  220/781 - Loss:  0.257, Seconds: 2.64\n",
      "Epoch  34/100 Batch  240/781 - Loss:  0.207, Seconds: 2.61\n",
      "Average loss for this update: 0.254\n",
      "New Record!\n",
      "Epoch  34/100 Batch  260/781 - Loss:  0.249, Seconds: 2.82\n",
      "Epoch  34/100 Batch  280/781 - Loss:  0.234, Seconds: 2.76\n",
      "Epoch  34/100 Batch  300/781 - Loss:  0.260, Seconds: 2.68\n",
      "Epoch  34/100 Batch  320/781 - Loss:  0.284, Seconds: 2.70\n",
      "Epoch  34/100 Batch  340/781 - Loss:  0.246, Seconds: 2.67\n",
      "Epoch  34/100 Batch  360/781 - Loss:  0.253, Seconds: 2.78\n",
      "Epoch  34/100 Batch  380/781 - Loss:  0.235, Seconds: 2.95\n",
      "Epoch  34/100 Batch  400/781 - Loss:  0.248, Seconds: 2.93\n",
      "Epoch  34/100 Batch  420/781 - Loss:  0.226, Seconds: 2.57\n",
      "Epoch  34/100 Batch  440/781 - Loss:  0.247, Seconds: 2.96\n",
      "Epoch  34/100 Batch  460/781 - Loss:  0.250, Seconds: 2.87\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  34/100 Batch  480/781 - Loss:  0.258, Seconds: 2.61\n",
      "Epoch  34/100 Batch  500/781 - Loss:  0.282, Seconds: 2.75\n",
      "Average loss for this update: 0.25\n",
      "New Record!\n",
      "Epoch  34/100 Batch  520/781 - Loss:  0.224, Seconds: 2.48\n",
      "Epoch  34/100 Batch  540/781 - Loss:  0.245, Seconds: 3.01\n",
      "Epoch  34/100 Batch  560/781 - Loss:  0.224, Seconds: 2.78\n",
      "Epoch  34/100 Batch  580/781 - Loss:  0.267, Seconds: 2.76\n",
      "Epoch  34/100 Batch  600/781 - Loss:  0.259, Seconds: 2.76\n",
      "Epoch  34/100 Batch  620/781 - Loss:  0.244, Seconds: 2.61\n",
      "Epoch  34/100 Batch  640/781 - Loss:  0.254, Seconds: 2.76\n",
      "Epoch  34/100 Batch  660/781 - Loss:  0.218, Seconds: 2.89\n",
      "Epoch  34/100 Batch  680/781 - Loss:  0.240, Seconds: 2.76\n",
      "Epoch  34/100 Batch  700/781 - Loss:  0.271, Seconds: 2.81\n",
      "Epoch  34/100 Batch  720/781 - Loss:  0.284, Seconds: 2.93\n",
      "Epoch  34/100 Batch  740/781 - Loss:  0.259, Seconds: 3.10\n",
      "Epoch  34/100 Batch  760/781 - Loss:  0.283, Seconds: 2.80\n",
      "Average loss for this update: 0.253\n",
      "No Improvement.\n",
      "Epoch  34/100 Batch  780/781 - Loss:  0.242, Seconds: 2.75\n",
      "Epoch  35/100 Batch   20/781 - Loss:  0.305, Seconds: 2.72\n",
      "Epoch  35/100 Batch   40/781 - Loss:  0.251, Seconds: 2.27\n",
      "Epoch  35/100 Batch   60/781 - Loss:  0.254, Seconds: 2.80\n",
      "Epoch  35/100 Batch   80/781 - Loss:  0.246, Seconds: 2.91\n",
      "Epoch  35/100 Batch  100/781 - Loss:  0.231, Seconds: 2.72\n",
      "Epoch  35/100 Batch  120/781 - Loss:  0.244, Seconds: 2.50\n",
      "Epoch  35/100 Batch  140/781 - Loss:  0.232, Seconds: 2.60\n",
      "Epoch  35/100 Batch  160/781 - Loss:  0.247, Seconds: 2.22\n",
      "Epoch  35/100 Batch  180/781 - Loss:  0.251, Seconds: 2.40\n",
      "Epoch  35/100 Batch  200/781 - Loss:  0.234, Seconds: 2.75\n",
      "Epoch  35/100 Batch  220/781 - Loss:  0.261, Seconds: 2.64\n",
      "Epoch  35/100 Batch  240/781 - Loss:  0.201, Seconds: 2.65\n",
      "Average loss for this update: 0.246\n",
      "New Record!\n",
      "Epoch  35/100 Batch  260/781 - Loss:  0.244, Seconds: 2.81\n",
      "Epoch  35/100 Batch  280/781 - Loss:  0.247, Seconds: 2.86\n",
      "Epoch  35/100 Batch  300/781 - Loss:  0.283, Seconds: 2.68\n",
      "Epoch  35/100 Batch  320/781 - Loss:  0.289, Seconds: 2.67\n",
      "Epoch  35/100 Batch  340/781 - Loss:  0.266, Seconds: 2.70\n",
      "Epoch  35/100 Batch  360/781 - Loss:  0.263, Seconds: 2.78\n",
      "Epoch  35/100 Batch  380/781 - Loss:  0.228, Seconds: 2.90\n",
      "Epoch  35/100 Batch  400/781 - Loss:  0.251, Seconds: 2.92\n",
      "Epoch  35/100 Batch  420/781 - Loss:  0.238, Seconds: 2.56\n",
      "Epoch  35/100 Batch  440/781 - Loss:  0.256, Seconds: 2.95\n",
      "Epoch  35/100 Batch  460/781 - Loss:  0.259, Seconds: 2.85\n",
      "Epoch  35/100 Batch  480/781 - Loss:  0.247, Seconds: 2.60\n",
      "Epoch  35/100 Batch  500/781 - Loss:  0.269, Seconds: 2.70\n",
      "Average loss for this update: 0.255\n",
      "No Improvement.\n",
      "Epoch  35/100 Batch  520/781 - Loss:  0.214, Seconds: 2.53\n",
      "Epoch  35/100 Batch  540/781 - Loss:  0.254, Seconds: 2.98\n",
      "Epoch  35/100 Batch  560/781 - Loss:  0.232, Seconds: 2.65\n",
      "Epoch  35/100 Batch  580/781 - Loss:  0.263, Seconds: 2.79\n",
      "Epoch  35/100 Batch  600/781 - Loss:  0.269, Seconds: 2.79\n",
      "Epoch  35/100 Batch  620/781 - Loss:  0.256, Seconds: 2.69\n",
      "Epoch  35/100 Batch  640/781 - Loss:  0.263, Seconds: 2.78\n",
      "Epoch  35/100 Batch  660/781 - Loss:  0.217, Seconds: 2.90\n",
      "Epoch  35/100 Batch  680/781 - Loss:  0.250, Seconds: 2.81\n",
      "Epoch  35/100 Batch  700/781 - Loss:  0.273, Seconds: 2.84\n",
      "Epoch  35/100 Batch  720/781 - Loss:  0.306, Seconds: 2.93\n",
      "Epoch  35/100 Batch  740/781 - Loss:  0.257, Seconds: 3.04\n",
      "Epoch  35/100 Batch  760/781 - Loss:  0.286, Seconds: 2.86\n",
      "Average loss for this update: 0.259\n",
      "No Improvement.\n",
      "Epoch  35/100 Batch  780/781 - Loss:  0.238, Seconds: 2.61\n",
      "Epoch  36/100 Batch   20/781 - Loss:  0.288, Seconds: 2.71\n",
      "Epoch  36/100 Batch   40/781 - Loss:  0.244, Seconds: 2.24\n",
      "Epoch  36/100 Batch   60/781 - Loss:  0.243, Seconds: 2.98\n",
      "Epoch  36/100 Batch   80/781 - Loss:  0.249, Seconds: 2.70\n",
      "Epoch  36/100 Batch  100/781 - Loss:  0.226, Seconds: 2.69\n",
      "Epoch  36/100 Batch  120/781 - Loss:  0.226, Seconds: 2.50\n",
      "Epoch  36/100 Batch  140/781 - Loss:  0.235, Seconds: 2.59\n",
      "Epoch  36/100 Batch  160/781 - Loss:  0.246, Seconds: 2.22\n",
      "Epoch  36/100 Batch  180/781 - Loss:  0.245, Seconds: 2.34\n",
      "Epoch  36/100 Batch  200/781 - Loss:  0.226, Seconds: 2.74\n",
      "Epoch  36/100 Batch  220/781 - Loss:  0.239, Seconds: 2.61\n",
      "Epoch  36/100 Batch  240/781 - Loss:  0.192, Seconds: 2.66\n",
      "Average loss for this update: 0.238\n",
      "New Record!\n",
      "Epoch  36/100 Batch  260/781 - Loss:  0.232, Seconds: 2.85\n",
      "Epoch  36/100 Batch  280/781 - Loss:  0.235, Seconds: 2.77\n",
      "Epoch  36/100 Batch  300/781 - Loss:  0.261, Seconds: 2.67\n",
      "Epoch  36/100 Batch  320/781 - Loss:  0.264, Seconds: 2.67\n",
      "Epoch  36/100 Batch  340/781 - Loss:  0.231, Seconds: 2.67\n",
      "Epoch  36/100 Batch  360/781 - Loss:  0.238, Seconds: 2.78\n",
      "Epoch  36/100 Batch  380/781 - Loss:  0.201, Seconds: 2.89\n",
      "Epoch  36/100 Batch  400/781 - Loss:  0.232, Seconds: 2.94\n",
      "Epoch  36/100 Batch  420/781 - Loss:  0.220, Seconds: 2.61\n",
      "Epoch  36/100 Batch  440/781 - Loss:  0.253, Seconds: 2.98\n",
      "Epoch  36/100 Batch  460/781 - Loss:  0.253, Seconds: 2.84\n",
      "Epoch  36/100 Batch  480/781 - Loss:  0.256, Seconds: 2.62\n",
      "Epoch  36/100 Batch  500/781 - Loss:  0.280, Seconds: 2.77\n",
      "Average loss for this update: 0.243\n",
      "No Improvement.\n",
      "Epoch  36/100 Batch  520/781 - Loss:  0.236, Seconds: 2.53\n",
      "Epoch  36/100 Batch  540/781 - Loss:  0.246, Seconds: 2.87\n",
      "Epoch  36/100 Batch  560/781 - Loss:  0.294, Seconds: 2.63\n",
      "Epoch  36/100 Batch  580/781 - Loss:  0.353, Seconds: 2.78\n",
      "Epoch  36/100 Batch  600/781 - Loss:  0.324, Seconds: 2.75\n",
      "Epoch  36/100 Batch  620/781 - Loss:  0.301, Seconds: 2.68\n",
      "Epoch  36/100 Batch  640/781 - Loss:  0.293, Seconds: 2.81\n",
      "Epoch  36/100 Batch  660/781 - Loss:  0.231, Seconds: 2.86\n",
      "Epoch  36/100 Batch  680/781 - Loss:  0.255, Seconds: 2.75\n",
      "Epoch  36/100 Batch  700/781 - Loss:  0.294, Seconds: 2.77\n",
      "Epoch  36/100 Batch  720/781 - Loss:  0.310, Seconds: 2.98\n",
      "Epoch  36/100 Batch  740/781 - Loss:  0.263, Seconds: 3.06\n",
      "Epoch  36/100 Batch  760/781 - Loss:  0.272, Seconds: 2.85\n",
      "Average loss for this update: 0.284\n",
      "No Improvement.\n",
      "Epoch  36/100 Batch  780/781 - Loss:  0.246, Seconds: 2.59\n",
      "Epoch  37/100 Batch   20/781 - Loss:  0.293, Seconds: 2.65\n",
      "Epoch  37/100 Batch   40/781 - Loss:  0.259, Seconds: 2.27\n",
      "Epoch  37/100 Batch   60/781 - Loss:  0.273, Seconds: 2.77\n",
      "Epoch  37/100 Batch   80/781 - Loss:  0.258, Seconds: 2.67\n",
      "Epoch  37/100 Batch  100/781 - Loss:  0.233, Seconds: 2.81\n",
      "Epoch  37/100 Batch  120/781 - Loss:  0.244, Seconds: 2.46\n",
      "Epoch  37/100 Batch  140/781 - Loss:  0.224, Seconds: 2.63\n",
      "Epoch  37/100 Batch  160/781 - Loss:  0.261, Seconds: 2.22\n",
      "Epoch  37/100 Batch  180/781 - Loss:  0.253, Seconds: 2.44\n",
      "Epoch  37/100 Batch  200/781 - Loss:  0.241, Seconds: 2.75\n",
      "Epoch  37/100 Batch  220/781 - Loss:  0.250, Seconds: 2.64\n",
      "Epoch  37/100 Batch  240/781 - Loss:  0.192, Seconds: 2.65\n",
      "Average loss for this update: 0.248\n",
      "No Improvement.\n",
      "Stopping Training.\n"
     ]
    }
   ],
   "source": [
    "# Train the Model\n",
    "learning_rate_decay = 0.95\n",
    "min_learning_rate = 0.0005\n",
    "\n",
    "display_step = 20 # Check training loss after every 20 batches\n",
    "stop_early = 0 \n",
    "stop = 3 # If the update loss does not decrease in 3 consecutive update checks, stop training\n",
    "per_epoch = 3 # Make 3 update checks per epoch\n",
    "update_check = (len(sorted_texts_short)//batch_size//per_epoch)-1\n",
    "\n",
    "update_loss = 0 \n",
    "batch_loss = 0\n",
    "summary_update_loss = [] # Record the update losses for saving improvements in the model\n",
    "\n",
    "checkpoint = \"./best_model.ckpt\" \n",
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # If we want to continue training a previous session\n",
    "    #loader = tf.train.import_meta_graph(\"./\" + checkpoint + '.meta')\n",
    "    #loader.restore(sess, checkpoint)\n",
    "    \n",
    "    for epoch_i in range(1, epochs+1):\n",
    "        update_loss = 0\n",
    "        batch_loss = 0\n",
    "        for batch_i, (summaries_batch, texts_batch, summaries_lengths, texts_lengths) in enumerate(\n",
    "                get_batches(sorted_summaries_short, sorted_texts_short, batch_size)):\n",
    "            start_time = time.time()\n",
    "            _, loss = sess.run(\n",
    "                [train_op, cost],\n",
    "                {input_data: texts_batch,\n",
    "                 targets: summaries_batch,\n",
    "                 lr: learning_rate,\n",
    "                 summary_length: summaries_lengths,\n",
    "                 text_length: texts_lengths,\n",
    "                 keep_prob: keep_probability})\n",
    "\n",
    "            batch_loss += loss\n",
    "            update_loss += loss\n",
    "            end_time = time.time()\n",
    "            batch_time = end_time - start_time\n",
    "\n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                print('Epoch {:>3}/{} Batch {:>4}/{} - Loss: {:>6.3f}, Seconds: {:>4.2f}'\n",
    "                      .format(epoch_i,\n",
    "                              epochs, \n",
    "                              batch_i, \n",
    "                              len(sorted_texts_short) // batch_size, \n",
    "                              batch_loss / display_step, \n",
    "                              batch_time*display_step))\n",
    "                batch_loss = 0\n",
    "\n",
    "            if batch_i % update_check == 0 and batch_i > 0:\n",
    "                print(\"Average loss for this update:\", round(update_loss/update_check,3))\n",
    "                summary_update_loss.append(update_loss)\n",
    "                \n",
    "                # If the update loss is at a new minimum, save the model\n",
    "                if update_loss <= min(summary_update_loss):\n",
    "                    print('New Record!') \n",
    "                    stop_early = 0\n",
    "                    saver = tf.train.Saver() \n",
    "                    saver.save(sess, checkpoint)\n",
    "\n",
    "                #else:\n",
    "                    #print(\"No Improvement.\")\n",
    "                    #stop_early += 1\n",
    "                    #if stop_early == stop:\n",
    "                        #break\n",
    "                #update_loss = 0\n",
    "            \n",
    "                    \n",
    "        # Reduce learning rate, but not below its minimum value\n",
    "        learning_rate *= learning_rate_decay\n",
    "        if learning_rate < min_learning_rate:\n",
    "            learning_rate = min_learning_rate\n",
    "        \n",
    "        #if stop_early == stop:\n",
    "            #print(\"Stopping Training.\")\n",
    "            #break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Making Our Own Summaries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see the quality of the summaries that this model can generate, you can either create your own review, or use a review from the dataset. You can set the length of the summary to a fixed value, or use a random value like I have here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_seq(text):\n",
    "    '''Prepare the text for the model'''\n",
    "    \n",
    "    text = clean_text(text)\n",
    "    return [vocab_to_int.get(word, vocab_to_int['<UNK>']) for word in text.split()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- **input_sentences**: a list of reviews strings we are going to summarize\n",
    "- **generagte_summary_length**: a int or list, if a list must be same length as input_sentences\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./best_model.ckpt\n",
      "- Review:\n",
      " If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\n",
      "- Summary:\n",
      " great product\n",
      "\n",
      "\n",
      "- Review:\n",
      " This case is being advertised on Amazon that it fits an IPhone XS 2018.It does not fit my IPhone XS 2018.This I consider poor customer service on Spigen’s par\n",
      "- Summary:\n",
      " a decent\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "input_sentences=[\"If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\",\n",
    "                 \"This case is being advertised on Amazon that it fits an IPhone XS 2018.It does not fit my IPhone XS 2018.This I consider poor customer service on Spigen’s par\"]\n",
    "#input_sentences=[\"If you are looking for the secret ingredient in Robitussin I believe I have found it.  I got this in addition to the Root Beer Extract I ordered (which was good) and made some cherry soda.  The flavor is very medicinal.\"]\n",
    "generagte_summary_length =  [3,2]\n",
    "\n",
    "texts = [text_to_seq(input_sentence) for input_sentence in input_sentences]\n",
    "checkpoint = \"./best_model.ckpt\"\n",
    "if type(generagte_summary_length) is list:\n",
    "    if len(input_sentences)!=len(generagte_summary_length):\n",
    "        raise Exception(\"[Error] makeSummaries parameter generagte_summary_length must be same length as input_sentences or an integer\")\n",
    "    generagte_summary_length_list = generagte_summary_length\n",
    "else:\n",
    "    generagte_summary_length_list = [generagte_summary_length] * len(texts)\n",
    "loaded_graph = tf.Graph()\n",
    "with tf.Session(graph=loaded_graph) as sess:\n",
    "    # Load saved model\n",
    "    loader = tf.train.import_meta_graph(checkpoint + '.meta')\n",
    "    loader.restore(sess, checkpoint)\n",
    "    input_data = loaded_graph.get_tensor_by_name('input:0')\n",
    "    logits = loaded_graph.get_tensor_by_name('predictions:0')\n",
    "    text_length = loaded_graph.get_tensor_by_name('text_length:0')\n",
    "    summary_length = loaded_graph.get_tensor_by_name('summary_length:0')\n",
    "    keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "    #Multiply by batch_size to match the model's input parameters\n",
    "    for i, text in enumerate(texts):\n",
    "        generagte_summary_length = generagte_summary_length_list[i]\n",
    "        answer_logits = sess.run(logits, {input_data: [text]*batch_size, \n",
    "                                          summary_length: [generagte_summary_length], #summary_length: [np.random.randint(5,8)], \n",
    "                                          text_length: [len(text)]*batch_size,\n",
    "                                          keep_prob: 1.0})[0] \n",
    "        # Remove the padding from the summaries\n",
    "        pad = vocab_to_int[\"<PAD>\"] \n",
    "        print('- Review:\\n\\r {}'.format(input_sentences[i]))\n",
    "        print('- Summary:\\n\\r {}\\n\\r\\n\\r'.format(\" \".join([int_to_vocab[i] for i in answer_logits if i != pad])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I hope that you found this project to be rather interesting and informative. One of my main recommendations for working with this dataset and model is either use a GPU, a subset of the dataset, or plenty of time to train your model. As you might be able to expect, the model will not be able to make good predictions just by seeing many reviews, it needs so see the reviews many times to be able to understand the relationship between words and between descriptions & summaries. \n",
    "\n",
    "In short, I'm pleased with how well this model performs. After creating numerous reviews and checking those from the dataset, I can happily say that most of the generated summaries are appropriate, some of them are great, and some of them make mistakes. I'll try to improve this model and if it gets better, I'll update my GitHub.\n",
    "\n",
    "Thanks for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
